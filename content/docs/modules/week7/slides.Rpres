oDCM - Course Summary & Exam Preparation
========================================================
author: Hannes Datta
date: 
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

<!--#

https://support.rstudio.com/hc/en-us/articles/200486468
-->

Welcome to the final lecture in oDCM!
========================================================

<br>
<br>

If you haven't done so, please **explore the exam page & example questions** at [https://odcm.hannesdatta.com/docs/exam]().

The __course evaluation__ is live at https://app.evalytics.nl. Please voice your opinions!


Agenda
========================================================

- Course summary
- From here onwards
  - Recognizing limitations
  - Seizing research opportunities
- Course evaluation
- Exam preparation
- Remaining questions

Positioning in the study program
========================================================

![odcm](odcm_positioning.png)

Lessons learnt #1: Why to use web data?
========================================================
incremental: true

- Research is by far more impactful than non-webdata-based research
  - Explore novel phenomena, be timely!
  - Boost ecological value, get closer to what managers are interested in!
  - Facilitate methodological advancement (e.g., text, video)
  - Better measurement (e.g., control variables)
- But: challenging!
  - Programming skills to master technical challenges
  - Conceptual thinking to navigate design choices & legal risks
  
Lessons learnt #2: Staying in balance
========================================================
incremental: true

![odcm](methodological_framework.png)

__Let's generate a few examples where validity, legal and technical goals conflict.__

Lessons learnt #3: Data Source Selection
========================================================
incremental: true

- Important of "broadening horizon" (e.g., assume perspective of various stakeholders, exceed geographical boundaries, use aggregators rather than primary data providers)
- Consider alternatives to scraping (i.e., avoid defaulting! e.g., APIs, but also ready-made datasets)
- Scope the data context (i.e., understand how the data is generated, assess reliability of the data, explore user conversations about the data, etc.)

Lessons learnt #4: Design Challenges (I)
========================================================
incremental: true

- Look out for information - especially the "surprising ones!"
- Look out for *variables* (e.g., to compare to other studies), but also to see *time availability* differences
- Spend time mapping your navigation path
- Real-time data collection required?! (e.g., wrong historical data, inaccurate timestamps?)
- Algorithmic biases present? Can exert control?

Lessons learnt #5: Design Challenges (II)
========================================================
incremental: true

- How long does the data collection run?
- How to sample/seed?
- To which population do the seeds generalize?
- At what frequency to extract the data (e.g., once? multiple times?)
- How to process the data on-the-fly? (e.g., first store as JSON, then parse; pandas-based parsing)

Lessons learnt #6: Design Challenges (III)
========================================================
incremental: true

- Scientific purpose, and run by research institution?
- Scale and scope? (all data vs. small sample? running time)
- Location of data provider and users
- "Go" decision from provider? Technical intrusiveness?
- Data management & use, commercialization

Lessons learnt #7: Extraction
========================================================
incremental: true

- Prototyping is extremely important
  - `requests` + `beautifulSoup` vs. `selenium` (Chromedriver)
  - extraction methodology (e.g., tags, classes, attribute-values) + stability
  - array misaligning (obey the hierarchy!)
  - scheduling, hiding passwords
  - revise navigation paths
  - add comments to code (make it understandable for others!)
- Start documentation from a readme template
  - Generate plots, descriptive stats
  - Think as a "data supplier" rather than narrowly focusing on *one* (research) question

Looking ahead: Recognizing Limitations
========================================================
incremental: true

- Web data entails modeling challenges - not covered in this course (e.g., self-selection, endogeneity, "messy" data)
- Web data can't give you all (i.e., you don't see internal data such as clickstream data) 
- Legal and ethical issues not fully explored yet

Potential Applications
========================================================
incremental: true

- Collecting data for Master thesis
  - tell my colleagues you have the skills
  - start now, use later (data collection can take a long time!)
 
- PhD and research master students can "invest" into data collections
  - maybe *you are* a future PhD student? Start today! ;)
  - data was crucial to what I study

Academic Opportunities from "what we study"
========================================================
incremental: true

1. Scout out emerging phenomena 
2. Study phenomena that can't be captured otherwise (i.e., unobtrusively)
3. Study diverse populations (e.g., moving being *WEIRD*, more socio-economic backgrounds + geographies)
4. Generating realistic stimuli for experiments (e.g., brand logos)

Academic Opportunities from "how we study" it
========================================================
incremental: true

1. Unleashing real-time data collections (cf. historical)
2. Conduct & support field experiments with a platform's user base
3. Use APIs to *access algorithms*, rather than data (e.g., Google Cloud Vision)
4. Build own [research APIs](https://www.jstatsoft.org/article/view/v094i09)


<!--
::: notes

  - e.g., new (?) format (live streams rather than lectures)
  - e.g., new content (e.g., Jupyter Notebooks rather than books)
  - ...
-->

Next steps: Projects & SPA
========================================================

- Please hand in so I can __make the data package public__! 
  - Take out any passwords (store as environment variables instead)
  - Remove any unnecessary files
  - Want to keep your names on the documentation or anonymize them? 
  - Don't make statements that are too bold!
  - Consider uploading datasets & documentation yourselves [to Zenodo](https://tilburgsciencehub.com/get/doi/?utm_campaign=referral-short)

- Re-read the [grading rubric on the course site](https://odcm.hannesdatta.com/docs/course/project/grading/)!

- You will receive __invite to self- and peer assessment__ via email


Exam overview: Practical part
=============

- Organization
  - When: 3 April, 2pm - 4pm, take home
  - Work max. 2 hours on this part
- How?
  - open book, on your computer
  - ChatGPT/GitHub Copilot et al. ONLY when explicitly asked for
- What?
  - let's look at [some questions now](https://odcm.hannesdatta.com/docs/exam/examplequestions/#practical-part)
  - prep well - expect new websites, new endpoints of APIs that you know

Exam overview: Theoretical part
=============

- Organization
  - When: 4 April, 1pm - 3.15pm (jointly w/ dPrep; break of 15 minutes inside)
  - Work max. 1 hour on this part
- How?
  - closed book
  - I can make selected resources available on the instruction page. __Which ones?__
- What?
  - let's look at [some questions now](https://odcm.hannesdatta.com/docs/exam/examplequestions/#theoretical-part)

Exam tips
========================================================
incremental: true

- Verify `selenium` and chrome work, next to the "regular" scraping toolkits (`requests`, `BeautifulSoup`)
- Let us __compile material__ you want to access for the theoretical part of the exam (I'm proposing the web scraping paper -- download via the introduction page)
- Practice scraping an unknown website (e.g., tutorials at tilburgsciencehub.com, working with the Reddit API) -- the scraper will account for a larger number of points on the exam!
- Importance of "Fields of Gold" paper, including the web appendix (+ reason through challenges)


Next steps: Official course evaluation
========================================================

- Course evaluation has been immensely important to this course
  - This edition: streamlined content of tutorials; introduced new format for coaching sessions; 
  - Last edition: spent one week on Python onboarding; allocated moret ime to coaching sessions; developed Pulse

- Course evaluation has been critical to my career
  - Without my past evaluations, I wouldn't be teaching to you today
  - I will look at all comments
- Scores are most important to show importance of this course

- You will be invited via Evalytics (evaluate [here](https://app.evalytics.nl/#/login)).

Informal feedback
===============

- is this coure too hard? how can I make it 'easier'/'smoother'?
  - e.g., what did you find particularly hard? 
  - what tips would you give to students before they start this course?
  - more on-campus sessions? (or not?)
- how should I deal with group dropouts? unfair to the team? or fine?
- WhatsApp vs. Canvas vs. E-mail? Feel you've got enough support?


Stay in touch!
========================================================


- Stay in touch
  - [LinkedIn](https://www.linkedin.com/in/hannes-datta/),[YouTube](https://www.youtube.com/c/hannesdatta), [Twitter](https://twitter.com/hannesdatta)
  - WhatsApp! +31 13 466 8938
- Contribute to __Tilburg's Open Science initiatives__! 
  - https://tilburgsciencehub.com as content developer / community manager
  - change this course at https://github.com/hannesdatta/course-odcm
<br><br>
- __And, finally... let's show to the world how awesome your new skills are!__

