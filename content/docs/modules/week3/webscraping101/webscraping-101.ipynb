{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101\n",
    "\n",
    "*After finishing this tutorial, you can extract data from multiple pages on the web, and export such data to CSV files so that you can use it in an analysis. Plan a few hours to work through this notebook. Taking a few breaks inbetween keeps you sharp!*\n",
    "\n",
    "*Just starting out with web scraping? Then make sure to have followed the [\"webdata for dummies\" tutorial](https://odcm.hannesdatta.com/docs/modules/week2/webdata-for-dummies/) first.*\n",
    "\n",
    "*Enjoy!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Our main goal is to compile a panel data set of music consumption data for (simulated) users of music-to-scrape.org, a platform developed for practicing web scraping skills.\n",
    "\n",
    "* Identifying a strategy to generating seeds (“sampling”)\n",
    "    * Extracting multiple elements at once using the `.find_all()` function\n",
    "    * Preventing array misalignment\n",
    "* Navigating on a website \n",
    "    * Using URLs to programmatically visit web pages\n",
    "    * Writing loops to execute data collections in bulk using functions\n",
    "* Improving extraction design\n",
    "    * Implementing timers and modularizing extraction code\n",
    "    * Storing data in CSV or JSON files with relevant meta data\n",
    "* Scraping more advanced, dynamic websites\n",
    "    * Understanding the difference between headless requests and browser emulation \n",
    "    * Learn when to apply one of the two methods (using `requests` and `selenium`)\n",
    "\n",
    "--- \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Support Needed?</b> \n",
    "    For technical issues outside of scheduled classes, please check the <a href=\"https://odcm.hannesdatta.com/docs/course/support\" target=\"_blank\">support section</a> on the course website.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating seeds (\"sampling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "So far, we've extracted (=parsed) some information (e.g., names of featured artists) from an artist's individual *artist page*. What we haven't done yet is to take a closer look at the consumption of individual users.\n",
    "\n",
    "In fact, individual users are often a focal point of attention in web scraping. For example, we can sample users' tweets on Twitter/X, or users' movie watching behavior on trakt.tv. \n",
    "\n",
    "Yet, before we can start building what is called a \"panel data set\" (i.e., multiple users, observed over multiple time periods), we need to decide for __which users to obtain information__. Ideally, we would like to capture information for a *sample of users* (or books, movies, series, games - depending on the platform.).\n",
    "\n",
    "In web scraping, we typically refer to a \"seed\" as a starting point for a data collection. Without a seed, there's no data to collect.\n",
    "\n",
    "For example, before we can crawl through all users available at [music-to-scrape.org](https://music-to-scrape.org), we first need to generate a *list of many users of the platform*. (Note that obtaining the user names of ALL users of the site is barely possible).\n",
    "\n",
    "One way to get there would be to:\n",
    "\n",
    "1. first visit the main homepage of [music-to-scrape.org](https://music-to-scrape.org), showing five recently active users at the time, and\n",
    "2. visit a users' profile page and start scraping their consumption data (or anything else on that page; we have done this in the webdata for dummies tutorial). \n",
    "\n",
    "Note that the homepage allows us to \"navigate\" to the users' profile pages, such as by clicking on the user name or the avatar (see red boxes in the figure below). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-users.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Collecting links to use as seeds\n",
    "\n",
    "Let's take a look at how the links for users' profile pages are written in the website's source code.\n",
    "\n",
    "Open the [website](https://music-to-scrape.org), and inspect the underlying HTML code with the Chrome or Firefox Inspector (right click --> inspect element). \n",
    "\n",
    "Each user contains a clickable link (`<a>`), containing the link (`href`) to the user's profile page. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-inspect-link.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we tell a computer to capture the links to the various user pages?\n",
    "\n",
    "One simple way is to select *elements by their tags*. For example, to extract all links (`<a>` tags). \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>How to extract multiple elements at once?</b>\n",
    "    <br>\n",
    "    \n",
    "- By working through other tutorials, you may already be familiar with the <code>.find()</code> function of BeautifulSoup. The <code>.find()</code> function returns the <b>first element</b> that matches your particular \"search query\". <br>\n",
    "- If you want to extract <b>all elements</b> that match a particular search pattern (say, a class name), you can use BeautifulSoup's <code>.find_all()</code> function.<br>\n",
    "- Note that the \"result\" of the <code>.find_all()</code> option is a list of results __that you need to iterate through.__\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.1__\n",
    "\n",
    "Please run the code cell below, which extracts all links (the `a` tag!), and prints the URL (`href`) to the screen. Don't worry, you don't need need to understand the code yet, we'll go over it line by line shortly!\n",
    "\n",
    "If you look at these links more closely, you'll notice that we're not interested in many of these links... \n",
    "\n",
    "Make a list of all links we're *not* interested in (i.e., those *not* pointing to a user page). Which ones are those? Can you find out why they are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/privacy_terms\n",
      "/privacy_terms\n",
      "about\n",
      "/\n",
      "/\n",
      "/tutorial_scraping\n",
      "/tutorial_api\n",
      "#\n",
      "https://api.music-to-scrape.org/docs\n",
      "/about\n",
      "song?song-id=SOQELHR12AC4689565\n",
      "song?song-id=SOZONJS12A58A7920D\n",
      "song?song-id=SOPTRXF12A8C135387\n",
      "song?song-id=SOHUDTL12AAFF43497\n",
      "song?song-id=SOLCJXU12A8C134178\n",
      "song?song-id=SOEHBWL12A6D4F95D1\n",
      "song?song-id=SOOIAGM12AB01862C7\n",
      "song?song-id=SOHHJIL12A8C144BDB\n",
      "song?song-id=SOTKTCQ12AB01863FF\n",
      "song?song-id=SODKRYJ12AC468A00F\n",
      "song?song-id=SOMNCXX12A8C130C46\n",
      "song?song-id=SOEUWYJ12A8C144BE8\n",
      "song?song-id=SOWUAYB12A6D4FA1D4\n",
      "song?song-id=SORUWYJ12AB0181EBF\n",
      "song?song-id=SOMURVQ12A67AD741A\n",
      "song?song-id=SOUNWEU12CF5F88ADE\n",
      "song?song-id=SOQLNGZ12A8C1314E8\n",
      "song?song-id=SOJUORK12A8C143BE5\n",
      "song?song-id=SOGYMWC12A6D4FBAF2\n",
      "song?song-id=SOCIFIF12A8C1378E5\n",
      "song?song-id=SOHWJYQ12A8C13658B\n",
      "song?song-id=SOXFSTR12A8AE463B0\n",
      "song?song-id=SOWYBEG12A6D4F9142\n",
      "song?song-id=SOUBVIH12A8C137C89\n",
      "song?song-id=SOQXGVE12CF5F86D20\n",
      "artist?artist-id=ARQIWOW11F4C840FF1\n",
      "artist?artist-id=ARWBL9E1187FB4E695\n",
      "artist?artist-id=ARSWORN1187B991A7B\n",
      "artist?artist-id=AR00A6H1187FB5402A\n",
      "artist?artist-id=ARCQCYR1187B99367F\n",
      "artist?artist-id=ARJ66JQ1187B99D2FF\n",
      "artist?artist-id=ARKIQSL1241B9C90C8\n",
      "artist?artist-id=ARWC1WN1187FB516CE\n",
      "user?username=PixelGalaxy84\n",
      "user?username=Pixel48\n",
      "user?username=CoderTech68\n",
      "user?username=PixelGamer12\n",
      "user?username=GalaxyNinja25\n",
      "user?username=GalaxyShadow34\n",
      "/tutorial_scraping\n",
      "/tutorial_api\n",
      "https://api.music-to-scrape.org/docs\n",
      "/about\n",
      "/privacy_terms\n",
      "https://www.linkedin.com/company/tilburgsciencehub\n",
      "https://github.com/tilburgsciencehub/music-to-scrape\n",
      "https://twitter.com/tilburgscience\n"
     ]
    }
   ],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "url = 'http://127.0.0.1:8000'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "for link in soup.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__\n",
    "\n",
    "The links we want to ignore are...\n",
    "\n",
    "* The links to the about or privacy pages\n",
    "* Any link pointing to the most popular songs or artists\n",
    "* Any social media links, etc.\n",
    "\n",
    "These links are present on the page, because they are used by users to navigate on the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Collecting *More Specific* Links\n",
    "\n",
    "__Importance__\n",
    "\n",
    "We've just discovered that selecting elements by their tags gives us many irrelevant links. But, how can we narrow down these links, or, in other words, __how can we scrape only the users we're interested in?__.\n",
    "\n",
    "To answer this question, we need to briefly revisit the notion of how an HTML code is structured. __Open your browser's inspect mode again and hover over the \"recently active users\" section on the site.__\n",
    "\n",
    "After inspecting, you'd probably notice that the page is generated according to a rigid structure: all user links are contained in a `<section>` tag, with the attribute `name=\"recent_users\"`. The \"wrong links\" extracted above (i.e., to the about or privacy pages) are *not* part of these elements. \n",
    "\n",
    "So, if we can tell our scraper that we're only interested in the `<a>` tags *within the particular `<section>` with attribute `name` equal to `recent_users`, we end up with our desired selection of links. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Like before, we'll use `.find_all()` to capture all matching elements on the page. The difference, however, is that we do not directly try to extract the __links__ with the tag `a`, but first try to obtain a __list with product containers__ identified by the classname `product_pod`.\n",
    "\n",
    "Run the code below, in which we first try to capture all book containers using the `product_pod` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user?username=CoderTech68',\n",
       " 'user?username=PixelGalaxy84',\n",
       " 'user?username=Pixel48',\n",
       " 'user?username=PixelGamer12',\n",
       " 'user?username=GalaxyNinja25',\n",
       " 'user?username=GalaxyShadow34']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make request\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "url = 'http://127.0.0.1:8000'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "users = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        users.append(link.attrs['href'])\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we retrieve up to six user names. You can now also use the `users` object to look at the data for the first, second, third, ... user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user?username=CoderTech68'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[0] # returns the link to the user page of the 1st user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...to subsequently try to extract the link for the first book..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the user list still contains a lot of \"other\" things, unrelated to the user name. Remember, we extracted the __links__ to the profile pages, not just the user names.\n",
    "\n",
    "If we want to remove anything but the usernames, we can modify our extraction function slightly, using Python's `split` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CoderTech68',\n",
       " 'PixelGalaxy84',\n",
       " 'Pixel48',\n",
       " 'PixelGamer12',\n",
       " 'GalaxyNinja25',\n",
       " 'GalaxyShadow34']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        users.append(link.attrs['href'].split('=')[1])\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need explanation on this code? Just copy-paste it to ChatGPT and ask for an explanation, e.g., using this prompt:\n",
    "\n",
    "> I struggle to understand this piece of Python code in the context of web scraping. \n",
    "> Can you please explain it, paying attention to the complicated last line (user.append())?\n",
    "\n",
    "Pretty cool, right? So let's proceed with some exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2\n",
    "1. Modify the loop (`for link in relevant_section`...) written above to extract the *absolute URLs* rather than the relative URLs. Specifically, combine the website's URL (`https://music-to-scrape.org/`) and the string you extracted earlier (`user?username=GalaxyShadow34`). The final URL needs to be: `https://music-to-scrape.org/user?username=GalaxyShadow34`.\n",
    "\n",
    "2. Write a function to collect many user names (seeds) from this page, returning this information as an array. \n",
    "\n",
    "3. Execute your function from 2) in a while loop, that runs every 2 seconds for a duration of 15 seconds. Importantly, write all URLs to a new-line separated text file, called `seeds.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://music-to-scrape.org/user?username=CoderTech68',\n",
       " 'https://music-to-scrape.org/user?username=PixelGalaxy84',\n",
       " 'https://music-to-scrape.org/user?username=Pixel48',\n",
       " 'https://music-to-scrape.org/user?username=PixelGamer12',\n",
       " 'https://music-to-scrape.org/user?username=GalaxyNinja25',\n",
       " 'https://music-to-scrape.org/user?username=GalaxyShadow34']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1 \n",
    "urls = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        extracted_link = link.attrs['href']\n",
    "        urls.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://music-to-scrape.org/user?username=GalaxyNinja25',\n",
       " 'https://music-to-scrape.org/user?username=PandaStealth43',\n",
       " 'https://music-to-scrape.org/user?username=CoderTech68',\n",
       " 'https://music-to-scrape.org/user?username=GamerMoon97',\n",
       " 'https://music-to-scrape.org/user?username=Geek61',\n",
       " 'https://music-to-scrape.org/user?username=Dragon05']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://music-to-scrape.org/'\n",
    "url = 'http://127.0.0.1:8000'\n",
    "\n",
    "def get_users():\n",
    "  \n",
    "    res = requests.get(url, headers = user_agent)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "    links = []\n",
    "    for link in relevant_section.find_all(\"a\"):\n",
    "        if 'href' in link.attrs: \n",
    "            extracted_link = link.attrs['href']\n",
    "            links.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "    return(links) # to return all links\n",
    "\n",
    "get_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "import time\n",
    "\n",
    "# Define the duration in seconds (1 minute = 60 seconds)\n",
    "duration = 15\n",
    "\n",
    "# Calculate the end time\n",
    "end_time = time.time() + duration\n",
    "\n",
    "f = open('seeds.txt','a')\n",
    "\n",
    "# Run the loop until the current time reaches the end time\n",
    "while time.time() < end_time:\n",
    "    for user in get_users():\n",
    "        f.write(user+'\\n')\n",
    "    time.sleep(2)  # Sleep for a few seconds between each execution\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Preventing array misalignment\n",
    "\n",
    "So far, we have only extracted *one* piece of information (the URL) from the list of recently active users. But, what if we want to also extract the recently consumed song? \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-song-tag.png\" align=\"left\" width=60%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A simple solution may be to just use multiple `.find_all()` commands.\n",
    "\n",
    "__Example__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://music-to-scrape.org/user?username=user24', 'https://music-to-scrape.org/user?username=user34', 'https://music-to-scrape.org/user?username=user7', 'https://music-to-scrape.org/user?username=user4', 'https://music-to-scrape.org/user?username=user42', 'https://music-to-scrape.org/user?username=user19']\n",
      "['Jewel - Serve The Ego (Hani Num Dub)', 'Cirrus - She kills', 'Lionel Rogg - Die Kunst der Fuge_ BWV 1080 (2007 Digital Remaster): Contrapunctus XVII - Inversus', 'Anna Abreu - Shame', 'Fudge Tunnel - Gut Rot']\n"
     ]
    }
   ],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://music-to-scrape.org/'\n",
    "url = 'http://127.0.0.1:8000'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "# getting links\n",
    "links = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        extracted_link = link.attrs['href']\n",
    "        links.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "\n",
    "# getting songs\n",
    "songs = []\n",
    "for song in relevant_section.find_all(\"span\"):\n",
    "    songs.append(song.get_text())\n",
    "\n",
    "\n",
    "# links for each user\n",
    "print(links)\n",
    "\n",
    "# recent songs for each user\n",
    "print(songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this approach seems easily implemented, it is __highly error-prone and needs to be avoided.__\n",
    "\n",
    "So... what happened?\n",
    "\n",
    "The length for these two objects - `links` and `songs` - differ! While the links properly render for each user, we can only retrieve song information for a subset of songs. In the end, we won't be able to tell WHICH song is part of WHICH user.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>What's an array misalignment?</b>\n",
    "    <br>\n",
    "    \n",
    "<ul>\n",
    "<li>\n",
    "When extracting information from the web, we sometimes are prone to \"ripping apart\" the website's original structure by putting data points into individual arrays (e.g., lists such as one list for user names and another for their recently consumed songs). </li>\n",
    "<li>In so doing, we violate the data's original structure: we should store information on users, and <b>each user</b> has a user name/link and song.</li>\n",
    "    <li>The <b>correct way of organizing the data</b> is to create a list of users (e.g., in a dictionary) and then store each attribute (e.g., the song, etc.) <b>within</b> these objects. <b>Only if we store data this way</b> can we be sure to store everything correctly. </li>\n",
    "<br>\n",
    "<li>When we do not adhere to this practice, we run the risk of \"array misalignment\". For example, if only ONE data point were missing for a user, then the (independent) user names array (say, with 6 items) wouldn't be \"1:1 aligned\" with the song array (say, with only 2-5 items).</li>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So, how to do it correctly?__\n",
    "\n",
    "We will first have to iterate through each __user__, and *within* each user, extract the required information.\n",
    "\n",
    "Storing the information in a list of dictionaries corresponds most to this solution (see the example below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'user?username=user24',\n",
       "  'song_name': 'Set Your Goals - the fallen...'},\n",
       " {'url': 'user?username=user34', 'song_name': 'Cirrus - She kills'},\n",
       " {'url': 'user?username=user7',\n",
       "  'song_name': 'Lionel Rogg - Die Kunst der Fuge_ BWV 1080 (2007 Digital Remaster): Contrapunctus XVII - Inversus'},\n",
       " {'url': 'user?username=user4', 'song_name': 'Anna Abreu - Shame'},\n",
       " {'url': 'user?username=user42', 'song_name': 'Lit - Lovely Day'},\n",
       " {'url': 'user?username=user19', 'song_name': 'Fudge Tunnel - Gut Rot'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = 'https://music-to-scrape.org/'\n",
    "url = 'http://127.0.0.1:8000'\n",
    "\n",
    "# Send an HTTP GET request to the URL and store the response\n",
    "res = requests.get(url, headers=user_agent)\n",
    "\n",
    "# Set the encoding of the response to the apparent encoding\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# Find the HTML section with the attribute 'name' equal to 'recent_users'\n",
    "relevant_section = soup.find('section', attrs={'name': 'recent_users'})\n",
    "\n",
    "# Identify individual users within the relevant section\n",
    "users = relevant_section.find_all(class_='mobile-user-margin')\n",
    "\n",
    "# Initialize a list to store user data\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user in the list of users\n",
    "for user in users:\n",
    "    # Check if the user has an 'href' attribute within an anchor tag\n",
    "    if 'href' in user.find('a').attrs:\n",
    "        # Extract the link from the 'href' attribute\n",
    "        extracted_link = user.find('a').attrs['href']\n",
    "    \n",
    "    # Check if the user has a 'span' element\n",
    "    if user.find('span') is not None:\n",
    "        # Get the text content of the 'span' element, which represents song names\n",
    "        song_name = user.find('span').get_text()\n",
    "    else:\n",
    "        # If there is no 'span' element, set the song_name to 'NA'\n",
    "        song_name = 'NA'\n",
    "    \n",
    "    # Create a dictionary object with the extracted data\n",
    "    obj = {'url': extracted_link, 'song_name': song_name}\n",
    "    \n",
    "    # Append the dictionary to the user_data list\n",
    "    user_data.append(obj)\n",
    "\n",
    "# user_data now contains a list of dictionaries, each representing user information with a URL and song name\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Navigating on a Website\n",
    "\n",
    "### 2.1. Using URLs\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Alright - what have we learnt up this point?\n",
    "\n",
    "We've learnt how to extract seeds (here: users) from __one page -- the homepage of the platform.__\n",
    "\n",
    "So... what's missing?\n",
    "\n",
    "Exactly! [`music-to-scrape.org`](https://music-to-scrape.org) contains data on many users. \n",
    "\n",
    "The objective of this section is to navigate through each user's __consumption history__. However, it's important to note that this information is spread across multiple pages, and we need to visit them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Open [the website](https://music-to-scrape.org/user?username=StarCoder49), and click on the \"previous\" button at the top of the page.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-user-page.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Repeat this a couple of times, and observe how the URL in your navigation bar is changing...\n",
    "\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=37`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=36`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=35`\n",
    "- ...\n",
    "\n",
    "Can you guess the next one...?\n",
    "\n",
    "Indeed! The URL can be divided into a __fixed base part__ (`https://music-to-scrape.org/user?username=StarCoder49`), and a __counter__ that is dependent on the page you're visiting (e.g., `&week=36`). \n",
    "\n",
    "__Now let's create a list of all URLs!__ \n",
    "\n",
    "Click once on \"previous week\" to figure out in which week you currently are. Then, we can create a variable, counting downwards from that number to 0 (the first week in which the platform was active). \n",
    "\n",
    "Then, we assemble the complete list of URLs. In our application (as of 11 Sept. 2023), the current week number is 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://music-to-scrape.org/user?username=StarCoder49&week=37',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=36',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=35',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=34',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=33',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=32',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=31',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=30',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=29',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=28',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=27',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=26',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=25',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=24',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=23',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=22',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=21',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=20',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=19',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=18',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=17',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=16',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=15',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=14',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=13',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=12',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=11',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=10',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=9',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=8',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=7',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=6',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=5',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=4',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=3',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=2',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=1',\n",
       " 'https://music-to-scrape.org/user?username=StarCoder49&week=0']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 37\n",
    "page_urls = []\n",
    "while counter >=0:\n",
    "    page_urls.append(f'https://music-to-scrape.org/user?username=StarCoder49&week={counter}')\n",
    "    counter-=1\n",
    "page_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this gives a list of all page URLs that contain consumption data for this particular user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of page urls in the list is: 38\n"
     ]
    }
   ],
   "source": [
    "# print the number of pages urls (btw, run print(page_urls) for yourself to see all page URLs!)\n",
    "print(\"The number of page urls in the list is: \" + str(len(page_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1\n",
    "\n",
    "Let's take a step back again, and practice combining the seeds from the previous exercises, with what you've just learnt. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the function `get_users()` to generate once a list of users currently active on the site (see exercise 1.2). Store these user names in a list called `users`.\n",
    "2. Create an empty object, called `urls`. Then, loop through your list of users and - at each iteration - append all URLs (i.e., user-week) to the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. Let's first use the get_users() function from above (see exercise 1.2) to regenerate a list of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = get_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let us now assemble the list of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "for user in users:\n",
    "    counter = 37\n",
    "    page_urls = []\n",
    "    while counter >=0:\n",
    "        urls.append(f'{user}&week={counter}')\n",
    "        counter-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://music-to-scrape.org/user?username=Wizard25&week=37',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=36',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=35',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=34',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=33',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=32',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=31',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=30',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=29',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=28',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=27',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=26',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=25',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=24',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=23',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=22',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=21',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=20',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=19',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=18',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=17',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=16',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=15',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=14',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=13',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=12',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=11',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=10',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=9',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=8',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=7',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=6',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=5',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=4',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=3',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=2',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=1',\n",
       " 'https://music-to-scrape.org/user?username=Wizard25&week=0',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=37',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=36',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=35',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=34',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=33',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=32',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=31',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=30',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=29',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=28',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=27',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=26',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=25',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=24',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=23',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=22',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=21',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=20',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=19',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=18',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=17',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=16',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=15',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=14',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=13',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=12',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=11',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=10',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=9',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=8',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=7',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=6',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=5',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=4',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=3',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=2',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=1',\n",
       " 'https://music-to-scrape.org/user?username=Rocket10&week=0',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=37',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=36',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=35',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=34',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=33',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=32',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=31',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=30',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=29',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=28',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=27',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=26',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=25',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=24',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=23',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=22',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=21',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=20',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=19',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=18',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=17',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=16',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=15',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=14',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=13',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=12',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=11',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=10',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=9',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=8',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=7',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=6',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=5',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=4',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=3',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=2',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=1',\n",
       " 'https://music-to-scrape.org/user?username=Vector13&week=0',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=37',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=36',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=35',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=34',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=33',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=32',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=31',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=30',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=29',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=28',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=27',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=26',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=25',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=24',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=23',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=22',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=21',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=20',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=19',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=18',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=17',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=16',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=15',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=14',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=13',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=12',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=11',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=10',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=9',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=8',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=7',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=6',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=5',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=4',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=3',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=2',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=1',\n",
       " 'https://music-to-scrape.org/user?username=Dragon21&week=0',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=37',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=36',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=35',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=34',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=33',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=32',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=31',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=30',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=29',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=28',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=27',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=26',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=25',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=24',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=23',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=22',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=21',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=20',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=19',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=18',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=17',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=16',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=15',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=14',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=13',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=12',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=11',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=10',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=9',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=8',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=7',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=6',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=5',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=4',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=3',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=2',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=1',\n",
       " 'https://music-to-scrape.org/user?username=NinjaPanda86&week=0',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=37',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=36',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=35',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=34',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=33',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=32',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=31',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=30',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=29',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=28',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=27',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=26',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=25',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=24',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=23',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=22',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=21',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=20',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=19',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=18',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=17',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=16',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=15',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=14',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=13',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=12',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=11',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=10',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=9',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=8',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=7',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=6',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=5',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=4',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=3',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=2',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=1',\n",
       " 'https://music-to-scrape.org/user?username=NinjaCyber84&week=0']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view result\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, one of the big disadvantages of this \"manual\" link building is that we need to \"know\" how many pages to extract information from. This may vastly differ by user and across time. \n",
    "\n",
    "We turn towards this issue next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using links contained in elements (e.g., buttons)\n",
    "\n",
    "__Importance__\n",
    "\n",
    "For now, the user link extraction has worked without problems. Yet, there's still one little improvement that we can make. *If the number of pages changes*, we need to manually update for how many pages we would like to retrieve seeds.\n",
    "\n",
    "A general solution is therefore to look up whether there is a `previous` button on the page (see HTML code below). We can then either \"grab\" the URL and visit it (so, in essence, we're still using URLs to navigate), or - instead - \"click\" on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-previous-page.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "So, let's write a snippet that \"captures\" the link of the next page button on the [books page](https://books.toscrape.com).\n",
    "\n",
    "We always proceed in small steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the website's source code and convert to BeautifulSoup object\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49'\n",
    "url = 'http://127.0.0.1:8000/user?username=StarCoder49'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"page-link\" href=\"user?username=StarCoder49&amp;week=36\" type=\"previous_page\">Previous\n",
       "                                        Week</a>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Trying to locate the previous button, using a combination of class names and attribute-value pairs.\n",
    "soup.find(class_='page-link', attrs={'type':'previous_page'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user?username=StarCoder49&week=36'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Trying to extract the `href` attribute\n",
    "soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user?username=StarCoder49&week=36'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Storing \"previous page\" link\n",
    "previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "previous_page_link # print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, we can observe how we're getting closer to the information we need.\n",
    "\n",
    "Now, we only need to combine the base URL (`https://music-to-scrape.org/`) with the page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://music-to-scrape.org/user?username=StarCoder49&week=36'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "f'https://music-to-scrape.org/{previous_page_link}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.2__\n",
    "\n",
    "Please first load the snippet below, which has wrapped the \"previous page\" capturing in a function. Observe the use of `try` and `except`, which accounts for the last page NOT having a next page button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_page(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    try:\n",
    "        previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "        return(f'https://music-to-scrape.org/{previous_page_link}')\n",
    "    except:\n",
    "        return('no previous page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Pass 'https://music-to-scrape.org/user?username=StarCoder49&week=36' to `previous_page()` and observe the output. Then, use  `https://music-to-scrape.org/user?username=StarCoder49&week=0`. Is that what you expected? \n",
    "\n",
    "2. Write a while loop that assembles a list of all product pages for the user `StarCoder49`, by extracting previous page URLs from each page and appending them to an array/list called `urls`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://music-to-scrape.org/no next page'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "previous_page('https://music-to-scrape.org/user?username=StarCoder49&week=36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/category/books_1/no next page'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_page('https://music-to-scrape.org/user?username=StarCoder49&week=0')\n",
    "# returns \"no next page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to get previous page URL from http://127.0.0.1:8000/user?username=StarCoder49\n",
      "Trying to get previous page URL from https://music-to-scrape.org/user?username=StarCoder49&week=36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://music-to-scrape.org/user?username=StarCoder49&week=36']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "urls = []\n",
    "\n",
    "# define first URL to start from\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49'\n",
    "url = 'http://127.0.0.1:8000/user?username=StarCoder49'\n",
    "\n",
    "while True:\n",
    "    print('Trying to get previous page URL from ' + url)\n",
    "    previous_url = previous_page(url)\n",
    "    if 'no previous page' in previous_url: break\n",
    "    url = previous_url\n",
    "    urls.append(url)\n",
    "    \n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Collecting information from each user page\n",
    "\n",
    "Up to this moment, we have defined which seeds to use (usernames from the homepage), and identified from which pages we would like to extract information (e.g., for weeks 37 through 0). Yet, we haven't yet extracted any of the consumption data from the website (e.g., which song a particular user has listened to in a given week.\n",
    "\n",
    "For this, we use our previous learnings (e.g., see \"Web scraping for Dummies\" tutorial in this course) to iterate through the table.\n",
    "\n",
    "__Exercise 2.3__\n",
    "\n",
    "View the code snippet below, which *prints* the information on what songs were listened to by a user to the screen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song \"So Ist Das Nun Mal\" by \"Andreas Dorau\"\n",
      "Song \"Hex Breaker\" by \"Taint\"\n",
      "Song \"Picture Book\" by \"Ray Davies\"\n",
      "Song \"Lombrigas e os vermes\" by \"Eddie\"\n",
      "Song \"124 Stomp\" by \"Azukx\"\n",
      "Song \"Untitled\" by \"Rhian Sheehan\"\n",
      "Song \"Bare As You Dare\" by \"Lady Saw\"\n",
      "Song \"Long Day (Album Version)\" by \"Soul Asylum\"\n",
      "Song \"Dear Friend\" by \"Shunza\"\n",
      "Song \"Bop Till You Drop\" by \"Michael Stanley Band\"\n",
      "Song \"On The Boards\" by \"Taste\"\n",
      "Song \"Fool For Your Loving\" by \"Whitesnake\"\n",
      "Song \"None Missing\" by \"Birdapres\"\n",
      "Song \"Hello\" by \"LL Cool J / Amil\"\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the website's source code and convert to BeautifulSoup object\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    #print(row)\n",
    "    data = row.find_all('td')\n",
    "    \n",
    "    if len(data)>0:\n",
    "        song_name=data[0].get_text()\n",
    "        artist_name=data[1].get_text()\n",
    "        date=data[2].get_text()\n",
    "        time=data[3].get_text()\n",
    "\n",
    "        print(f'Song \"{song_name}\" by \"{artist_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Store the information in a list of dictionaries, containing the following data points:\n",
    "    - username\n",
    "    - song\n",
    "    - artist\n",
    "    - date\n",
    "    - time\n",
    "2. Wrap your code in a function, that returns the JSON dictionary from 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'song_name': 'So Ist Das Nun Mal',\n",
       "  'artist_name': 'Andreas Dorau',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '23:03:59',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Hex Breaker',\n",
       "  'artist_name': 'Taint',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:59:18',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Picture Book',\n",
       "  'artist_name': 'Ray Davies',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:56:46',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Lombrigas e os vermes',\n",
       "  'artist_name': 'Eddie',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:53:56',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': '124 Stomp',\n",
       "  'artist_name': 'Azukx',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:46:37',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Untitled',\n",
       "  'artist_name': 'Rhian Sheehan',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:45:58',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Bare As You Dare',\n",
       "  'artist_name': 'Lady Saw',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:42:07',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Long Day (Album Version)',\n",
       "  'artist_name': 'Soul Asylum',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:39:21',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Dear Friend',\n",
       "  'artist_name': 'Shunza',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:34:43',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Bop Till You Drop',\n",
       "  'artist_name': 'Michael Stanley Band',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:30:19',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'On The Boards',\n",
       "  'artist_name': 'Taste',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:24:17',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Fool For Your Loving',\n",
       "  'artist_name': 'Whitesnake',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:20:06',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'None Missing',\n",
       "  'artist_name': 'Birdapres',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:16:12',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Hello',\n",
       "  'artist_name': 'LL Cool J / Amil',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:12:20',\n",
       "  'username': 'StarCoder49'}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1:\n",
    "\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49'\n",
    "url = 'http://127.0.0.1:8000/user?username=StarCoder49'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "json_data=[]\n",
    "for row in rows:\n",
    "    data = row.find_all('td')\n",
    "\n",
    "    if len(data)>0:\n",
    "        song_name=data[0].get_text()\n",
    "        artist_name=data[1].get_text()\n",
    "        date=data[2].get_text()\n",
    "        time=data[3].get_text()\n",
    "        json_data.append({'song_name': song_name,\n",
    "                          'artist_name': artist_name,\n",
    "                          'date': date,\n",
    "                          'time': time,\n",
    "                          'username': url.split('=')[1]})\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "\n",
    "def get_consumption_history(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    \n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    json_data=[]\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "    \n",
    "        if len(data)>0:\n",
    "            song_name=data[0].get_text()\n",
    "            artist_name=data[1].get_text()\n",
    "            date=data[2].get_text()\n",
    "            time=data[3].get_text()\n",
    "            json_data.append({'song_name': song_name,\n",
    "                              'artist_name': artist_name,\n",
    "                              'date': date,\n",
    "                              'time': time,\n",
    "                              'username': url.split('=')[1]})\n",
    "    return(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'song_name': 'So Ist Das Nun Mal',\n",
       "  'artist_name': 'Andreas Dorau',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '23:03:59',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Hex Breaker',\n",
       "  'artist_name': 'Taint',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:59:18',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Picture Book',\n",
       "  'artist_name': 'Ray Davies',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:56:46',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Lombrigas e os vermes',\n",
       "  'artist_name': 'Eddie',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:53:56',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': '124 Stomp',\n",
       "  'artist_name': 'Azukx',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:46:37',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Untitled',\n",
       "  'artist_name': 'Rhian Sheehan',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:45:58',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Bare As You Dare',\n",
       "  'artist_name': 'Lady Saw',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:42:07',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Long Day (Album Version)',\n",
       "  'artist_name': 'Soul Asylum',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:39:21',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Dear Friend',\n",
       "  'artist_name': 'Shunza',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:34:43',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Bop Till You Drop',\n",
       "  'artist_name': 'Michael Stanley Band',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:30:19',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'On The Boards',\n",
       "  'artist_name': 'Taste',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:24:17',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Fool For Your Loving',\n",
       "  'artist_name': 'Whitesnake',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:20:06',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'None Missing',\n",
       "  'artist_name': 'Birdapres',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:16:12',\n",
       "  'username': 'StarCoder49'},\n",
       " {'song_name': 'Hello',\n",
       "  'artist_name': 'LL Cool J / Amil',\n",
       "  'date': '2023-09-11',\n",
       "  'time': '22:12:20',\n",
       "  'username': 'StarCoder49'}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try running the function\n",
    "get_consumption_history('http://127.0.0.1:8000/user?username=StarCoder49')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'song_name': 'Fiel Enamorado',\n",
       "  'artist_name': 'Estrellas Cubanas',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '23:15:18',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Trop De...',\n",
       "  'artist_name': 'Ramses',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '23:12:42',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Midnight',\n",
       "  'artist_name': 'Joe Satriani',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '23:09:36',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Five Long Years',\n",
       "  'artist_name': 'Freddie King',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '23:05:12',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Birak Yakami',\n",
       "  'artist_name': 'Ebru Yasar',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '23:01:37',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Bailando',\n",
       "  'artist_name': 'Casual',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '22:58:14',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Dis_ Oh Dis (Everybody Loves A Lover)',\n",
       "  'artist_name': 'Line Renaud',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '22:55:23',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Apogee (ft. TechTonic)',\n",
       "  'artist_name': 'Paul Taylor',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '22:47:50',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Glow',\n",
       "  'artist_name': 'King Swamp',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '06:25:58',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'South Side Rep Your Hood (Ringtone_ Sonnerie)',\n",
       "  'artist_name': 'Blingtones',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '06:25:19',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Sentimento (Asteri_ Asteraki)',\n",
       "  'artist_name': 'Dakis',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '06:21:49',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Das Guru',\n",
       "  'artist_name': 'Jazzy B',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '06:16:06',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Hustle',\n",
       "  'artist_name': 'The Pharcyde / Big Boy / Schmooche Cat / Randy Mack',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '06:10:31',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'A Movie Of A Placid Lake On A Moonless Night In September - Reprise',\n",
       "  'artist_name': 'Will Ackerman',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '06:07:17',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Was wollen wir noch mehr?',\n",
       "  'artist_name': 'Die Fantastischen Vier',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '06:02:00',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Shuncata Penda',\n",
       "  'artist_name': 'Pardesi',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '05:56:51',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Believe',\n",
       "  'artist_name': 'Saga',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '05:51:56',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Robbers Kill Cop',\n",
       "  'artist_name': 'BT',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '05:51:12',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Hel Vete',\n",
       "  'artist_name': 'Finntroll',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '05:46:57',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'For The Singer Of R.E.M.',\n",
       "  'artist_name': 'fIREHOSE',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '05:43:36',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Please Send Me Someone To Love',\n",
       "  'artist_name': 'Percy Mayfield',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:35:46',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Luck You Got',\n",
       "  'artist_name': 'The High Strung',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:32:39',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Still Waters',\n",
       "  'artist_name': 'Chris Jagger',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:28:30',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Amazing Grace',\n",
       "  'artist_name': 'Jo-El Sonnier',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:25:17',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': '(The Grave Prelude)',\n",
       "  'artist_name': 'Mobb Deep',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:24:27',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Bianca's Midnight Lullaby\",\n",
       "  'artist_name': 'Al Di Meola',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:22:33',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'No Such Thing As Control (Album Version)',\n",
       "  'artist_name': 'Beneath The Sky',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:17:57',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Three Little Words',\n",
       "  'artist_name': 'The Refreshments',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:14:14',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Shrine (Reissue) (Album Version)',\n",
       "  'artist_name': 'King Diamond',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:09:51',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Kinghead Shuffle (Little Freddie King)',\n",
       "  'artist_name': 'Little Freddie King',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:06:02',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Show Me - Splitloop Remix',\n",
       "  'artist_name': 'Deekline & Wizard',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '11:00:23',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Think About Me',\n",
       "  'artist_name': 'Sister Hazel',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '10:57:15',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Viens Danser Jeannette',\n",
       "  'artist_name': 'Jean Segurel',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '10:54:22',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Summer Love Triangle',\n",
       "  'artist_name': 'David Tao',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '10:50:02',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Lords Prayer',\n",
       "  'artist_name': 'Jane Winther',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '10:47:36',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Speak Softly',\n",
       "  'artist_name': 'Claw Hammer',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '10:42:42',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'You Are Holy',\n",
       "  'artist_name': '4Him',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '10:37:46',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Turn The Heat Up',\n",
       "  'artist_name': 'Bobaflex',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:55:12',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Minuet Of The Robots',\n",
       "  'artist_name': 'Perrey And Kingsley',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:52:54',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Where Do The Children Play? (LP Version)',\n",
       "  'artist_name': 'Big Mountain',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:49:18',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Pete's Crusade\",\n",
       "  'artist_name': 'Light Of The World',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:45:11',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Strange Days ( LP Version )',\n",
       "  'artist_name': 'The Doors',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:42:04',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Poor Tom (Album Version)',\n",
       "  'artist_name': 'Led Zeppelin',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:39:02',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Explode (Epic Extended Mix)',\n",
       "  'artist_name': 'Jordan & Baker',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:31:57',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Loving You Too Long',\n",
       "  'artist_name': 'Billy Bragg',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:29:03',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Fall in Philadelphia',\n",
       "  'artist_name': 'Hall & Oates',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:26:13',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Se Viene',\n",
       "  'artist_name': 'Bersuit Vergarabat',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:22:33',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Doodlin'\",\n",
       "  'artist_name': 'Kai Winding',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:19:00',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': '2 Fists Full Of Nothing',\n",
       "  'artist_name': 'Bruisers',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:14:11',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"We'll Learn\",\n",
       "  'artist_name': 'Kuba',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:07:28',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'We Wish You A Merry Christmas',\n",
       "  'artist_name': 'SNOWPATROL',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '02:03:35',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Anvi Viv?',\n",
       "  'artist_name': 'Patrick St-Eloi',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '01:58:25',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Sea_ Subsurface',\n",
       "  'artist_name': 'Aisha Duo',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '01:52:34',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Liquid Frequencies (liquid Soul Mix)',\n",
       "  'artist_name': 'liquid soul & freq',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '01:45:52',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Control It (Album Version)',\n",
       "  'artist_name': 'Static-X',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '01:42:46',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Bad Seed',\n",
       "  'artist_name': 'Jan Howard',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '01:40:01',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Now You Know',\n",
       "  'artist_name': 'The Real Kids',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:56:24',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Motownphilly',\n",
       "  'artist_name': 'Boyz II Men',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:52:18',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'It Came To Pass (Part II)',\n",
       "  'artist_name': 'Mississippi Mass Choir',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:49:13',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'US Air Force',\n",
       "  'artist_name': \"The Sun Harbor's Chorus-Documentary Recordings\",\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:46:55',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Everyone (LP Version)',\n",
       "  'artist_name': 'Socialburn',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:42:53',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Widescreen',\n",
       "  'artist_name': 'Pinch',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:38:05',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Cremona (Instrumental)',\n",
       "  'artist_name': 'Vrse Murphy',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:34:04',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Calm Da Seas',\n",
       "  'artist_name': 'Dave Hollister',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:29:46',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Another Day Another Night',\n",
       "  'artist_name': 'Klubbingman feat. Beatrix Delgado',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:24:13',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Clothes Of Sand',\n",
       "  'artist_name': 'Solas',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:19:57',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Oh Look Misery',\n",
       "  'artist_name': 'Blind Blake & The Royal Victoria Hotel Calypsos',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:17:26',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'La marine',\n",
       "  'artist_name': 'Marc Perrone',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:14:03',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Way We Were',\n",
       "  'artist_name': 'The London Pops Orchestra',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:10:44',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Shake Some Action',\n",
       "  'artist_name': 'Cracker',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:06:21',\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'In The Good Old Summertime',\n",
       "  'artist_name': 'Sophie Tucker',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '04:05:09',\n",
       "  'username': 'StarCoder49&week'}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it also works for different weeks\n",
    "get_consumption_history('http://127.0.0.1:8000/user?username=StarCoder49&week=12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve consumption data, you could now loop through the previously generated list of links (see webdata for dummies tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improving Extraction Design\n",
    "\n",
    "### 3.1 Timers\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Before we started running some of the cells above, you may have observed the usage of the `time.sleep` function. Sending many requests at the same time can overload a server. Therefore, it's highly recommended to pause between requests rather than sending them all simultaneously. This avoids that your IP address (i.e., numerical label assigned to each device connected to the internet) gets blocked, and you can no longer visit (and scrape) the website. \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "In Python, you can import the `time` module, which pauses the execution of future commands for a given amount of time. For example, the print statement after `time.sleep(3)` will only be executed after 3 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll be printed to the console after 3 seconds!\n"
     ]
    }
   ],
   "source": [
    "# run this cell again to see the timer in action yourself!\n",
    "import time\n",
    "pause = 3\n",
    "time.sleep(pause)\n",
    "print(f\"I'll be printed to the console after {pause} seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1__\n",
    "\n",
    "Modify the code above to sleep for 2 minutes. Go grab a coffee inbetween. Did it take you longer than 2 minutes?\n",
    "\n",
    "(if you want to abort the running code, just select the cell and push the \"stop\" button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "time.sleep(2*60)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modularization\n",
    "\n",
    "**Importance**  \n",
    "\n",
    "In scraping, many things have to be executed *multiple times*. For example, whenever we open a new page on books.toscrape.com, we would like to extract all the available book links.\n",
    "\n",
    "To help us execute things over and over again, we will \"modularize\" our code into functions. We can then call these functions whenever we need them. Another benefit from using functions is that we can improve the readability and reusability of our code. If you need a quick refresher on functions, please revisit section 4 of the [Python Bootcamp](https://odcm.hannesdatta.com/docs/tutorials/pythonbootcamp/).\n",
    "\n",
    "**Let's try it out**\n",
    "\n",
    "Let's finish up our book URL scraper by putting together everything we have learned thus far.\n",
    "\n",
    "1. We need a function that extracts all seeds, given a category URL. We would like to store these seeds in a JSON file and save it to the disk. This will consititute our \"sample\" going forward.\n",
    "2. We need a function that opens this JSON file, and captures all of the relevant product information (for now, let's use the title and price).\n",
    "\n",
    "__Exercise 3.2__\n",
    "\n",
    "Write a function to accomplish (1) above? (capturing the seeds and storing them in a JSON file)? Start with the solution in 2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_seeds(start_url = 'https://books.toscrape.com/catalogue/category/books_1/'):\n",
    "    seeds = []\n",
    "    url = start_url\n",
    "    counter = 0 #initialize counter so that you can break earlier from this loop when needed\n",
    "\n",
    "    while True:\n",
    "        counter+=1\n",
    "\n",
    "        if (counter>4): break # (de)activate this comment if you want to break after x iterations for prototyping\n",
    "\n",
    "        print(f'Trying to get next page URL from {url}')\n",
    "\n",
    "        header = {'User-agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers=header)\n",
    "        res.encoding = res.apparent_encoding\n",
    "        soup = BeautifulSoup(res.text)\n",
    "\n",
    "        # extract information\n",
    "        urls = soup.find_all(class_=\"product_pod\")\n",
    "        for book in urls:\n",
    "            url_book = book.find(\"a\").attrs[\"href\"]\n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + url_book\n",
    "            book_url = book_url.replace('../', '')\n",
    "            seeds.append({'product_url': book_url,\n",
    "                          'page_url': url,\n",
    "                          'timestamp': int(time.time())})\n",
    "        \n",
    "        # next page available?\n",
    "        try:\n",
    "            url = 'https://books.toscrape.com/catalogue/category/books_1/' + soup.find(class_='next').find('a')['href']\n",
    "        except:\n",
    "            break # no next page present\n",
    "            \n",
    "    return(seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-3.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-4.html\n"
     ]
    }
   ],
   "source": [
    "data = get_seeds('https://books.toscrape.com/catalogue/category/books_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'product_url': 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-requiem-red_995/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-black-maria_991/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/set-me-free_988/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/olio_984/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676484189},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/in-her-wake_980/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/how-music-works_979/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/foolproof-preserving-a-guide-to-small-batch-jams-jellies-pickles-condiments-and-more-a-foolproof-guide-to-making-small-batch-jams-jellies-pickles-condiments-and-more_978/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/chase-me-paris-nights-2_977/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/black-dust_976/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/birdsong-a-story-in-pictures_975/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/americas-cradle-of-quarterbacks-western-pennsylvanias-football-factory-from-johnny-unitas-to-joe-montana_974/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/aladdin-and-his-wonderful-lamp_973/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/worlds-elsewhere-journeys-around-shakespeares-globe_972/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/wall-and-piece_971/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-four-agreements-a-practical-guide-to-personal-freedom_970/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-five-love-languages-how-to-express-heartfelt-commitment-to-your-mate_969/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-elephant-tree_968/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-bear-and-the-piano_967/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sophies-world_966/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/penny-maybe_965/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/maude-1883-1993she-grew-up-with-the-country_964/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/behind-closed-doors_962/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/you-cant-bury-them-all-poems_961/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676484190},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/slow-states-of-collapse-poems_960/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/reasons-to-stay-alive_959/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/private-paris-private-10_958/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/higherselfie-wake-up-your-life-free-your-soul-find-your-tribe_957/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/without-borders-wanderlove-1_956/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/when-we-collided_955/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/we-love-you-charlie-freeman_954/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/untitled-collection-sabbath-poems-2014_953/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unseen-city-the-majesty-of-pigeons-the-discreet-charm-of-snails-other-wonders-of-the-urban-wilderness_952/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unicorn-tracks_951/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unbound-how-eight-technologies-made-us-human-transformed-society-and-brought-our-world-to-the-brink_950/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/tsubasa-world-chronicle-2-tsubasa-world-chronicle-2_949/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/throwing-rocks-at-the-google-bus-how-growth-became-the-enemy-of-prosperity_948/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/this-one-summer_947/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/thirst_946/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-torch-is-passed-a-harding-family-story_945/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-secret-of-dreadwillow-carse_944/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-pioneer-woman-cooks-dinnertime-comfort-classics-freezer-food-16-minute-meals-and-other-delicious-ways-to-solve-supper_943/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-natural-history-of-us-the-fine-art-of-pretending-2_941/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676484191},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-nameless-city-the-nameless-city-1_940/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-murder-that-never-was-forensic-instincts-5_939/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-most-perfect-thing-inside-and-outside-a-birds-egg_938/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-mindfulness-and-acceptance-workbook-for-anxiety-a-guide-to-breaking-free-from-anxiety-phobias-and-worry-using-acceptance-and-commitment-therapy_937/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-life-changing-magic-of-tidying-up-the-japanese-art-of-decluttering-and-organizing_936/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-inefficiency-assassin-time-management-tactics-for-working-smarter-not-longer_935/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-gutsy-girl-escapades-for-your-life-of-epic-adventure_934/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-electric-pencil-drawings-from-inside-state-hospital-no-3_933/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-death-of-humanity-and-the-case-for-life_932/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-bulletproof-diet-lose-up-to-a-pound-a-day-reclaim-energy-and-focus-upgrade-your-life_931/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-art-forger_930/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-age-of-genius-the-seventeenth-century-and-the-birth-of-the-modern-mind_929/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-activists-tao-te-ching-ancient-advice-for-a-modern-revolution_928/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/spark-joy-an-illustrated-master-class-on-the-art-of-organizing-and-tidying-up_927/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/soul-reader_926/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/security_925/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/saga-volume-6-saga-collected-editions-6_924/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/saga-volume-5-saga-collected-editions-5_923/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/reskilling-america-learning-to-labor-in-the-twenty-first-century_922/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/rat-queens-vol-3-demons-rat-queens-collected-editions-11-15_921/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676484192}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data in new-line separated JSON files\n",
    "\n",
    "import json\n",
    "f = open('seeds.json','w',encoding = 'utf-8')\n",
    "for item in data:\n",
    "        f.write(json.dumps(item))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.3__\n",
    "\n",
    "Now, let's write some code that loads `seeds.json`, and visits each of the websites to extract the product title and price. Remember to build in a little timer (e.g., waiting for 1 second). The prototype/starting code below stops automatically after 5 iterations to minimize server load. Try removing the prototyping condition using the comment character `#` when you think you're done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n",
      "https://books.toscrape.com/catalogue/soumission_998/index.html\n",
      "https://books.toscrape.com/catalogue/sharp-objects_997/index.html\n",
      "https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n"
     ]
    }
   ],
   "source": [
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    print(obj['product_url'])\n",
    "    \n",
    "    # eventually sleep for a second\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tips</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            Use the function <code>parse_website</code> from exercise 1.6 in the \"webdata for dummies\" tutorial and remove the file saving part.\n",
    "        </li>\n",
    " \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the parse_website() function here from an earlier tutorial. Remember also using the import statements!\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_website(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'} # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "    request = requests.get(url, headers = header)\n",
    "    request.encoding = request.apparent_encoding # set encoding to UTF-8\n",
    "    source_code = request.text\n",
    "\n",
    "    # make information \"extractable\" using BeautifulSoup\n",
    "    soup = BeautifulSoup(source_code)\n",
    "    \n",
    "    # title\n",
    "    title = soup.find('h1').get_text()\n",
    "    price = soup.find(class_='price_color').get_text()\n",
    "    instock = soup.find(class_='instock availability').get_text().strip()\n",
    "    stars = soup.find(class_='star-rating').attrs['class'][1]\n",
    "\n",
    "    data = {'title': title,\n",
    "            'price': price,\n",
    "            'instock': instock,\n",
    "            'stars': stars}\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Set Me Free',\n",
       " 'price': '£17.46',\n",
       " 'instock': 'In stock (19 available)',\n",
       " 'stars': 'Five'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether the function works (I just randomly picked a book)\n",
    "parse_website('https://books.toscrape.com/catalogue/set-me-free_988/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data for https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html.\n",
      "Retrieving data for https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html.\n",
      "Retrieving data for https://books.toscrape.com/catalogue/soumission_998/index.html.\n",
      "Retrieving data for https://books.toscrape.com/catalogue/sharp-objects_997/index.html.\n",
      "Retrieving data for https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html.\n"
     ]
    }
   ],
   "source": [
    "# now start from the code above and \"use\" the function\n",
    "\n",
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    url = obj['product_url']\n",
    "    print(f'Retrieving data for {url}.')\n",
    "    \n",
    "    retrieved_data = parse_website(url)\n",
    "    retrieved_data['timestamp_retrieval'] = int(time.time())\n",
    "    # store data\n",
    "    f = open('book_data.json', 'a', encoding = 'utf-8')\n",
    "    f.write(json.dumps(retrieved_data))\n",
    "    f.write('\\n')\n",
    "    f.close() \n",
    "    \n",
    "    # eventually sleep for a second\n",
    "    time.sleep(1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>instock</th>\n",
       "      <th>stars</th>\n",
       "      <th>timestamp_retrieval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock (22 available)</td>\n",
       "      <td>Three</td>\n",
       "      <td>2023-02-15 08:14:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>2023-02-15 08:14:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>2023-02-15 08:14:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Four</td>\n",
       "      <td>2023-02-15 08:14:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Five</td>\n",
       "      <td>2023-02-15 08:14:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock (22 available)</td>\n",
       "      <td>Three</td>\n",
       "      <td>2023-02-15 18:03:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>2023-02-15 18:03:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>2023-02-15 18:03:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Four</td>\n",
       "      <td>2023-02-15 18:03:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Five</td>\n",
       "      <td>2023-02-15 18:03:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title   price                  instock  \\\n",
       "0                   A Light in the Attic  £51.77  In stock (22 available)   \n",
       "1                     Tipping the Velvet  £53.74  In stock (20 available)   \n",
       "2                             Soumission  £50.10  In stock (20 available)   \n",
       "3                          Sharp Objects  £47.82  In stock (20 available)   \n",
       "4  Sapiens: A Brief History of Humankind  £54.23  In stock (20 available)   \n",
       "5                   A Light in the Attic  £51.77  In stock (22 available)   \n",
       "6                     Tipping the Velvet  £53.74  In stock (20 available)   \n",
       "7                             Soumission  £50.10  In stock (20 available)   \n",
       "8                          Sharp Objects  £47.82  In stock (20 available)   \n",
       "9  Sapiens: A Brief History of Humankind  £54.23  In stock (20 available)   \n",
       "\n",
       "   stars timestamp_retrieval  \n",
       "0  Three 2023-02-15 08:14:16  \n",
       "1    One 2023-02-15 08:14:17  \n",
       "2    One 2023-02-15 08:14:19  \n",
       "3   Four 2023-02-15 08:14:20  \n",
       "4   Five 2023-02-15 08:14:22  \n",
       "5  Three 2023-02-15 18:03:18  \n",
       "6    One 2023-02-15 18:03:20  \n",
       "7    One 2023-02-15 18:03:21  \n",
       "8   Four 2023-02-15 18:03:23  \n",
       "9   Five 2023-02-15 18:03:25  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data in pandas\n",
    "import pandas as pd\n",
    "pd.read_json('book_data.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Summary\n",
    "\n",
    "At the beginning of this tutorial, we set out the promise of writing multi-page scrapers from start to finish. Although the examples we have studied are relatively simple, the same principles (seed definition, data extraction plan, page-level data collection) apply to any other website you'd like to scrape. \n",
    "\n",
    "But... then, there are more *advanced websites*, which we address next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scraping more advanced, dynamic websites\n",
    "\n",
    "In previous tutorials, you have used the `requests` library to retrieve web data. For example, re-run the following code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>Sharp Objects</h1>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "request = requests.get('https://books.toscrape.com/catalogue/sharp-objects_997/index.html', headers = header)\n",
    "request.encoding = request.apparent_encoding\n",
    "source_code = request.text\n",
    "\n",
    "# save website \n",
    "f=open('simple_website.html','w',encoding='utf-8')\n",
    "f.write(source_code)\n",
    "f.close()\n",
    "\n",
    "# parse some information\n",
    "soup=BeautifulSoup(source_code)\n",
    "soup.find('h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for relatively simple websites, but... try the same for the homepage of Twitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = requests.get('https://www.twitch.tv/', headers = header)\n",
    "request.encoding = request.apparent_encoding\n",
    "source_code = request.text\n",
    "soup=BeautifulSoup(source_code)\n",
    "\n",
    "# save website \n",
    "f=open('advanced_website.html','w',encoding='utf-8')\n",
    "f.write(source_code)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to open `advanced_website.html` in your browser, you quickly realize there is a problem. You can't see what's on the website when you manually open it using the URL. This mainly has to do with how advanced a website is: in the case of Twitch, you'd encounter quite a dynamic site with a video player, previews, real-time updates on the number of streams, etc. The normal request library isn't just able to handle it. \n",
    "\n",
    "So, we're resorting to an alternative way to retrieve data, using `selenium`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Making a connection to a website using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Installing Selenium and Chromedriver</b> \n",
    "\n",
    "To install Selenium and Chromedriver locally, please follow the <a href=\"https://tilburgsciencehub.com/configure/python-for-scraping/?utm_campaign=referral-short\">Tutorial on Tilburg Science Hub</a>.\n",
    "    \n",
    "You can also use the code snippet below to automate the installation. Running this snippet takes a little longer each time, but the benefit is that it almost always works!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing and starting up Chrome using Webdriver Manager\n",
    "!pip install webdriver_manager\n",
    "!pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Opening the Twitch site\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "url = \"https://twitch.tv/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went smooth, your computer opened a new Chrome window, and opened `twitch.tv`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Using Google Colab</b> \n",
    "\n",
    "If you're using Google Colab, you don't see your browser open up manually.\n",
    "    \n",
    "Whenever you switch pages, just manually open that page in your browser. Although this feels like a little less interactive, you will still be able to work through this tutorial!\n",
    "\n",
    "</div>\n",
    "\n",
    "From now onwards, you can use `driver.get('https://google.com')` to point to different websites (i.e., you don't need to install it over and over again, unless you open up a new instance of Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using BeautifulSoup with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now also try to extract information. Note that we're converting the source code of the site to a `BeautifulSoup` object (because you may have learnt how to use `BeautifulSoup` earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need the time package to wait a few seconds until the page is loaded\n",
    "import time\n",
    "url = \"https://twitch.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using the \"source code\" obtained with the `requests` library, we can now convert the source code of the Selenium website to a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and start experimenting with querying the site, such as retrieving the titles of the currently active streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1: 🔴CLICK HERE🔴CLICK NOW🔴CLICKY CLICKY🔴NEWS BIG🔴DRAMA MEGA🔴NO VALENTINE ANDY🔴LONELY CERTIFIED CONTENT🔴BASEMENT WARLORD🔴#1 GOBLIN🔴xQc\n",
      "Stream 2: HIGLIGHTS: G2 Esports vs Heroic - IEM Katowice 2023 - Grand FinalESL_CSGO\n",
      "Stream 3: VCT LOCK//IN  TH vs. EG— Alpha Bracket Day 3VALORANT\n",
      "Stream 4: PSN: AuzioMF - 86+ MIXED CAMPAIGN PLAYER PICKS! 🔥 !prime @AuzioMFAuzioMF\n",
      "Stream 5: ADEYEMI'S ARMYdannyaarons\n",
      "Stream 6: #ANALYSE 5 AVEC SLIPIXotplol_\n",
      "Stream 7: [DROPS] Annie Huffley Hufflepuff playthrough - HARD mode - 100% challenges complete, 92% trophies !nordvpnAnnieFuchsia\n",
      "Stream 8: freelancingsips_\n",
      "Stream 9: [DROPS] CAN I GET A HOYYAHHHHHHHHH!!! hogwarts later <3 !discord !skillsharelydiaviolet\n",
      "Stream 10: ❌Donaton Stream❌ !Donaton !TSamSaberi\n",
      "Stream 11: DON'T HAVE MUCH TIME BUT I WANT TO STREAMFoolish_Gamers\n",
      "Stream 12: 🦐 WHOLESOME ORCA IS BACK! AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA ~ BLO'HOLE BLAST FLAVOR RELEASE → !gg 《VTuber》!socials !gg !merchShylily\n",
      "Stream 13: i r wizard | discord.gg/SnowuhSnow\n",
      "Stream 14: New playtrough - Griffoendor with storytelling! 🧙🏼 Drops onMathia\n",
      "Stream 15: [24/7 LIVE] !drops - Murder time?? [Live Since 2013]StreamerHouse\n",
      "Stream 16: veigar cho buffs waiting room !nordlol_nemesis\n",
      "Stream 17: NNO Flexathon 2-1NoWay4u_Sir\n",
      "Stream 18: whisper stream (my walls are paper thin) | !vodsElosanta\n",
      "Stream 19: 1 MILLION SUBSCRIBER PARTY?? 😮 GET IN HERE!!!Wirtual\n",
      "Stream 20: Not cooking on a wednesday?? Change scares me BUT NOT AS MUCH AS HOW MUCH RIZZ I LACK !merch available at onimart.gg !stoneforged !ttsonigirien\n",
      "Stream 21: [DROPS] 🔥 CONTENDER CUP - HUNTER x REVENJA 🔥 NO RESTREAMSolaryFortnite\n",
      "Stream 22: CUP OF THE DAY WINNER POV | TMCL MATCH 10PM CETScrapie\n",
      "Stream 23: 3 HEARTS HERO MODE - BAH GAWDBarbarousKing\n",
      "Stream 24: BACK TO BACK CUP OF THE DAY WIN!?? - !newvidSpammiej\n",
      "Stream 25: First Touch | Episode 9 | Season 3RocketLeague\n",
      "Stream 26: New Killer PTB (& more) incoming!! | !ptb !faq !newkiller !displateOtzdarva\n",
      "Stream 27: KSP Story Mode - Other Worlds ModpackGiantwaffle\n",
      "Stream 28: KOI vs. w7m esports // Six Invitational 2023 – Playoffs – Day 8Rainbow6\n",
      "Stream 29: New Vaults and Exotic Weapons | Code NickEh30 #EpicPartnerNickEh30\n",
      "Stream 30: HIGHEST DROP % ON THIS STREAM GET YOUR DROPS HERE  🪄 !tsm !gfuel !tiktok !ytMande\n",
      "Stream 31: [DROPS ON] ENDGAME CONTENT (SU & FH) !HelloFresh | DAY 871Enviosity\n",
      "Stream 32: DROPS - Lost Ark - Reset Day, lets hang out!Elly\n",
      "Stream 33: I WILL NOT RAGE. I WILL NOT RAGE | Drops | Ghostbusters: Spirits UnleashedSR_Kaif\n",
      "Stream 34: Art Lessons w/ Yoclesh - !picks !factionwars !twitter !vtuberversusrprx\n",
      "Stream 35: Heute drückn wir fleißig PLAY! | 80er/90er/Charts | !insta !Löwenanteil #werbungSintica\n",
      "Stream 36: 🔞Drawing valentine waifu in lingerie❤ | Premade emotes and PNGtubers on my !Etsy | !twitter !Discord !rogueChaisia\n",
      "Stream 37: Arena Kings | !merch !charity !newvid !akGMHikaru\n",
      "Stream 38: ⚠️Feeling Some Wins Today⚠️| !FirestoneRduLIVE\n",
      "Stream 39: [4/5 WINS] HUGE SEASON 2 UPDATE RIGHT NOW! | NUKE ATTEMPTS ALL DAY! 🤠 | 100 NUKES! | 687 WINS!Symfuhny\n",
      "Stream 40: !vote HBOX IS AWAKE!!! $3,000 COINBOX LATER TODAY! SUBATHON DAY 3!!! ▪ !subathonHungrybox\n",
      "Stream 41: Ohlalala on découvre Smash dans le sac à dos d'Etoiles !Gotaga\n",
      "Stream 42: On explique Smash Bros a Gotaga !Etoiles\n",
      "Stream 43: it aint much but it's honest work | !nitro (#ad) !docket !prime 🐢Northernlion\n",
      "Stream 44: C9 k3soju | lebron of tft cookin up rank 1 ladder and corrupted cup | !yt !clips !factork3soju\n",
      "Stream 45: metroid prime after two a20 heartsAlbinoVEVO\n",
      "Stream 46: WARIO WEDNESDAY? Wario Land 3 Playthrough!360Chrism\n",
      "Stream 47: Metroid -> Dark and Darker (LAST DAY FOR REAL)DumbDog\n",
      "Stream 48: THE BIGGEST TROLL COLLAB IN MARIO MAKER HISTORY ~~ FINALE ~~CarlSagan42\n",
      "Stream 49: IMSA +LMP2 fixed -  !youtube !discordFastnFresh\n",
      "Stream 50: Subathon Day 2😤| Road to 9k | RAmer 24/7 | !Subathon 50% To Cancer ResearchDriveDead12\n",
      "Stream 51: Bandits & Murderers (F4 & IMSA) etc...EmilBernstorff\n",
      "Stream 52: RTTF !YTTheFutAccountant\n",
      "Stream 53: WW JESUS games! | League SBC grind | Hogwarts Day 8 | $500 giveaway | Socials: Nick28TNick28T\n",
      "Stream 54: New TOTW Inbound! @NepentheZ !primeNepentheZ\n"
     ]
    }
   ],
   "source": [
    "streams = soup.find_all('a', attrs = {'data-test-selector':\"TitleAndChannel\"})\n",
    "\n",
    "# print a list of stream names\n",
    "counter = 0\n",
    "for stream in streams:\n",
    "    counter = counter + 1\n",
    "    print('Stream ' + str(counter) + ': ' + stream.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - this is cool. You've just learnt a second way to open websites using `selenium`. The benefit of `selenium` is that you can work with highly dynamic websites (which also helps you to not getting blocked). The drawback is that `selenium` is slower than just using the `requests` library, and it may sometimes be buggy on computers without a screen (which matters when you scale up your data collection.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Awesome stuff with Selenium</b> \n",
    "\n",
    "Selenium is your best shot at navigating a dynamic website. It can do amazing things, such as \n",
    "    \n",
    "<ul>\n",
    "    <li>\"clicking\" on buttons</li>\n",
    "    <li>scrolling through a site</li>\n",
    "    <li>hovering over items and capturing information from popups,</li>\n",
    "    <li>starting to play a stream,</li>\n",
    "    <li>typing text and submitting it in the chat, and</li>\n",
    "    <li>so much more...!</li>\n",
    "</ul>\n",
    "    \n",
    "Note though that we won't cover the advanced functionality of Selenium in this tutorial, but the optional \"Web data advanced\" tutorial holds the necessary information.\n",
    "   \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "__Exercise 4.1__\n",
    "\n",
    "Please write code snippets to extract the following pieces of information. Do you choose `requests` or `selenium`?\n",
    "\n",
    "1. The titles of all `<h2>` tags from `https://odcm.hannesdatta.com/docs/course/`\n",
    "2. The titles of all available TV series from `https://www.bol.com/nl/nl/l/series/3133/30291/` (about 24)\n",
    "\n",
    "```\n",
    "soup.find_all('a', class_='product-title')\n",
    "```\n",
    "\n",
    "\n",
    "We also need the time package to wait a few seconds until the page is loaded.\n",
    "\n",
    "```\n",
    "import time\n",
    "url = \"https://twitch.tv/\" # some example URL\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructor\n",
      "Course description\n",
      "Prerequisites\n",
      "Teaching format\n",
      "Assessment\n",
      "Code of Conduct\n",
      "Structure of the course\n",
      "More links\n"
     ]
    }
   ],
   "source": [
    "# Solution to question 1:\n",
    "header = {'User-agent': 'Mozilla/5.0'} # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "request = requests.get('https://odcm.hannesdatta.com/docs/course/', headers = header)\n",
    "request.encoding = request.apparent_encoding # set encoding to UTF-8\n",
    "soup = BeautifulSoup(request.text)\n",
    "for title in soup.find_all('h2'): print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to question 2:\n",
    "driver.get('https://www.bol.com/nl/nl/l/series/3133/30291/')\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/nl/nl/p/midsomer-murders-seizoen-19-deel-2/9200000119833762/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-17/9200000132010294/',\n",
       " '/nl/nl/p/ncis-seizoen-19/9300000135569426/',\n",
       " '/nl/nl/p/fawlty-towers/9300000087454356/',\n",
       " '/nl/nl/p/sisi-seizoen-2/9300000139818897/',\n",
       " '/nl/nl/p/chicago-fire-seizoen-10/9300000123634169/',\n",
       " '/nl/nl/p/flikken-maastricht-seizoen-16/9300000096688928/',\n",
       " '/nl/nl/p/star-trek-discovery-seizoen-4/9300000127973053/',\n",
       " '/nl/nl/p/house-of-the-dragon-seizoen-1/9300000127606162/',\n",
       " '/nl/nl/p/game-of-thrones-seizoen-1-8/9300000045366024/',\n",
       " '/nl/nl/p/ncis-los-angeles-s12/9300000058801046/',\n",
       " '/nl/nl/p/star-trek-picard-seizoen-2/9300000123707493/',\n",
       " '/nl/nl/p/nachtwacht-het-donkere-spiegelbeeld/9300000128499338/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-12-deel-2/9200000132010284/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-18-deel-1/9200000132010326/',\n",
       " '/nl/nl/p/columbo-complete-collection/9200000096426621/',\n",
       " '/nl/nl/p/outlander-seizoen-6-blu-ray-import-met-nl-ondertiteling/9300000137849257/',\n",
       " '/nl/nl/p/dragon-ball-super-complete-series/9300000015744662/',\n",
       " '/nl/nl/p/the-sommerdahl-murders-seizoen-3/9300000139818898/',\n",
       " '/nl/nl/p/murder-she-wrote-complete-collection/9200000095914989/',\n",
       " '/nl/nl/p/knutby/9300000139818896/',\n",
       " '/nl/nl/p/ncis-los-angeles-seizoen-13-dvd-import-zonder-nl-ondertiteling/9300000140350227/',\n",
       " '/nl/nl/p/ncis-seizoen-18/9300000091148747/',\n",
       " '/nl/nl/p/death-in-paradise-s11/9300000091412821/',\n",
       " '/nl/nl/p/grimm-complete-collection/9200000116002373/',\n",
       " '/nl/nl/p/law-order-s-v-u-seizoen-23/9300000130379562/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-18-deel-2/9200000132010306/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-19-deel-1/9200000119833764/',\n",
       " '/nl/nl/p/star-trek-picard-seizoen-2/9300000123707486/',\n",
       " '/nl/nl/p/call-the-midwife-series-10/9300000043667535/']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for url in soup.find_all('a', class_='product-title'):\n",
    "    urls.append(url.attrs['href'])\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Using interactive elements (e.g., by clicking buttons)\n",
    "\n",
    "__Importance__\n",
    "\n",
    "For more dynamic websites, we may have to click on certain elements (rather than extracting some URL).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Extracting elements using Selenium, not BeautifulSoup</b> \n",
    "\n",
    "Selenium is really great for navigating dynamic website. There are two ways in which you can use it for querying sites:\n",
    "    \n",
    "<ul>\n",
    "    <li>put the \"selenium\" source code (<code>driver.page_source</code>) to BeautifulSoup, and then use BeautifulSoup commands, or </li>\n",
    "    <li>directly use selenium (and it's own query language) to extract elements.</li>\n",
    "</ul>\n",
    "    \n",
    "In the next few examples, we are using selenium's \"internal\" query language (which you identify easily because it is a subfunction of the `driver` object, and because it has a different name (`find_element`, instead of `find` or `find_all`).\n",
    "    \n",
    "Want to know more about selenium's built-in query language? Check out the \"Advanced Web Scraping Tutorial\", or dig up some extra material from the web. Knowing both BeautifulSoup and Selenium makes you most productive!\n",
    "  \n",
    "</div>\n",
    "\n",
    "__Try it out__\n",
    "\n",
    "If you haven't done so, rerun the installation code for `selenium` from above. Then, proceed by running the following cell and observe what happens in your browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://books.toscrape.com/catalogue/category/books_1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, your browser will have loaded the website in Chrome. Now, run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"381144fe48efd393c0dbb5cb4d5a4689\", element=\"ecbeb367-1848-4d47-bdba-e7a98ed9578e\")>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Let's try location the element\n",
    "from selenium.webdriver.common.by import By\n",
    "driver.find_element(By.CLASS_NAME, 'next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"381144fe48efd393c0dbb5cb4d5a4689\", element=\"5f555662-5379-47ab-9a2a-e6e76c2ea298\")>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Finding the link within the `next` class\n",
    "driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clicking the link!\n",
    "driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! In step 3, we finally clicked on the link. Just try rerunning this cell with step 3 over and over again. Does iterating through the pages work?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.2__\n",
    "\n",
    "Iterate through the entire set of pages, until there are no new pages left. This time, use `selenium` and click on the next page button. You can start on page 47 (`https://books.toscrape.com/catalogue/category/books_1/page-47.html`) to speed up this exercise a bit.\n",
    "\n",
    "Make use of the `time.sleep(2)` function to make the code wait a bit after each page load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "urls = []\n",
    "driver.get('https://books.toscrape.com/catalogue/category/books_1/page-47.html')\n",
    "time.sleep(1)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a').click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        break\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After-class exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Extending the code written for exercise 3.2 in \"Web data 101\", please collect seeds from ten self-chosen product categories and store them in a file called `all_seeds.json`.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Please use the code written in exercise 3.3 in \"Web Data 101\" and extend it so capture more information (e.g., not only title and price, but also as other attributes/data points you are interested in. In particular, try getting the product description!\n",
    "\n",
    "Try running your code and store the product data in a JSON dictionary called `all_books.json`.\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Please complete an entire data collection project in a `.py` file, capturing data for 10 product categories and all products contained on all of the pages. You can proceed in two steps: first collect the seeds, then obtain all data. In addition, parse all retrieved data to a CSV file (with rows and columns), using `pd.read_json(filename, lines = True)` for reading in the JSON data, and `pd.to_csv(filename)` for saving the data in tabular format.\n",
    "\n",
    "Run your data collection from the terminal.\n",
    "\n",
    "The final deliverable is\n",
    "- `all_seeds.json`\n",
    "- `all_books.json`\n",
    "- `all_books.csv`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup: Executing Python Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebooks versus editors such as Visual Studio Code, PyCharm, or Spyder\n",
    "\n",
    "Jupyter Notebooks are ideal for combining programming and markdown (e.g., text, plots, equations), making it the default choice for sharing and presenting reproducible data analyses. Since we can execute code blocks one by one, it's suitable for developing and debugging code on the fly. \n",
    "\n",
    "That said, Jupyter Notebooks also have some severe limitations when using them in production environments. That's where an \"Integrated Development Environment\" (IDE) comes in, such as Visual Studio Code or PyCharm. Let's revisit the most important differences.\n",
    "\n",
    "First, the order in which you run cells within a notebook may affect the results. While prototyping, you may lose sight of the top-down hierarchy, which can cause problems once you restart the kernel (e.g., a library is imported after it is being used). Second, there is no easy way to browse through directories and files within a Jupyter Notebook. Third, notebooks cannot handle large codebases nor big data remarkably well. \n",
    "\n",
    "That's why we recommend starting in Jupyter Notebooks, moving code into functions along the way, and once all seems to be running well, save your Jupyter Notebook as a `.py` file and continue working with it in Visual Studio Code.\n",
    "\n",
    "Below, we introduce you to the IDE (here, Spyder, but VS Code looks very similar), and show you how to run Python files from the command line. \n",
    "\n",
    "### Introduction to Spyder\n",
    "The first time you need to click on the green \"Install\" button in Anaconda Navigator, after which you start Spyder by clicking on the blue \"Launch\" button (alternatively, type `spyder` in the terminal). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/anaconda_navigator.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interface consists of three panels: \n",
    "1. **Code editor** = where you write Python code (i.e., the content of code cells in a notebook)\n",
    "2. **Variable / files** = depending on which tab you choose either an overview of all declared variables (e.g. look up their type or change their values) or a file explorer (e.g., to open other Python files)\n",
    "3. **Console** = the output of running the Python script from the code editor (what normally appears below each cell in a notebook)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/spyder.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**     \n",
    "Copy the solution from exercise 3.3 to a new file, called `webscraping_101.py`. To run the script you can\n",
    "\n",
    "- click on the green play button to run all code, or\n",
    "- highlight the parts of the script you want to execute and then click the run selection button.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/toolbar.png\" width=40% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the script is running, you may need to interrupt the execution because it is simply taking too long or you spotted a bug somewhere. Click on the red rectangular in the console to stop the execution. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/interrupt.gif\" width=80% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Python Files \n",
    "\n",
    "__For Mac and Linux users__\n",
    "\n",
    "1. Open the terminal and navigate to the folder in which the `.py` file has been saved (use `cd` to change directories and `ls` to list all files).\n",
    "2. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/running_python.gif\" width=60% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For Windows users__\n",
    "\n",
    "1. Open Windows explorer and navigate to the folder in which the `.py` file has been saved. Type `cmd` to open the command prompt. Alternatively, open the command prompt from the start menu (and use `cd` to change directories and `dir` to list files).\n",
    "2. Activate Anaconda by typing `conda activate`.\n",
    "3. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
