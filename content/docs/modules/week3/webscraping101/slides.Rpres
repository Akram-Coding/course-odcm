oDCM - Web Scraping 101 (Tutorial)
========================================================
author: Hannes Datta
date:
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

<!--#

https://support.rstudio.com/hc/en-us/articles/200486468
-->

```{r setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(reticulate)
py_install("requests")
py_install("bs4")
```

Welcome to oDCM!
========================================================

We're about to start with __today's tutorial ("web scraping 101")__.


- If you haven't done so, open slides at https://odcm.hannesdatta.com/docs/tutorials/webdata-101/
- Today's (live) code is available at [tiu.nu/livecoding]() (refresh needed)
- You can use Google Colab at the beginning of this tutorial, BUT... you will have to use __Jupyter Notebook on your laptops__ towards the end
- Login at [http://pulse.tilburg-digital.com]() to explore this week's to do's


Before we start
========================================================

- Recap coaching session for team activity #1
  - explore broadly! ("universe discovery!") --> check challenge #1.1
  - compare sources thoroughly --> e.g., comparison table + challenge #1.2
  - Consider example projects (e.g., overview tables, illustrations)
  
- Any issues you would like to address?

Framework
=======

![](https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/jmxa/2022/jmxa_86_5/00222429221100750/20220801/images/large/10.1177_00222429221100750-fig2.jpeg)

Today: zooming in more on __collection design__

Recap from last week's tutorial
==========

- We focused on which __information to extract__ (Challenge #2.1)
  - e.g., explore a website, navigate broadly to find information that's interesting, etc.
- We focused on __how to extract__ that information
  - BeautifulSoup: `.find()` function
  - Use (a combination of) tags, classes, attributes, attribute-value pairs

DO: Recap from last week's tutorial
==========
class: small-code

__Extend the code snippet below to extract the product's UPC__.

```{python, eval=TRUE}
import requests
from bs4 import BeautifulSoup
url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'
request = requests.get(url, headers =  {'User-agent': 'Mozilla/5.0'})
soup = BeautifulSoup(request.text)
soup.find('h1').get_text()
```

DO: Solution
==========
class: small-code

- Use the browser's __inspect__ mode to find the relevant attributes
- Consecutively develop the code

```{python, eval=TRUE}
soup.find(class_='table-striped').find('tr').find('td').get_text()
```

__Recommendations:__
- Practice even more, e.g., with __new information__ that you haven't extracted before from this website.
- Always build code __gradually__, never from scratch

Today's focus
================

__Remaining challenges for the "design" phase of collecting data.__

- How to sample from a website? (Challenge #2.2)
  - Recall that we do not have access to a firm's database (so we can't sample from a population of, say, customers)
  - With web scraping, we always need a "starting point" for a data collection
- Examples for seeds/sample
  - product overview pages (books.toscrape.com)
  - home page of Twitch (for currently active live streams)
<br>
__Question: From where could we start a data collection on books.toscrape.com?__

DO: Starting up a data collection for your own project
============

__Using your own project ideas__, tell us how you could __sample__ from the site?

<br>
<br>

Tips:
- Make use of the best practices for Challenge #2.2 in [Table 3](https://journals.sagepub.com/doi/full/10.1177/00222429221100750?journalCode=jmxa#table3-00222429221100750)


We need some new techniques: Finding MORE content
===========
class: small-code

- So far, we only searched for individual items (max. 1!) using the `.find()` function
- BeautifulSoup also has a `.find_all()` function - returning __all of the relevant elements__
- For example, the snippet below *loops* through the attributes table

```{python include=FALSE}
import requests
from bs4 import BeautifulSoup
url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'
request = requests.get(url, headers =  {'User-agent': 'Mozilla/5.0'})
request.encoding = request.apparent_encoding
soup = BeautifulSoup(request.text)
```

```{python, eval=TRUE}
table = soup.find(class_='table-striped')
rows = table.find_all('tr')
for row in rows:
  attribute_name = row.find('th').get_text()
  value=row.find('td').get_text()
  print(f'Attribute: {attribute_name}; value: {value}')

```

DO: Extract all links
=======
class: small-code

__Please use the code snippet below to extract the 'href' attribute of all links on the website.__

- Links are identifiable by the "a" tag. 
- The relevant attribute is called "href"
- Build your code consecutively!

```{python, eval = FALSE}
url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'
res = requests.get(url, headers = {'User-agent': 'Mozilla/5.0'})
res.encoding = res.apparent_encoding
soup = BeautifulSoup(res.text)
```

Solution: Extract all links
=====

```{python, eval=FALSE}
for link in soup.find_all("a"): 
    print(link.attrs["href"])
```

- But... are all of these links really relevant?
- Recall: what "seeds" do we need to gather?

Narrowing down our extraction
============

- Let us explore the site structure a bit more
- Particularly, can we identify 'other', nested ways to navigate on the site?
- __Let's open the inspect mode of our browser and come up with a new strategy.__


Narrowing down our extraction
============

- The class `product_pod` identifies individual elements that we're interested in.

```{python, eval=TRUE}
# return all book containers
url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'
res = requests.get(url, headers = {'User-agent': 'Mozilla/5.0'})
res.encoding = res.apparent_encoding
soup = BeautifulSoup(res.text)

books = soup.find_all(class_="product_pod")
len(books)

```

__DO:__ Can you come up with a way to loop through all of the links WITHIN the classes `product_pod`?

Solution: Narrowing down our extraction
============
class: small-code

```{python, eval=TRUE}
# return all links within book containers
books = soup.find_all(class_="product_pod")
for book in books:
  print(book.find('a').attrs['href'])

```

- Let's now take a look at these links more closely. 

Modifying the links
========
class: small-code

- Notice that the links were *relative* links and we won't be able to use them for looping (e.g., try pasting them in your browser - they won't work!)
- So, let's turn them into absolute links, simply by concatenating (= combining) strings.

```{python, eval=TRUE}
# return all book containers
books = soup.find_all(class_="product_pod")
for book in books:
  url = book.find('a').attrs['href']
  url_cleaned = url.replace('../../','')
  print('https://books.toscrape.com/catalogue/'+url_cleaned)

```

DO: Remember functions?
======

Write a function that...

- accepts an input argument, called `url`
- makes a web request to the particular URL at books.toscrape.com
- use __this URL__: `https://books.toscrape.com/catalogue/category/books_1/index.html`
- extract all links to the books and returns them back 

__Do you remember why we like functions so much?__

Solution
======
class: small-code

```{python, eval=TRUE}
# return all book containers
import requests
from bs4 import BeautifulSoup

def get_seeds(url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'):
  request = requests.get(url, headers =  {'User-agent': 'Mozilla/5.0'})
  request.encoding = request.apparent_encoding
  soup = BeautifulSoup(request.text)
  books = soup.find_all(class_="product_pod")
  urls = []
  for book in books:
    url = book.find('a').attrs['href']
    url_cleaned = url.replace('../../','')
    urls.append('https://books.toscrape.com/catalogue/'+url_cleaned)
  return(urls)

# get_seeds() # remove hash to run
```

- Functions allow us to re-use code; it prevents errors, and helps us structure our code.

Extracting more information, even from the category page
=========

- Access to the books object gives us convenient access to more data, e.g., valence, price
- We can already scrape this information directly (and skip the product pages...) -- remember challenge #2.1?
- Because the data is nested, we ideally store it in dictionaries (important to __prevent array misalignment__)

__Let's explore array misalignment now__ (e.g., on Twitch)


Let's take a step back...
=======

- What we've learnt so far...
  - Extract individual information from a product overview page (ch. #2.1)
  - Collect all links from a category overview page (ch. #2.2)
  
- What's missing
  - MORE THAN ONE overview pages with information
  - We need to loop through them
  
- We can then tie things together in a scraper
  - "collect" all seeds from all of the category pages (scraper 1)
  - then collect all product information from individual product pages (scraper 2)
  
Navigating on a website
=========
class: small-code

- Two strategies
  - Understand how links are built (prebuilding them; strategy 1)
  - Understand how to "click" to the next page (consecutively building them; strategy 2)
  
```{python, eval=FALSE}
urls = []
counter = 1
while counter <= 50:
  urls.append(f'https://books.toscrape.com/catalogue/category/books_1/page-{counter}.html')
  counter = counter  + 1
urls
```

Navigating on a Website (strategy 2)
=========
class: small-code

```{python, eval=FALSE}
import requests
from bs4 import BeautifulSoup
base_url = 'https://books.toscrape.com/catalogue/category/books_1/'
url = base_url

while url:
    print(f'Making request for {url}.')
    request = requests.get(url, headers = {'User-agent': 'Mozilla/5.0'})
    # we can direclty start scraping information from the page here
    soup = BeautifulSoup(request.text)
    try:
        next_page = soup.find(class_='next').find('a')['href']
        url = base_url + next_page
    except:
        url = None
  
```

__DISCUSS:__ When do we use strategy 1 (prebuilt) vs. strategy 2 (do it on the fly)?

Let's tie things together
=========
class: small-code

- It's good practice to separate the "seeding" stage of your scraper from the actual collection stage.
- For example, you collect seeds once, and then repeatedly collect product data (e.g., challenge #2.3)
- Let's take a look at some pseudo code!

```
seeds = []
for p in overview_pages: # challenge #2.2
    links = get_all_links(p) 
    seeds.append(links)

product_data = []
for s in seeds:
  product = get_product_info(s) # challenge #2.1
  product_updated = processing(product) # challenge #2.4
  time.sleep(1) # challenge #2.3
  product_data.append(product_updated)

```

Challenge #2.3: At which frequency to extract the data?
=======

- We can decide how often to capture information from a site
  - e.g., once, every 5 minutes, every day
- Potential gains in extracting data multiple times
- Extraction limits & __technically feasible sample size!__
- More issues explained in table 3, challenge #2.3

 
Challenge #2.4: How to process data during collection
==========

- value of retaining raw data

```
product_data = []
for s in seeds:
  # store raw html code here
  store_to_file(s, 'website.html')
  # continue w/ parsing some information
  product = get_product_info(s)
  product_data.append(product)

```

Challenge #2.4: How to process data during collection
==========

- parsing data on the fly (vs. after the collection)

```
product_data = []
for s in seeds:
  product = get_product_info(s) 
  product_updated = processing(product)
  time.sleep(1) 
  product_data.append(product_updated) 
  # write data to file here (parsing on the fly)
  
# write data to file here (after the collection)

```

Scraping more advanced websites
=========
incremental: true
class: small-code
 
- So far, we have done this
  - `requests` (to get) --> `beautifulsoup` (to extract information, "parse")
  
- __DO:__ Run the snippet below and open `amazon.html`  
```{python, eval=FALSE}
import requests
header = 
f = open('amazon.html', 'w', encoding = 'utf-8')
f.write(requests.get('https://amazon.com', headers =  {'User-agent': 'Mozilla/5.0'}).text)
f.close()
```

__Can you explain what happened?__

Alternative ways to make connections
========
class: small-code

- Many dynamic sites require what I call "simulated browsing"

- Try this:

```{python, eval=FALSE}
!pip install webdriver_manager
!pip install selenium

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# Opening the site
driver = webdriver.Chrome(ChromeDriverManager().install())

url = "https://amazon.com/"
driver.get(url)
```

- __See your browser opening up?__
- Beware of rerunning code - a new instance will run each time!

Continuing with beautifulsoup
=========

- We can convert the site's source code (`page_source`) to `BeautifulSoup`, and proceed as always

```{python, eval=FALSE}
from bs4 import BeautifulSoup
soup = BeautifulSoup(driver.page_source)

for el in soup.find_all(class_ = 'a-section'):
    title = el.find('h2')
    if title is not None: print(title.text)
```

Different ways to open a site
=========

- Static/easy site: use `requests`
  - `requests` --> `BeautifulSoup` --> `.find()`
- Dynamic/difficult site: also use `selenium`
  - `selenium` --> wait --> `BeautifulSoup` --> `.find()`
  
Do: Scraping Twitch.tv using `selenium`
=====

1) Use selenium to open `twitch.tv`

```{python, eval=FALSE}
import time
url = "https://twitch.tv/"
driver.get(url)
time.sleep(3) # wait for a few seconds for everything to be loaded
```

__Tip: Use the code snippet from above first to open the browser window!__.

2) Extend code to be able to use BeautifulSoup, and find all links (tag is `a`). Recall you can use `soup.find_all()`


Next steps
===========

- It's vacation next week
  - But: catch up content, prep future content, work on team project
- Next coaching
   - settle on extraction design (challenges #1.1-1.3) & pitch idea to friends
   - get started w/ __prototyping data extraction__
   - consider `selenium` and APIs (see next tutorials)
- Contribute to the course (and earn bonus points): https://github.com/hannesdatta/course-odcm/issues

Thanks!
=======

- Any questions?
- Be in touch for feedback via WhatsApp.