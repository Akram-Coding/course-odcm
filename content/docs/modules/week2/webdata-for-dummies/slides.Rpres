oDCM - Web Data for Dummies (Tutorial)
========================================================
author: Hannes Datta
date:
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

<!--#

https://support.rstudio.com/hc/en-us/articles/200486468
-->

```{r setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(reticulate)
py_install("requests")
py_install("bs4")
```

Welcome to oDCM!
========================================================

We're about to start with __today's tutorial ("web data for dummies")__.

- If you haven't done so, open this slide deck at https://odcm.hannesdatta.com/docs/modules/week2/webdata-for-dummies/
- Coaching session start after today's tutorial (we will change rooms)
- Team assignment final?

Agenda
========================================================

- In-class
  - Go through in-class tutorial ("webdata for dummies in class.ipynb")
  - We'll work on a **selection of exercises**
  - Also address some challenges summarized in "Fields of Gold"
<br>
- After class
  - Work on team activity #1
  - Complete tutorial and exercises + be in touch for feedback during coaching hours
  

Framework
=======

![](https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/jmxa/2022/jmxa_86_5/00222429221100750/20220801/images/large/10.1177_00222429221100750-fig2.jpeg)

- Focus in __team project__ at this stage: source selection
- Focus in today's tutorial: __collection design__


What are differences between web scraping and APIs?
========================================================
incremental: true

- official vs. unofficial data access
- scaling (APIs scale more)
- web scraping largely free, APIs for-pay
- APIs are linchpin of internet economy
- Learn about other differences and commonalities in the [Web Appendix of "Fields of Gold"](https://journals.sagepub.com/doi/suppl/10.1177/00222429221100750/suppl_file/sj-pdf-1-jmx-10.1177_00222429221100750.pdf).


We get started with web scraping
=================

- To do scraping, we need to understand what websites consist of:
  - HTML ("what you see and where?")
  - CSS ("how it looks")
  - Javascript (capability for advanced interaction)
<br>
- Examples:
  - toybox at codepen: https://codepen.io/rcyou/pen/QEObEk/
  - inspecting [tilburguniversity.edu]()
  
Which information to extract from a website (challenge #2.1)?
================

- We need to decide which information to extract from a site 
  - which information to we need to justify that what we measure is measured well ("construct operationalization")?
  - is the information publicly accessible or hidden after a login wall?
  - can we reliably get the data, even after many iterations on the site?

Which information to extract from a website (challenge #2.1)?
================

- Best practices
  - Explore different types of pages
  - Really get "used" to the website, browse a bit ("how to navigate")
  - Identify roadblocks such as captchas
  - Explore limits to iterating through a site

DO: Explore books.toscrape.com
=====

- Open [music-to-scrape.org]() (fallback option: [books.toscrape.com]())
- Familiarize yourself with the structure of the site
- Check which information you find and what it could be used for

Finding information in HTML code 
================

- Now, it's time to "narrow" down on the information of interest
- For this, we can employ various extraction techniques
  - tags, e.g., `<table>`, `<h1>`, `<div>`
  - attributes, e.g., `<table id="example-table">`
  - classes, e.g., `<table class="striped-table">`
  - attribute-value pairs, e.g., `<table some_field_name = "123">`

__One or more of these extraction techniques need to be combined to extract information.__

DO: Identifying information on music-to-scrape.org
======

- Open https://music-to-scrape.org/artist?artist-id=ARICCN811C8A41750F
- How would you identify the following information?
  - name of the artist
  - number of song plays on the platform
  - top 10 songs?


DO: Identifying information on books.toscrape.com (fall-back option)
======

- Open https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html
- How would you identify the following information?
  - book title
  - price
  - in-stock availability, and 
  - number of stars?


CSS selectors and XPATHS (I)
========

- When following some extra tutorials, some coders advocate for the use of __CSS selectors and XPATHS__ to extract information.
<br>
- XPATH
  - Think of it as the "path" to the specific data point
  - Example for "price": `html/body/div/div/div[2]/div[2]/article/div[1]/div[2]/p[1]`
  - Likely to break very easily (say, something changes *before* the particular element)
  
CSS selectors and XPATHS (II)
========

- CSS selector
  - Example for extracting "price": `p.price_color:nth-child(2)`
  - Also "highly" dependent: say, tag & class name + position -- can also break
<br>
- So, here, we are sticking mostly to _classes_, _attributes_, and _attribute-value pairs_.

Getting the website into Python
==================
class: small-code

```{python, eval=TRUE}
import requests

# make a get request to the "A Light in the Attic" webpage
url = 'https://music-to-scrape.org/artist?artist-id=ARICCN811C8A41750F'
header = {'User-agent': 'Mozilla/5.0'} # with the user agent, we let Python know for which browser version to retrieve the website
web_request = requests.get(url, headers = header)

# return the source code from the request object
web_request_source_code = web_request.text
```

- Let's save the website in a file and take a look at it.
- Key problem when scraping: What you see while inspecting the site may not always be what you retrieve in Python!


BeautifulSoup 101
================
class: small-code

- Why? Query the HTML code!
- Think of it as a pipeline: download data --> pump to BeautifulSoup --> query data

```{python, eval=TRUE}
import requests
from bs4 import BeautifulSoup
url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'
header = {'User-agent': 'Mozilla/5.0'} 
book_request = requests.get(url, headers = header)
book_request.encoding = book_request.apparent_encoding
soup = BeautifulSoup(book_request.text)
print(soup.find('h1').get_text())
```

- Change the snippet to __show the price__! Tip: use `.find(class_ = 'classname')`

<!--
print(soup.find(class_ = 'price_color').get_text())
-->

DO: BeautifulSoup exercises
=============

- Adapt/extend the code above to extract the following information:
  - product title, price, in-stock availability, and the number of stars.

- Tips
  - `.find(class_='class-name')` for classes
  - `.find(attrs={'name_of_attribute': 'value'})` for attribute-value pairs
  - `len()` for counting
  - you may need to use `.find_all()`

Wrapping code in a function (I)
==================
class: small-code

- Functions are extremely useful to "reuse" code over and over again
  - Functions have a name (say, `def FUNCTIONNAME)
  - ...and arguments (say, `def FUNCTIONNAME(argument1, argument2)`)
<br>
- We can now wrap the code above in a function 
  - `def download_data(url)`
  - the name of the function is `download_data`, and it requires `url` as input

Wrapping code in a function (II)
==================
class: small-code

```{python, eval=TRUE}
import requests
from bs4 import BeautifulSoup

def download_data(url):
  book_request = requests.get(url, headers = {'User-agent': 'Mozilla/5.0'} )
  book_request.encoding = book_request.apparent_encoding
  soup = BeautifulSoup(book_request.text)
  print(soup.find('h1').get_text())
```

DO: Wrap code into a function
==================
class: small-code

1. Adapt the function to also extract the other attributes (i.e., price, etc.)
2. Write a loop to extract data using this function for the following URLs.

```
urls = ['https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',
        'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',
        'https://books.toscrape.com/catalogue/soumission_998/index.html']
```

Tips: 
- Status messages with variables: `print(f'Done retrieving {url}')`


Wrapping results into JSON data
===========

- JSON is most flexible format, supports "hierarchical data"
- Demonstrate how to create object
- "Full" JSON objects, vs. __new-line separated JSON objects__
  - Full: the entire FILE (e.g., `data.json`) is ONE giant JSON object
  - New line separation: each line in your file has one JSON object
  
How to sample? (Challenge #2.2)
==================

- In the demos above, we just used a hard-coded list of URLs
- In practice, getting to that list of books/products (generally speaking: "seeds") is a web scraper in itself!
- Examples
  - scrape all URLs for books from the category overview page; then loop through them an extract information for each book
  - scrape user names from reddit.com; then, use the reddit API to get user metadata
  - ...
  
How to sample? (Challenge #2.2)
==================
- Many challenges in sampling
  - sample size? generalizability? panel attrition?
  - all of the firm's data? or just a little bit? are subjects vulnerable (say, kids)?
  - can we actually "get" data on all subjects? can we match data?
- Solutions and best practices are discussed in "Fields of Gold"

Writing a complete web scraper
==============

- Have list of seeds to start up data collection (here, book urls)
- loop through list of URLs
  - store raw data for diagnostic purposes
  - extract relevant data to JSON file with raw data


Wrapping of web scraping
=====================

- Get content from a website
- Since we don't want "everything", we need to "find" relevant elements on the site using
  - classes
  - attributes
  - attribute-value pairs
- We first build a prototype; then gradually improve by
  - modularizing code as much as possible (using functions, so we can reuse code)
  - ensuring code runs top-down
  

APIs
=====
incremental: true

- Standard way for exchanging data, functions or algorithms
- __Which APIs did you already encounter/explore?__
- Reddit is super easy to use - other APIs require advanced authentication procedures
- Structure corresponds to web scraping
  - have list of seeds (e.g., community names; this was "URLs" for web scraping earlier)
  - store data in new-line separated JSON files (if necessary: convert back to CSV)

Exploring JSON objects on Reddit.com
======
class: small-code

__Please work on exercise 2.3.__

```{python, eval=FALSE}
import requests
url = 'https://www.reddit.com/r/marketing/about/.json'
headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0'}
response = requests.get(url, headers=headers)
json_response = response.json()
```

1. leave out headers - what happens?
2. wrap in while loop & pausing for five seconds: print out active users
3. write `get_usercount()` function

At which frequency to extract the data? (Challenge #2.3)
======

- Here, we extracted data every five seconds.
- But, extraction frequency vastly differs by project
- Considerations
  - archival vs. live data?
  - at which frequency does your phenomenon occur?
  - what's the refresh rate of the data source?
  - any excessive burden on server's caused by frequency of extraction?
  
At which frequency to extract the data? (Challenge #2.3)
======
- Some solutions
  - explore gains of live data collections
  - adhere to best practices (say, 1 req per second)
  - randomize extraction order
  - use automatic schedulers for consistency

Exercises with Reddit
===============

__Please work on exercise 2.4.__

1. wrap in function and extend to 10 subreddits
2. add timestamp to JSON data

Processing data during the collection (challenge #2.4)
========

- Processing can have various degrees
  - just "save" the raw data
  - extract only necessary information
  - choose data format for saving (say, JSON vs. CSV)
- Some selected challenges
  - GDPR vs value in retaining raw data
  - Anonymization or pseudonymization required?
  
Processing data during the collection (challenge #2.4)
========

- Solutions
  - Retain raw when possible
  - Parse minimal amount of data on the fly
  - Remove sensitive info
  - Ensure proper encoding


Next steps in this class
=============

- Work on team activity #1
  - online coaching session next week 
  - make team allocation definite (see Canvas!)
- If you haven't done so
  - go through the self-study material of this week
  - update your 'status' on Pulse!
- Stay engaged
  - discuss ideas, post solutions, identify business opportunities, etc.
  - try fiddling with an AI API or use chat GPT for starting code
  - be in touch on WhatsApp for any issues/bugs/etc.
