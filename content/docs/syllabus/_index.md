---
weight: 1
bookFlatSection: true
title: "Syllabus"
bookHidden: false
---

# Syllabus

## Course structure

- The course consists of 5 modules
- Each module builds up your expertise along one or multiple of the course's learning goals
    - Preparation
      - Prerecorded or live lectures
      - Prerecorded tutorials
      - Live-stream feedback session
    - Activity to implement theory in practice
    - Live-stream feedback session / presentations, etc.
      - Share learnings
    - Advanced content / hangout
      - Discuss advanced issues

- Take in for students: interested in business? interested in academic research? already have some academic research experience? digital meetup


## Module overview

### Module 0: Preparation before the course starts

- Software environment setup (video clip, explaining choice for Python)
- Recommended content to upgrade Python/Jupyter Notebook skills ("bootcamp")

### Module 1: Research and Business Opportunities From Online Data Collections
- Kickoff livestream
  - What is web scraping? What are APIs?
  - How do web scrapers and APIs work?
  - Goals of using web scraping and APIs for...
    - doing academic research
    - developing new business ideas and conducting marketing research

    <!--
    - data enrichment (e.g., ML APIs)
    - data collection and intelligence (e.g., search; chartmetric)
    - market research (e.g., pricewatch)

    -->

  - Break-out activity and discussion: Idea generation for academic research and new business

  <!--

  - Generate groups on the fly: academic versus business; API vs. web scraping; substantive areas in which you're interested; initial research ideas

  - Create break-out groups: 20-30m of discussion
  - Leading questions:
    - What's hot right now?
    - What are important phenomena?
    - What excites you about research?
    - Which websites do you spend a lot of time on?
    - What websites would be cool to monitor today?
    - Have you used an API before?
    - Have you stumbled upon some interesting documentation?

    - What excites you?
    - Search for websites: what do they show?
    - Search for APIs: what do they show? Are they accessible?
    - Why is it an important phenomena, whom does it affect?

  - Come back in the "big room" + quick presentation + feedback

  - The result is a board with websites, and people that are interested in it
  - Subscribe to at least 3 data sources.

  - Together with other team members, fill in the "steckbrief" of these sites

  <!--[split by academic research/ topic, vs. business): generating ideas for potential data sources / phenomena ("what's hot right now? What are important phenomena?", "what excites you about a potential area? what would be cool websites to monitor today?"

  - Discuss: in which area does it fall? are people monitoring that site already? what capture would you hope to achieve? is this more "investment in data?"/"timeliness", or more robustness? is this more a covariate or a key thing?
  -->

  - Brief overview about the course
    - Introduction to the course website and its structure
    - Team survey + group distribution

- Self-study
  - Readings
    - Web scraping article Hannes/Johannes/Abhi/Andrew
    - Ethics in scraping and APIs

  - Video: Assessing research fit of web scraping and APIs [recorded]

  - Preparation for next time
    - Assessing research fit of websites and APIs <!--- phenomena-based; then website-->

- Feedback livestream
    - Present research fit assessment by site/API
    - Team distribution ("RQ"/"business"; assessing fit); refinement of data retrieval strategy on the basis of feedback

<!-- Hybrid teams
-->

<!--(Module 1b: Legality and Terms of Use
paper? advice?))-->

### Module 2: Design principles

- Kickoff livestream
  - Retrieval technology
    - API: package versus self-programmed request library
    - Web scraping: headless versus emulated browser
  - Dynamic versus static/one-shot retrieval
  - One instance versus multiple instances (capacity planning, sample size)
  - Navigation technology (e.g., click, URL) and path mapping (e.g., writing pseudo navigation code)
  - Direct parse versus modular parsing

<!--  - Multiple sites vs. single site; single entity vs. multiple entities
  - Sampling: internal vs. external
-->

- Self-study
  - Tutorial: Web scraping 101
  - Tutorial: Web scraping advanced
  - Exercises for webscraping 101 (e.g., CSS challenge)

- Feedback livestream
  - Discussing exercises for webscraping
    - CSS challenge
    - Navigation paths
  - Finalize teams

  <!--
  Kickoff livestream
  - ...
  - Breakout: Discuss CSS of what you want to get; type of scraper you may need
  - Try out whether to get there, or not
  - Come back in big session: show what you get ,what are core elements, demonstrate this works

- Activity:
  - CSS selector hackathon
  - Navigation challenge + writing down a navigation path

  - Tutorial: API 101

- Exercises: Pseudo-code for navigation challenge


  -->

### Module 3: APIs

- Preparatation
  - API1

- Kickoff lecture

- Feedback livestream in public

- Activity: Endpoint challenge
- Parsing challenge

### Module 4: Data Management and Deployment in Production
  - Software Stack
  - Computing Infrastructure
  - Dockers
  - Structured and Unstructured databases
  - "Polishing" Code

### Module 5: Data sharing

- Presentation of research / business projects

## Meetings

...
