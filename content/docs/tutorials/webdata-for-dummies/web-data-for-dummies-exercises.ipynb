{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web data for dummies (after-class exercises)\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Further deepen your knowledge and skills.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Support Needed?</b> \n",
    "    For technical issues outside of scheduled classes, please check the <a href=\"https://odcm.hannesdatta.com/docs/course/support\" target=\"_blank\">support section</a> on the course website.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Up to this moment, we have only parsed some attributes from the book pages. In this exercise, please try to parse all relevant data from the book pages, e.g.\n",
    "\n",
    "- product description\n",
    "- UPC\n",
    "- number of reviews\n",
    "\n",
    "Use the code written for exercise 1.6 in web data for dummies as a starter.\n",
    "\n",
    "Then, append the time stamp of data collection (used in the API part of the webdata for dummies tutorial), and add it to the generated dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have used Jupyter Notebook to execute our code. But, what if you wanted to *schedule and automatically run* your data extraction (e.g., even when you are asleep)?\n",
    "\n",
    "1. Copy your code written in (3) of exercise 2.4 in web data for dummies to a `.py` file, and execute it \n",
    "from the terminal (`python myscript.py`). \n",
    "\n",
    "2. Work through the scheduling tutorial on [Tilburg Science Hub](https://tilburgsciencehub.com/schedule/task/?utm_campaign=referral-short).\n",
    "\n",
    "3. Combine 1 & 2 to automatically schedule the extraction of the API data, every 10 minutes, for a duration of 2 hours\n",
    "\n",
    "4. Open the downloaded `.json` data using the `pandas` package and provide some summary statistics:\n",
    "    \n",
    "    - number of unique subreddits\n",
    "    - number of times each subreddit was scraped\n",
    "    - start and end timestamp of the scraper\n",
    "    - average active users per subreddit\n",
    "\n",
    "```\n",
    "# snippet to load the data into Python\n",
    "import pandas as pd\n",
    "pd.read_json('subreddits.json', lines = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
