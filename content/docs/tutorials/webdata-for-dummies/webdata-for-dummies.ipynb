{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webdata for Dummies (oDCM)\n",
    "\n",
    "*Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce pretium risus at ultricies egestas. Vivamus sit amet arcu sem. In hac habitasse platea dictumst. Nulla pharetra vitae mauris sed mollis. Pellentesque placerat mauris dui, in venenatis nisl posuere ac. Nunc vitae tincidunt risus, ut pellentesque odio. Donec quam neque, iaculis id eros et, condimentum vulputate nulla. Nullam sed ligula leo.*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to: \n",
    "* Define what web scraping is and the issues surrounding it\n",
    "* Develop strategies for identifying relevant structures in semi-structed data using browser console tools\n",
    "* Utilize Python-based libraries to make request and parse web data\n",
    "* Navigate and access structured web data like HTML, XML, and JSON\n",
    "* Retrieve data from platforms' application programming interfaces (APIs)\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This course draws on online resources built by Brian Keegan, Colt Steele, David Amos, Hannah Cushman Garland, Kimberly Fessel, and Thomas Laetsch. \n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Contact\n",
    "For technical issues try to be as specific as possible (e.g., include screenshots, your notebook, errors) so that we can help you better.\n",
    "\n",
    "**WhatsApp**  \n",
    "+31 13 466 8938\n",
    "\n",
    "**Email**  \n",
    "odcm@uvt.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Learn Web Scraping?\n",
    "\n",
    "Say that you want to store or analyze data from a website. Then of course you can manually copy paste the data from each page but that has several limitations. What if the data on the page gets updated? Or what if there are simply so many pages that you cannot do it all by hand? Web scraping can help you overcome these issues by programmaticaly grabbing data from the web. Before we can extract elements from a website, we need to understand how a page is built up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## HTML basics\n",
    "\n",
    "A web page consists of specifically formatted text files which serve various functions:\n",
    "\n",
    "- `.html` (HyperText Markup Language) files give structure to a page (e.g., menu navigation, body text, footer)\n",
    "- `.css` (Cascading Style Sheet) files determine how the page looks (e.g., colors and fonts)\n",
    "- `.js` (JavaScript) files add interactivity (e.g., animations)\n",
    "\n",
    "Most HTML elements are represented by a pair of tags -- an opening tag and a closing tag. A table, for example, starts with `<table>` and ends with `</table>`. The first tag tells the browser: \"Hey! I got a table here! Render it as a table.\" The closing tag (note the forward slash!) tells the browser: \"Hey! I'm all done with that table, thanks.\" Inside the table are nested more HTML tags representing rows (`<tr>`) and cells (`<td>`).\n",
    "\n",
    "\n",
    "```html\n",
    "<table id=\"example-table\" class=\"striped-table\" style=\"width: 95%\">\n",
    "    <tr> <!-- Header -->\n",
    "        <td>Column A</td>\n",
    "        <td>Column B</td>\n",
    "        <td>Column C</td>\n",
    "    </tr>\n",
    "    <tr> <!-- Row 1 --->\n",
    "        <td>Row 1, Column A</td>\n",
    "        <td>Row 1, Column B</td>\n",
    "        <td>Row 1, Column C</td>\n",
    "    </tr>\n",
    "    <tr> <!-- Row 2 --->\n",
    "        <td>Row 2, Column A</td>\n",
    "        <td>Row 2, Column B</td>\n",
    "        <td>Row 2, Column C</td>\n",
    "    </tr>\n",
    "</table>\n",
    "```\n",
    "\n",
    "HTML elements can have any number of attributes, such as IDs, which uniquely identify elements --\n",
    "\n",
    "```html\n",
    "<table id=\"example-table\">\n",
    "```\n",
    "\n",
    "-- classes, which identify a type of element --\n",
    "\n",
    "```html\n",
    "<table class=\"striped-table\">\n",
    "```\n",
    "\n",
    "-- and styles, which define how specific elements appear --\n",
    "\n",
    "```html\n",
    "<table style=\"width:95%;\">\n",
    "```\n",
    "\n",
    "-- that will be useful to know about when we're scraping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting HTML in your browser\n",
    "\n",
    "You can look at the HTML that makes up any web page by _inspecting the source_ in a web browser. We assume you're using Chrome, if you're using Safari or Mozilla, there are slightly different workflows. \n",
    "\n",
    "#### Inspect element\n",
    "\n",
    "You can inspect specific elements on the page by right-clicking on the page and selecting \"Inspect\" or \"Inspect Element\" from the context menu that pops up. Hover over elements in the \"Elements\" tab to highlight them on the page. This can be helpful when you're trying to figure how to uniquely identify the element you want to scrape.\n",
    "\n",
    "In this exercise we look at the HTML structure of a fictitious [online bookstore](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html). Each of the 1000 books has its own page which shows the title, stock level, star rating, product description, and a table with other product information. Note that the prices and ratings are randomly generated and therefore the figures on your screen may deviate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/inspect.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the screenshot above I selected the book title (\"A Light in the Attic\") with a right-click, and chose \"Inspect\". The same text is highlighted in blue in the HTML code below. The `<h1>` and `</h1>` tags surrounding the title indicate that the text is a header on the web page. Move your pointer down to\n",
    "`<p class=\"price_color\">£51.77</p>` and you'll see that in the top screen it now highlights the price (rather than the title) of the book. \n",
    "\n",
    "This way you can easily investigate through any webpage. As we discussed earlier, tags can be nested within other tags. This also becomes clear from the screenshot below in which the small gray triangles (▶) indicate that there is code hidden within these blocks. Click on them to expand the code and see what's inside. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/html_structure.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises \n",
    "1. Use the inspect tool to find the HTML element that constitutes the table header \"**Number of reviews**\" at the bottom of the page.\n",
    "2. Look up how many elements on the page are associated with the class `sub-header` (within the Inspector screen use `Ctrl+F` on a PC or `⌘+F` on Mac to search)\n",
    "3. You can make local (only on your computer) changes to the web page by double clicking in the inspector and swapping the code for something else. Change the price of the book to £39.95 and assign it a 5 star-rating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting HTML\n",
    "Rather than using the Inspector to look up the source code, we can use Python's `requests` library for that purpose. As this library is not loaded by default, we first need to import it. The total source code contains over 9000 characters, therefore we only print out the product description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        <div id=\"product_description\" class=\"sub-header\">\n",
      "            <h2>Product Description</h2>\n",
      "        </div>\n",
      "        <p>It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more</p>\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# make a get request to the \"A Light in the Attic\" webpage\n",
    "url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "book_request = requests.get(url)\n",
    "\n",
    "# return the source code from the request object\n",
    "book_source_code = book_request.text\n",
    "\n",
    "# print out part of the source code\n",
    "print(book_source_code[5710:6860])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn! Use slicers in the code below so that it prints out the title of the book (without all other shebang)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_source_code[... : ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably noticed that was quite a hassle to extract the right elements from the page. Fortunately, there is a much better way using XPath and CSS Selectors, which we'll discuss next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## XPath & Selectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Single forward slash `/` used to move forward one generation\n",
    "* Double forward slashes `//` used to direct to all elements within the entire HTML code\n",
    "* Tag names between slashes give direction to which element(s)\n",
    "* Brackets [] after a tag name tell us which of the selected siblings to choose.\n",
    "\n",
    "In case the cell below throws a `ModuleNotFoundError` at you, you first need to install the `scrapy` package. Go to your terminal and type `conda install scrapy` and press `y` to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//h1' data='<h1>A Light in the Attic</h1>'>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "html = requests.get(url).content \n",
    "sel = Selector(text=html)\n",
    "\n",
    "sel.xpath(\"//h1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## CSS Locators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Application Programming Interface\n",
    "* Basic Reddit requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/kimfetti/Conferences/tree/master/PyCon_2020\n",
    "* https://www.youtube.com/watch?v=RUQWPJ1T6Zc&t=190s\n",
    "* https://github.com/hancush/web-scraping-with-python/blob/master/session/web-scraping-with-python.ipynb#HTML-basics\n",
    "* https://www.udemy.com/course/the-modern-python3-bootcamp/learn/lecture/7991196#overview\n",
    "* https://campus.datacamp.com/courses/web-scraping-with-python/introduction-to-html?ex=1\n",
    "* https://realpython.com/python-web-scraping-practical-introduction/\n",
    "* https://github.com/CU-ITSS/Web-Data-Scraping-S2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
