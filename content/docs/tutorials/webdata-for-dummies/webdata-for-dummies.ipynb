{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webdata for Dummies (oDCM)\n",
    "\n",
    "*The internet offers abundant possibilities to collect data that can be used in empirical research projects and provide business value. In this tutorial we start off with a gentle introduction to online data collection. What is web scraping? What does API actually stand for, and how does it differ from web scraping? All this and much more, coming up in the Webdata for Dummies tutorial!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to: \n",
    "* Define what web scraping is and the issues surrounding it\n",
    "* Develop strategies for identifying relevant structures in semi-structed data using browser console tools\n",
    "* Utilize Python-based libraries to make request and parse web data\n",
    "* Navigate and access structured web data like HTML and JSON\n",
    "* Retrieve data from platforms' application programming interfaces (APIs)\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This course draws on online resources built by Brian Keegan, Colt Steele, David Amos, Hannah Cushman Garland, Kimberly Fessel, and Thomas Laetsch. \n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Contact\n",
    "For technical issues try to be as specific as possible (e.g., include screenshots, your notebook, errors) so that we can help you better.\n",
    "\n",
    "**WhatsApp**  \n",
    "+31 13 466 8938\n",
    "\n",
    "**Email**  \n",
    "odcm@uvt.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Learn Web Scraping?\n",
    "\n",
    "Say that you want to store or analyze data from a website. Then of course you can manually copy paste the data from each page but that has several limitations. What if the data on the page gets updated? Or what if there are simply so many pages that you cannot do it all by hand? Web scraping can help you overcome these issues by programmaticaly grabbing data from the web. Before we can extract elements from a website, we need to understand how a page is built up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 2. HTML basics\n",
    "\n",
    "### 2.1 Structure \n",
    "A web page consists of specifically formatted text files which serve various functions:\n",
    "\n",
    "- `.html` (HyperText Markup Language) files give structure to a page (e.g., menu navigation, text, tables)\n",
    "- `.css` (Cascading Style Sheet) files determine how the page looks (e.g., colors and fonts)\n",
    "- `.js` (JavaScript) files add interactivity (e.g., page animations)\n",
    "\n",
    "Most HTML elements are represented by a pair of tags -- an opening tag and a closing tag. A table, for example, starts with `<table>` and ends with `</table>`. The first tag tells the browser: \"Hey! I got a table here! Render it as a table.\" The closing tag (note the forward slash!) tells the browser: \"Hey! I'm all done with that table, thanks.\" Inside the table are nested more HTML tags representing rows (`<tr>`) and cells (`<td>`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<html>\n",
    "    <table id=\"example-table\" class=\"striped-table\" style=\"width: 95%\">\n",
    "        <tr> <!-- Header -->\n",
    "            <td>Column A</td>\n",
    "            <td>Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 1 --->\n",
    "            <td>Row 1, Column A</td>\n",
    "            <td>Row 1, Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 2 --->\n",
    "            <td>Row 2, Column A</td>\n",
    "            <td>Row 2, Column B</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This what the rendered HTML table looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <table id=\"example-table\" class=\"striped-table\" style=\"width: 95%\">\n",
    "        <tr> <!-- Header -->\n",
    "            <td>Column A</td>\n",
    "            <td>Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 1 --->\n",
    "            <td>Row 1, Column A</td>\n",
    "            <td>Row 1, Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 2 --->\n",
    "            <td>Row 2, Column A</td>\n",
    "            <td>Row 2, Column B</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "HTML elements can have any number of attributes, such as IDs, which uniquely identify elements --\n",
    "\n",
    "```html\n",
    "<table id=\"example-table\">\n",
    "```\n",
    "\n",
    "-- classes, which identify a type of element (contrary to ids, a class may be used more than once) --\n",
    "\n",
    "```html\n",
    "<table class=\"striped-table\">\n",
    "```\n",
    "\n",
    "-- and styles, which define how specific elements appear (e.g. the width of the table) --\n",
    "\n",
    "```html\n",
    "<table style=\"width:95%;\">\n",
    "```\n",
    "\n",
    "As you may already noticed, we use spaces (or TABs) to separate the elements from one another (a.k.a. indentation) to provide structure and improve readability. For example, the `<table>` tag is placed farther to the right than the `<html>` tag indicates that the table is nested within the HTML block.\n",
    "\n",
    "This may be a lot to take in if you're completely new to HTML, but don't worry as the goal of this section is not to learn you how to code from scratch but rather to teach you what HTML is, and why it is relevant for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "Double click on the rendered table above to edit the HTML structure, finish the exercises below, and run the cell again (`Shift + Enter`) to see the result of your changes.\n",
    "\n",
    "1. Add another row in the table above so that it becomes a 2 (columns) x 4 (rows) table. That is, 3 regular rows and 1 table header row.\n",
    "2. Fill the cells with the corresponding text labels (e.g., Row 3, Column A). \n",
    "3. Change the table width to `50%` so that the table becomes narrower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### 2.2 Inspecting HTML in your browser\n",
    "\n",
    "You can look at the HTML that makes up any web page by _inspecting the source_ in a web browser. We assume you're using Chrome, if you're using Safari or Mozilla, there are slightly different workflows. \n",
    "\n",
    "#### Inspect element\n",
    "\n",
    "You can inspect specific elements on the page by right-clicking on the page and selecting \"Inspect\" or \"Inspect Element\" from the context menu that pops up. Hover over elements in the \"Elements\" tab to highlight them on the page. This can be helpful when you're trying to figure how to uniquely identify the element you want to scrape.\n",
    "\n",
    "In this exercise we look at the HTML structure of a fictitious [online bookstore](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html). Each of the 1000 books has its own page which shows the title, stock level, star rating, product description, and a table with other product information. Note that the prices and ratings are randomly generated and therefore the figures on your screen may deviate from the ones below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/inspect.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the screenshot above I selected the book title (\"A Light in the Attic\"), right-clicked, and chose \"Inspect\". The same text is highlighted in blue in the HTML code below. The `<h1>` and `</h1>` tags surrounding the title indicate that this text is a header on the web page . Move your pointer down to the line below (`<p class=\"price_color\">£51.77</p>`) and you'll see that in the top screen it now highlights the price (rather than the title) of the book. This way you can easily investigate any webpage. \n",
    "\n",
    "As we discussed earlier, tags can be nested within other tags. This also becomes clear from the screenshot below in which the small gray triangles (▶) indicate that there is code hidden within these blocks. Click on them to expand the code, see what's inside, and click again to collapse them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/html_structure.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "1. Use the inspect tool to find the HTML element that constitutes the table header \"**Number of reviews**\" at the bottom of the [page](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html).\n",
    "2. Look up how many elements on the page are associated with the class `sub-header` (within the Inspector screen use `Ctrl+F` on a PC or `⌘+F` on Mac to search)\n",
    "3. You can make local (only on your computer) changes to the web page by double clicking in the inspector and swapping the code for something else (yes, you can overwrite what's already written there!). Change the price of the book to £39.95 and assign it a 5 star-rating. What happens once you refresh the page?\n",
    "\n",
    "<img src=\"images/exercise_inspector.png\" width=40% align=\"left\"  style=\"border: 1px solid black\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Requesting HTML\n",
    "Rather than using the Inspector to look up the source code, we can use Python's `requests` library for that purpose. As this library is not loaded by default, we first need to import it. The total source code contains over 9000 characters, therefore we only print out the product description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        <div id=\"product_description\" class=\"sub-header\">\n",
      "            <h2>Product Description</h2>\n",
      "        </div>\n",
      "        <p>It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more</p>\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# make a get request to the \"A Light in the Attic\" webpage\n",
    "url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "book_request = requests.get(url)\n",
    "\n",
    "# return the source code from the request object\n",
    "book_source_code = book_request.text\n",
    "\n",
    "# print out part of the source code\n",
    "print(book_source_code[5710:6860])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn! Use slicers in the code below so that it prints out the title of the book: *A Light in the Attic* (without all other shebang)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_source_code[... : ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably noticed, that was quite a hassle to extract the right elements from the page. Fortunately, there is a much better way using XPath selectors and CSS locators, which we'll discuss in week 3. For now we'll give you a quick preview of what's coming up next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Introduction to BeautifulSoup \n",
    "This open-source Python library allows you to navigate through and extract data from HTML files using Python. It does NOT gather information from the web, for this we use `requests` as we did above. So first you send a request to a page to gather the data, and then you send it over to Beautifulsoup to extract the information. \n",
    "\n",
    "In the code snippet below we import the package and turn the `book_source_code` (the HTML code from the \"A Light in the Attic\" webpage we used earlier) into a BeautifulSoup object. Once parsed, we can easily navigate the code by tag name. Since we know that the title is surrounded by `<h1>` tags (see Google Inspector screenshot above), we use `soup.find('h1')` to print out the title of the book. Do you see how much easier that is?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>A Light in the Attic</h1>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(book_source_code)\n",
    "print(soup.find('h1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.find()` method will always print out the first matching element that it finds. For example, the web page has two `<h2>` elements which contain the \"Product Description\" and \"Product Information\" subheaders.  Only the first one will be returned by `.find()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Product Description</h2>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('h2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture all matching `<h2>` elements you use the `find_all()` method like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2>Product Description</h2>, <h2>Product Information</h2>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('h2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it now returns a list of elements (`[element1, element2]`), so to access individual elements you need to apply indexing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Product Description</h2>\n",
      "<h2>Product Information</h2>\n"
     ]
    }
   ],
   "source": [
    "# obtain first h2 element \n",
    "print(soup.find_all('h2')[0])\n",
    "\n",
    "# obtain second h2 element\n",
    "print(soup.find_all('h2')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both subheaders are still surrounded by `<h2>` and `</h2>` tags. To get rid of them, append `.get_text()` to your code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Description\n"
     ]
    }
   ],
   "source": [
    "# sub header without h2 tags\n",
    "print(soup.find_all('h2')[0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "1. Collect the HTML table that contains product information (UPC, type, price, tax, etc.) of the \"A Light in the Attic\" book using BeautifulSoup. The output should look like this (\"Â\" has to do with the GBP-pound symbol): \n",
    "\n",
    "```html\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>UPC</th><td>a897fe39b1053632</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Product Type</th><td>Books</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Price (excl. tax)</th><td>Â£51.77</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Price (incl. tax)</th><td>Â£51.77</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Tax</th><td>Â£0.00</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Availability</th>\n",
    "        <td>In stock (22 available)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Number of reviews</th>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "```\n",
    "\n",
    "\n",
    "2. Obtain the tax amount from the table. Tip: you can chain `.find()` and `find_all()` statements, for example: `.find('body').find_all('h1')` returns all `<h1>`s in the `body`. Your code should return the following output: \n",
    "\n",
    "```html\n",
    "Â£0.00 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 3. Application Programming Interface\n",
    "\n",
    "\n",
    "### 3.1 What is an API?\n",
    "Another common data collection method is using an Application Programming Interface (API). That's a mouthful, but in essence it is nothing more than a version of a website intended for computers to talk with one another. Here's an example of what the [output](https://www.reddit.com/r/science/comments/k0bjqt/study_finds_users_not_notifications_initiate_89.json) of an API may look like: \n",
    "\n",
    "<img src=\"images/api_example.png\" width=70% align=\"left\"  style=\"border: 1px solid black\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things stand out right away: it only contains text which is structured according to a data structure (e.g., list (`[]`) and dictionary (`{}`)), there's no human interface with buttons, menus, and links, yet you can access it like any other website by filling out the URL in your browser (`reddit.com/r/science/...` in this example). In fact, the API output above corresponds to the following [Reddit webpage](https://www.reddit.com/r/science/comments/k0bjqt/study_finds_users_not_notifications_initiate_89)  (i.e., an American social news aggregation and discussion site). For example, have a look at the third and fourth line from above which states the `title` of the post you see below.  \n",
    "\n",
    "<img src=\"images/reddit.png\" width=70% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have taken a quick look at the [API output](https://www.reddit.com/r/science/comments/k0bjqt/study_finds_users_not_notifications_initiate_89/.json), you may have come to the conclusion that making sense of raw JavaScript Object Notation (JSON) is easier said than done. Fortunately, this [plugin](https://chrome.google.com/webstore/detail/json-viewer/gbmdgpbipfallnflgajpaliibnhdgobh) automatically formats and highlights the output such that it's easier to digest. For the following exercise, we therefore highly recommend installing the Chrome plugin. Alternatively, you can copy paste the JSON output into this [online viewer](http://jsonviewer.stack.hu) and inspect the \"Viewer\" tab. \n",
    "\n",
    "#### Exercise 5\n",
    "Navigate through the JSON tree structure of the post above and anwer the following questions:\n",
    "1. At the parent level you find two dictionaries at line 5 and 197 (i.e., the blue arrows). Collapse the content and describe in your own words what each dictionary represents. How does it relate to the Reddit HTML page? \n",
    "\n",
    "<img src=\"images/json_viewer.png\" width=70% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suppose that you want to extract the date of each comment. What path do you need to navigate? Note that times are often registered in UTC format which is a common time scale used across the world.\n",
    "3. The first comment is from the post author (fotogneric) and has gathered the most points. How many downvotes did this comment get (you find the answer in the JSON output)? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand what APIs are, you may rightfully wonder: why should I learn APIs when I could scrape the elements from the website instead (like the book webshop)? One of the major advantages of APIs is that you can directly access the data you need without all the hassle of selecting the right HTML tags. Another advantage is that you can often customize your API request (e.g., the first 100 comments, or only posts about science) which may not always be possible in the web interface. That's why we recommend using an API whenever possible. However, in practise, you may find yourself in a scenario where there is no API to access the data you want, or the access it too limited. In these scenarios, web scraping would allow you to access the data as long as it is available on a website.\n",
    "\n",
    "In week 3, we'll learn you how to extract data from the Reddit API. For now, we'll look at a somewhat simpler example to get you started with APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Open an API endpoint\n",
    "[icanhazdadjoke.com](https://icanhazdadjoke.com) is the largest selection of dad jokes on the internet. Every time you visit the  webpage, it shows you a random joke. The reason we picked this website is that we can call the API without providing authentication tokens. This means that you can collect jokes without creating an account, and that the API does not ask for a secret key or token (i.e., password to tell the API: \"this is user X requesting data about Y!\"). In general, there are limits to how often or how much data you can request. Therefore, most commercial APIs require authentication so that they can track your usage of the API and verify whether that you're a paid subscriber or not. \n",
    "\n",
    "<img src=\"images/icanhazdadjoke.png\" width=70% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 2.3 on web scraping, we requested the HTML data from the \"A Light in the Attic\" and saved it to a variable `book_request`. Here we also use the `requests` library, but this time we add `headers={\"Accept\": application/json}\"` to our request so that we do not get back the raw HTML but rather the output in JSON. Thereafter, we return the JSON from the request object (`response.json()`). Each `joke_request` object contains three attributes: \n",
    "* `id` = a unique identifier for each joke\n",
    "* `joke` = the text of the joke\n",
    "* `status` = the HTML status code (200 indicates a successful request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'fNmOm3Ediyd', 'joke': 'What do you call a dictionary on drugs? High definition.', 'status': 200}\n"
     ]
    }
   ],
   "source": [
    "# request JSON output from icanhazdadjoke API\n",
    "url = \"https://icanhazdadjoke.com\"\n",
    "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
    "joke_request = response.json() \n",
    "print(joke_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "1. What happens if you run the cell above again? Why is that? \n",
    "2. You can extract the text of the joke as follows: `joke_request['joke']`. Revise the code snippet above such that it stores the text of 10 jokes in a list (tip: use a for-loop).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, it is common use to customize requests so that the API returns the exact data you need. You have probably already done this a dozen times without even knowing it. For example, if you Google the word \"cat\", the results page may look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/google.png\" width=70% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the link in the browser starts off with [`google.com/search?q=cat...`](https://www.google.com/search?q=cat). Thus, the search query `cat` is already embedded in the link itself. So rather than filling out the search box on the webpage itself, you can also tweak it in the URL directly. In a similar way, you can request `cat` jokes from the [`icanhazdadjoke.com/search`](https://icanhazdadjoke.com/search?term=cat) page with the `term` parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/search.png\" width=70% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this idea in mind, we can update the `search_url` and include the `params` attribute which contains a dictionary with parameters that further specifies our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://icanhazdadjoke.com/search\"\n",
    "\n",
    "response = requests.get(search_url, \n",
    "                        headers={\"Accept\": \"application/json\"}, \n",
    "                        params={\"term\": \"cat\"})\n",
    "joke_request = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `joke_request` object contains a list with all cat-related jokes (`joke_request['results']`), the search term (`cat`), and the total number of jokes (`10`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "1. Change the search term parameter to `dog` and revisit `joke_request['results']`. How many dog jokes are there? \n",
    "2. Write a function `find_joke()` that takes a query as input parameter and returns the first related joke from the `icanhazdadjoke` search API. \n",
    "3. Currently, the API provides 650 jokes in total. You can check that for yourself by passing an empty string (`\"\"`) as a search `term`. The results object only shows you the first 20 jokes. To view the remaining 630 jokes you need pagination: rather than returning all output at once, the API divides the data into subsets which can be accessed on various pages. By default each page contains 20 jokes, where page 1 shows joke 1 to 20, page 2 jokes 21 to 40, ..., and page 33 jokes 641 to 650. You can adjust the number of results on each page (max. 30) with the `limit` parameter (e.g., `params={\"term\": \"\", \"limit\": 10}`). Choose different levels for limit (e.g., `10`, `20`, and `30`) and see how it affects the `total_pages` field. Why is that? \n",
    "4. You can specify the current page number with the `page` parameter (e.g., `params={\"term\": \"\", \"page\": 2}`. Adapt the function `find_joke()` such that it loops over all available pages and stores the jokes in a list. You can leave the `limit` parameter at its default value (20). Make sure that your function also works when you pass it a search `term`. Tip: to determine how many page you need to loop through, you can use `total_pages` field (e.g., there are only 10 cat jokes, so in that case 1 page would suffice). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
