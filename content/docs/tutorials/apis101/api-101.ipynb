{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API 101 (oDCM)\n",
    "\n",
    "*The focus in this tutorial lies on pagination and parameters in the context of APIs. We know you love dad jokes, so guess what? We're back with many more jokes, and you're going to learn how to save them all! Thereafter, we show you how to conduct a user-level analysis of activity on Reddit!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to: \n",
    "* Send HTTP requests to retrieve data from APIs\n",
    "* Understand and apply request parameters\n",
    "* Iterate over multiple pages \n",
    "* Extract and store results of API request\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This course draws on a variety of online resources which can be retrieved from the [course website](https://odcm.hannesdatta.com/#student-profile--prerequisites). \n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Support Needed?\n",
    "For technical issues outside of scheduled classes, please check the [support section](https://odcm.hannesdatta.com/docs/course/support) on the course website.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. icanhazdajoke\n",
    "### 1.1 Use parameters to modify the API results   \n",
    "As you may remember you can customize requests so that the API returns the *exact* data you need. You have probably already done this a dozen times without even knowing it. For example, if you Google the word `cat`, the results page may look something like this:\n",
    "\n",
    "<img src=\"images/google.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the link in the browser starts off with [`google.com/search?q=cat...`](https://www.google.com/search?q=cat). Thus, the search query `cat` is already embedded in the link itself. Cool, right?\n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "So, rather than filling out the search box on the web page itself, you can also tweak it in the URL directly. Try it!\n",
    "\n",
    "In a similar way, you can request `cat` jokes from the [`icanhazdadjoke.com/search`](https://icanhazdadjoke.com/search?term=cat) page with the `term` parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cat_jokes.gif\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this idea in mind, we can update the `search_url` and include the `params` attribute which contains a dictionary with parameters that further specify our request. Run the cell below to see cat jokes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'current_page': 1, 'limit': 20, 'next_page': 1, 'previous_page': 1, 'results': [{'id': '8UnrHe2T0g', 'joke': '‘Put the cat out’ … ‘I didn’t realize it was on fire'}, {'id': 'iGJeVKmWDlb', 'joke': 'My cat was just sick on the carpet, I don’t think it’s feline well.'}, {'id': 'daaUfibh', 'joke': 'Why was the big cat disqualified from the race? Because it was a cheetah.'}, {'id': '1wkqrcNCljb', 'joke': \"Did you know that protons have mass? I didn't even know they were catholic.\"}, {'id': 'BQfaxsHBsrc', 'joke': 'What do you call a pile of cats?  A Meowtain.'}, {'id': 'O7haxA5Tfxc', 'joke': 'Where do cats write notes?\\r\\nScratch Paper!'}, {'id': 'TS0gFlqr4ob', 'joke': 'What do you call a group of disorganized cats? A cat-tastrophe.'}, {'id': '0wcFBQfiGBd', 'joke': 'Did you hear the joke about the wandering nun? She was a roman catholic.'}, {'id': 'AQn3wPKeqrc', 'joke': 'It was raining cats and dogs the other day. I almost stepped in a poodle.'}, {'id': '39Etc2orc', 'joke': 'Why did the man run around his bed? Because he was trying to catch up on his sleep!'}], 'search_term': 'cat', 'status': 200, 'total_jokes': 10, 'total_pages': 1}\n"
     ]
    }
   ],
   "source": [
    "search_url = \"https://icanhazdadjoke.com/search\"\n",
    "\n",
    "response = requests.get(search_url, \n",
    "                        headers={\"Accept\": \"application/json\"}, \n",
    "                        params={\"term\": \"cat\"})\n",
    "joke_request = response.json()\n",
    "print(joke_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `joke_request` object now contains a list with all cat-related jokes (`joke_request['results']`), the search term (`cat`), and the total number of jokes (`10`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "1. Change the search term parameter to `dog` and revisit `joke_request['results']`. How many dog jokes are there? \n",
    "2. Write a function `find_joke()` that takes a query as an input parameter and returns the number of jokes from the `icanhazdadjoke` search API. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 \n",
    "search_url = \"https://icanhazdadjoke.com/search\"\n",
    "\n",
    "response = requests.get(search_url, \n",
    "                        headers={\"Accept\": \"application/json\"}, \n",
    "                        params={\"term\": \"dog\"})\n",
    "joke_request = response.json()\n",
    "print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "def find_jokes(term):\n",
    "    search_url = \"https://icanhazdadjoke.com/search\"\n",
    "\n",
    "    response = requests.get(search_url, \n",
    "                            headers={\"Accept\": \"application/json\"}, \n",
    "                            params={\"term\": term})\n",
    "    joke_request = response.json()\n",
    "    num_results = joke_request['total_jokes']\n",
    "    return num_results\n",
    "\n",
    "find_jokes(\"asdfasdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the API provides about 649 jokes in total. You can check that for yourself by passing an empty string (`\"\"`) as a search `term` in the web interface (or in the `find_jokes()` function of course!). \n",
    "\n",
    "The API output, however, only shows you the first 20 jokes. To view the remaining 629 jokes, you need pagination. That is, the API divides the data into subsets that can be accessed on various pages, rather than returning all output at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of jokes is 649\n"
     ]
    }
   ],
   "source": [
    "print(f'The total number of jokes is {find_jokes(\"\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, each page contains 20 jokes, where page 1 shows jokes 1 to 20, page 2 jokes 21 to 40, ..., and page 33 jokes 641 to 649. You can adjust the number of results on each page (max. 30) with the `limit` parameter (e.g., `params={\"limit\": 10}`). In the example below, we set `limit` equal to `10`, `20`, and `30` and see how it affects the number of pages. As expected we find that the higher the limit, the more results fit on a single page, and thus the lower the number of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit 10 gives 65 pages\n",
      "Limit 20 gives 33 pages\n",
      "Limit 30 gives 22 pages\n"
     ]
    }
   ],
   "source": [
    "for limit in range(10, 31, 10):\n",
    "    response = requests.get(search_url, \n",
    "                            headers={\"Accept\": \"application/json\"}, \n",
    "                            params={\"term\": \"\", \n",
    "                                   \"limit\": limit})\n",
    "    joke_request = response.json()\n",
    "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Exercise 2\n",
    "\n",
    "In addition to the `limit` parameter, you can specify the current page number with the `page` parameter (e.g., `params={\"term\": \"\", \"page\": 2}`. \n",
    "\n",
    "Adapt the function `find_joke()` such that it loops over all available pages and stores the ids and jokes in a list. You can leave the `limit` parameter at its default value (20). Make sure that your function also works when you pass it a search `term`. Tip: to determine how many pages you need to loop through, you can use the `total_pages` field (e.g., there are only ten cat jokes, so in that case, 1 page would suffice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jokes(term):\n",
    "    search_url = \"https://icanhazdadjoke.com/search\"\n",
    "    page = 1\n",
    "    jokes = []\n",
    "\n",
    "    while True: \n",
    "        response = requests.get(search_url, \n",
    "                                headers={\"Accept\": \"application/json\"}, \n",
    "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
    "                                        \"page\": page})\n",
    "        joke_request = response.json()\n",
    "        jokes.extend(joke_request['results'])\n",
    "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
    "            page += 1\n",
    "        else: \n",
    "            return jokes\n",
    "\n",
    "output = find_jokes(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Wrap-up\n",
    "To sum up, we have seen how parameters can be a powerful tool when working with APIs. They allow you to tailor your request to be more specific, loop through multiple pages, and many other applications. In the API documentation you find more information about the available parameters and the values they can take on. For example, the `icanhazdadjoke` [documentation](https://icanhazdadjoke.com/api) includes a section on the `/search` endpoint and the accepted parameters (`page`, `limit`, `term`). These parameters, however, differ from one API to another as you will see in the next section about the Reddit API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Reddit\n",
    "\n",
    "### 2.1 Subreddits\n",
    "\n",
    "Although we already touched upon the Reddit API last time, we'll provide a more thorough description of subreddits here as this entire tutorial is devoted to getting started with the API. Users can post content in subreddits which are niche communities around a specific topic. There is a subreddit for almost everything, and they all start with `reddit.com/r/...`, for example, [askreddit](https://www.reddit.com/r/AskReddit), [aww](https://www.reddit.com/r/aww/), [gifs](https://www.reddit.com/r/gifs/), [showerthoughts](https://www.reddit.com/r/Showerthoughts), [lifehacks](https://www.reddit.com/r/lifehacks), [getmotivated](https://www.reddit.com/r/GetMotivated), [moviedetails](https://www.reddit.com/r/MovieDetails), [todayilearned](https://www.reddit.com/r/todayilearned/), or [foodporn](https://www.reddit.com/r/FoodPorn/). \n",
    "\n",
    "<img src=\"images/reddit_science.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subreddits are hosted by moderators and come with their own set of rules (e.g., links to papers you share in [`r/science`](https://www.reddit.com/r/science/) must be less than 6 months old). Other users can join a subreddit so that they receive updates about new posts and comments.\n",
    "\n",
    "<img src=\"images/reddit_moderators.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "Consult the [`marketing`](https://www.reddit.com/r/marketing/hot/) subreddit and answer the following questions: \n",
    "1. For your thesis, you need to collect a couple more survey responses. Are you allowed to share a link to your survey in this subreddit? Please elaborate on how you came to this conclusion. \n",
    "2. You're a bit stubborn and decide to do it anyway and therefore run the risk of being reported by one of the moderators. How many moderators take care of managing this subreddit? \n",
    "3. Like other social media platforms, you can navigate towards Reddit's user-profiles and learn more about these persons. Inspect the profile of one of the moderators of the marketing subreddit, [`sixwaystop313`](https://www.reddit.com/user/sixwaystop313), and describe in your own words what types of information you can gather from this user. How is the feed organized? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. No, the subreddit rules prescribe users not to post surveys and homework assignments (right sidebar).\n",
    "2. `r/marketing` is moderated by 10 users (of which 1 AutoModerator)\n",
    "3. On a user page you find the bio, trophies, communities the user moderates, connected accounts, and most importantly: all user's posts and comments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 API headers  \n",
    "\n",
    "\n",
    "**Importance**  \n",
    "\n",
    "To request data from the Reddit API we need to include so-called `headers` in our request. HTTP headers are an important part of the API request as they include meta-data associated with the request (e.g., type of browser, language, expected data format, etc.). \n",
    "\n",
    "**Let's try it out**  \n",
    "\n",
    "Below we make a request to the moderators' page of the [`marketing`]() subreddit that includes such a header. In the upcoming exercise, we make our very first request to the Reddit API and parse the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.reddit.com/r/marketing/about/moderators/.json'\n",
    "\n",
    "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=0', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
    "response = requests.get(url, headers=headers)\n",
    "json_response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "1. First, take a look at the `json_response` object. Then, leave out the `headers` parameter in your request, run the cell again, and inspect the `json_response` another time. Are there any differences? \n",
    "2. Write a for-loop that prints the moderator `name` of the `marketing` subreddit. Every subreddit includes a bot moderator (`AutoModerator`) which should not be included.\n",
    "3. Convert your code from the previous exercise into a function `get_mods()` that takes a `subreddit` as input and returns a list of moderators' names. Test your function for the `science` subreddit. How many moderators does it have? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. Without the `headers` parameter, it returns an error code (429)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpatrick86\n",
      "v022450781\n",
      "r0nin\n",
      "Gustomaximus\n",
      "everythingswan\n",
      "sixwaystop313\n",
      "shampine\n",
      "JonODonovan\n",
      "AptSeagull\n"
     ]
    }
   ],
   "source": [
    "# Question 2 \n",
    "# don't forget to run the request object with headers again!\n",
    "for item in json_response['data']['children']: \n",
    "    moderator_name = item['name']\n",
    "    if moderator_name != 'AutoModerator': \n",
    "        print(moderator_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `science` subreddit is moderated by 1545 users\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "def get_mods(subreddit):\n",
    "    moderator_names = []\n",
    "    response = response = requests.get(f'https://www.reddit.com/r/{subreddit}/about/moderators/.json', headers=headers)\n",
    "    json_response = response.json()\n",
    "    for item in json_response['data']['children']:\n",
    "        moderator_name = item['name']\n",
    "        if moderator_name != 'AutoModerator':\n",
    "            moderator_names.append(moderator_name)\n",
    "    return moderator_names\n",
    "    \n",
    "science_moderators = get_mods('science')\n",
    "print(f\"The science subreddit is moderated by {len(science_moderators)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3 Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance**  \n",
    "\n",
    "In addition to subreddits (`r/...`) and moderator pages (`.../about/moderators`), Reddit users have their own profile page. Let's have another look at the marketing moderator [profile](https://www.reddit.com/user/sixwaystop313) we saw before. Each of the `children` in the `data` is characterized by a type (e.g., `t1` = comment, `t3` = post), subreddit, timestamp, number of comments, upvotes, downvotes, and many others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = \"sixwaystop313\"\n",
    "response = requests.get(f'https://www.reddit.com/user/{mod}.json', headers=headers)\n",
    "json_response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "1. The `json_response` object contains both comments and posts that are ordered chronologically (as they appear on the profile page). Pick a comment of the author (`kind`: `'t1'`) and store the text of the comment in a variable called `comment_text`. \n",
    "2. What happens to `comments_text` once the author publishes another post? \n",
    "3. How many objects are stored in `json_response['data']['children']`? What does that mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. At the moment of creating this solutions file, the 1st item in the list is a comment which we extract as follows:\n",
    "`comment_text = json_response['data']['children'][0]['data']['body']`. In your case, it may be 2nd (or 3rd, 4th, ... item), however, provided that all other items in the lists are posts. For that reason, the counter after `[0]` may deviate from time to time. \n",
    "2. Since the list items are ordered chronologically, new items are appended at the beginning of the list and thus push existing items to the \"right\" (i.e., index 0 becomes index 1, etc.). Suppose that the author publishes another post, then index `[0]` would no longer contain a comment. And because post items have been structured differently than comment items this could potentially break your script once you try to parse not existing items. For example, posts do not have a `['body']` element that stores the comment text. \n",
    "3. The object comprises 25 items (`len(json_response['data']['children'])`). This means that only the 25 most recent comments and posts are shown, and thus that we need to apply pagination to obtain historical records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you just noticed, the API only returns a subset of all records (every time you scroll to the bottom of the page, it pulls in new data - ordered chronologically). After all, it would take ages to show all data for a user that has been active on Reddit since 2009! \n",
    "\n",
    "Similar to `icanhazdadjoke` we apply pagination to tell the API which part of the data it needs to return. The difference, however, is that it's not a number (like `\"page\": 2`) but a string of characters that can only be obtained from the previous request (i.e., we cannot derive  what the next key will be from a pattern, like: page 2, 3, ..., etc.). In fact, the request we already made contains this \"secret\" key in the attribute `after`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_k20wsy'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response['data']['after']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we attach this key to our request with the `after` parameter to obtain the next subset of items and assign the responses to a variable called `json_response_after`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = json_response['data']['after']\n",
    "url = f'https://www.reddit.com/user/{mod}.json'\n",
    "response = requests.get(url, \n",
    "                        headers=headers, \n",
    "                        params={\"after\": after})\n",
    "json_response_after = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the point of writing this tutorial, the last item in `json_respose` is the following post (`Detroit's Brewing Heritage' on tap at Historical Museum`): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/json_response.png\" width=60% align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and second item in `json_response_after` are the two comments below that (\"Shame on ... us back.\" and \"Are you ... comment /u/ehchip\"). In other words, where one object ends, another begins. We apply this concept to loop over the first 10 pages. Every time we store the value of the `after` attribute which we use as a parameter in the follow-up request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = None\n",
    "item_type = []\n",
    "\n",
    "for counter in range(10): \n",
    "    url = f'https://www.reddit.com/user/{mod}.json'\n",
    "    response = requests.get(url, \n",
    "                            headers=headers, \n",
    "                            params={\"after\": after})\n",
    "    json_response = response.json()\n",
    "    after = json_response['data']['after']\n",
    "\n",
    "    # loop over all items in a request\n",
    "    for item in json_response['data']['children']:\n",
    "        item_type.append(item['kind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "1. Why do we define `after = None` at the top of the file? Can we leave it out? \n",
    "2. Without looking at the length of the list: how many items do you expect in `item_type`? \n",
    "3. Of those items, calculate the the percentage of posts (`t3`) and comments (`t1`). What does this tell you? \n",
    "4. Convert the code snippet above into a function `reddit_activity()` that takes a `username`, `attribute`, and `num_pages` as input and returns the attribute for the given user. For example, `reddit_activity(\"sixwaystop313\", \"subreddit_name_prefixed\", 40)` should return a list of the subreddits in which the user has posted or commented across the 1000 most recent items. Has this moderator actively contributed to the `r/marketing` subreddit recently? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions \n",
    "1. In our first request we don't know the value of `after` yet. It is important, however, to include this line because otherwise the `after` value in `params={}` is undefined. \n",
    "2. We expect the list to have a size of 10 (number of requests) * 25 (number of items per request) = 250. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of posts and comments is 64.4% and 35.6%, respectively\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "def item_frequency(items, item_filter):\n",
    "    total_items = len(items)\n",
    "    item_filter_count = items.count(item_filter)\n",
    "    return item_filter_count / total_items * 100\n",
    "            \n",
    "perc_posts = item_frequency(item_type, 't1')\n",
    "perc_comments = item_frequency(item_type, 't3')\n",
    "\n",
    "print(f\"The percentage of posts and comments is {perc_posts}% and {perc_comments}%, respectively\")\n",
    "# Thus, based on this subset of data, the author is more likely to start a new post than to comment on others' posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of posts and comments in the marketing subreddit is 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "def reddit_activity(username, attribute, num_pages):\n",
    "    after = None\n",
    "    activity = []\n",
    "\n",
    "    for counter in range(num_pages): \n",
    "        url = f'https://www.reddit.com/user/{username}.json'\n",
    "        response = requests.get(url, \n",
    "                                headers=headers, \n",
    "                                params={\"after\": after})\n",
    "        json_response = response.json()\n",
    "        after = json_response['data']['after']\n",
    "\n",
    "        # loop over all items in a request\n",
    "        for item in json_response['data']['children']:\n",
    "            activity.append(item['data'][attribute])\n",
    "    return activity\n",
    "\n",
    "reddit_data = reddit_activity(\"sixwaystop313\", \"subreddit_name_prefixed\", 40)\n",
    "print(f\"The percentage of posts and comments in the marketing subreddit is {item_frequency(reddit_data, 'r/marketing')}%\")\n",
    "# Thus, the moderator has not actively contributed to the marketing subreddit recently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.4 Time Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both posts and comments contain the `created_utc` attribute which a timestamp that indicates the number of seconds since 1970. With the use of the `time` library we can easily convert it into a readable date and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2020, tm_mon=7, tm_mday=24, tm_hour=6, tm_min=17, tm_sec=14, tm_wday=4, tm_yday=206, tm_isdst=0)\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "time_example = 1595571434\n",
    "time_converted = time.gmtime(time_example)\n",
    "print(time_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `time_converted` you can extract the day, month, and year separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The day is: 24\n",
      "The month is: 7\n",
      "The year is: 2020\n"
     ]
    }
   ],
   "source": [
    "print(f\"The day is: {time_converted.tm_mday}\")\n",
    "print(f\"The month is: {time_converted.tm_mon}\")\n",
    "print(f\"The year is: {time_converted.tm_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or together, like this (characters that start with `%` have a special meaning, the `-` in  between these characters are literally the dashes you see in the output): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-07-2020\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%d-%m-%Y\", time_converted))  \n",
    "# %d = day\n",
    "# %m = month\n",
    "# %Y = year (4 digits) and %y = year (2 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7 \n",
    "1. In a similar way, you can convert the UTC time into an hour (`%H`) and minute (`%M`). Transform `time_example` into a readable time. The output should be `06:17`. \n",
    "2. Suppose we want to analyze the Reddit use of `sixwaystop313` throughout the day. More specifically, we want to know during what hours he is most active on the platform. \n",
    "  * Use the function `reddit_activity()` you wrote earlier to pull in the UTC timestamps (set `num_items` to `10`). \n",
    "  * Extract the hour from these timestamps. \n",
    "  * Determine the top 3 hours the user is most active on Reddit. You can assume that the total number of posts and comments is a reasonable proxy for time spend on the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:17\n"
     ]
    }
   ],
   "source": [
    "# Question 1 \n",
    "print(time.strftime(\"%H:%M\", time_converted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour 0: 16 items\n",
      "Hour 1: 22 items\n",
      "Hour 2: 16 items\n",
      "Hour 3: 21 items\n",
      "Hour 4: 25 items\n",
      "Hour 5: 6 items\n",
      "Hour 6: 2 items\n",
      "Hour 7: 0 items\n",
      "Hour 8: 3 items\n",
      "Hour 9: 0 items\n",
      "Hour 10: 1 items\n",
      "Hour 11: 3 items\n",
      "Hour 12: 4 items\n",
      "Hour 13: 8 items\n",
      "Hour 14: 12 items\n",
      "Hour 15: 15 items\n",
      "Hour 16: 17 items\n",
      "Hour 17: 10 items\n",
      "Hour 18: 7 items\n",
      "Hour 19: 10 items\n",
      "Hour 20: 15 items\n",
      "Hour 21: 7 items\n",
      "Hour 22: 14 items\n",
      "Hour 23: 16 items\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "time_data = reddit_activity(\"sixwaystop313\", \"created_utc\", 10)\n",
    "hours = []\n",
    "\n",
    "for timestamp in time_data: \n",
    "    time_converted = time.gmtime(timestamp)\n",
    "    hours.append(time_converted.tm_hour)\n",
    "    \n",
    "for hour in range(24):\n",
    "    print(f\"Hour {hour}: {hours.count(hour)} items\")\n",
    "    \n",
    "# For this dataset the top 3 hours are: 1, 3, and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Wrap-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working on this set of exercises you should be able to further explore the Reddit API on your own. Does `sixwaystop313` spend most time in subreddits in which he get the most upvotes? Did his posting behavior change over time? Are moderators more likely to be a premium Reddit user. Give it a try! \n",
    "\n",
    "At the same time, you should realize that we have only scratched the surface of what's possible. Headers and pagination play an important role in requests and were sufficient thus far, yet the majority of [API endpoints](https://www.reddit.com/dev/api/) require authentication (oauth) which is a whole topic on its own. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
