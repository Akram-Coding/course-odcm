oDCM - APIs 101 (Tutorial)
========================================================
author: Hannes Datta
date:
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

<!--#

https://support.rstudio.com/hc/en-us/articles/200486468
-->

```{r setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(reticulate)
py_install("requests")
```

Welcome to oDCM!
========================================================

We're about to start with __today's tutorial ("API 101")__.

- If you haven't done so, open slides at https://odcm.hannesdatta.com/docs/tutorials/apis101/
- Today's (live) code is available at [tiu.nu/livecoding]() (refresh needed)
- You can work in Google Colab throughout this class (except for Reddit...)


<!--
- Login at [http://pulse.tilburg-digital.com]() to explore this week's to do's
-->

Before we start
========================================================

- Tech development & extra material
  - Check "advanced" tutorials for extra code snippets + selenium
  - [Legality of scraping](https://htmlpreview.github.io/?https://github.com/hannesdatta/legal-situation-of-web-scraping/blob/main/legal-status-of-web-scraping.html#/title-slide)
    - Use of ChatGPT: experience?
- Team projects
  - enthusiastic about many of your ideas
  - get started building your prototypes
- Pulse: great to see so many using it -- feedback appreciated
<br>
- Any other issues?

<!--
- API authentication will be pain point - highly idiosyncratic!
-->

Agenda
=======
- In-class 
  - "tutorial-style API" -- https://icanhazdadjoke.com/
  - real life w/ Reddit -- https://reddit.com
- After class (complete tutorial yourselves)

What are APIs?
============

- APIs are connectors within and between platforms and databases, using the web to talk to each other
  - backend to frontend: e.g., database <-> Reddit API <-> website
  - backend to backend: e.g., Twitter <-> Twitter API <-> some trading software
- Mostly made for developers, rarely for researchers
- Available usually only after registration and approval

Technical differences between APIs and web scraping
============

- Web scraping
  - not always 100% clear whether allowed
  - have to structure (unstructured) HTML code
<br>
- API
  - good if you have permission (but, be realistic!)
  - *structured* JSON data

Discovering an API documentation
=========

- Probably the most difficult part w/ APIs is to get authenticated
  - recall: need platform's permission (but this API doesn't require authentication)
- Find an endpoint (aka page)

__Do:__ Browse the [documentation](https://icanhazdadjoke.com/api) of the dad joke API. 

1) Describe which endpoints are available. What do they do?

2) Can you come up with reasons why these endpoints are available?

Do: Making a connection with an API
=================
class: small-code
  
Run the snippet below (and again, and again)

```{python, eval=FALSE}
import requests
search_url = "https://icanhazdadjoke.com/search"
response = requests.get(search_url, 
                        headers={"Accept": "application/json"}, 
                        params={"term": "cat"})
joke_request = response.json()
joke_request
```

1. Can you explain what happens when you run this cell (multiple times)?
2. Modify the search parameter (`term`) to search for `dog` jokes (rather than `cat` jokes!)

How can we make sense of JSON data?
========

- JSON data is structured in a hierarchy
  - use [JSONviewer.stack.hu](http://jsonviewer.stack.hu) to make sense of it
  - alternatively, write to `.json` file and view in VS Code
  - many other possibilities exist (e.g., in-browser) - check what you like best!
  
__DO:__ Run this snippet to write JSON output to a file! Inspect it using [JSONviewer](http://jsonviewer.stack.hu) or VS Code!

```{python, eval=FALSE}
import json
f=open('jokes.json', 'w', encoding='utf-8')
f.write(json.dumps(joke_request))
f.close()
```

Understanding parameters in API calls
===============

- Many ways to "submit" parameters to an API
  - directly in URL, e.g., `myapi.com/search/cats_and_dogs`
  - w/ parameters in URL, e.g., `myapi.com/search/?query=cats_and_dogs`
  - in __header__ of request (think of it like the address [=header] on an envelop [=data])

<br><br>

__The API documentation will tell you what is required!__

Do: Iterating through API endpoints
=========
class: small-code

- Remember iterating through pages on a website to "view" data? APIs know the same concept!

1. Run the snippet below and say how many jokes there are available.
2. How many pages you would have to iterate through to capture all jokes?
3. Print the number of pages from (2) to the screen.

```{python, eval=FALSE}
import requests
search_url = "https://icanhazdadjoke.com/search"
response = requests.get(search_url, headers={"Accept": "application/json"})
joke_request = response.json()
print(joke_request)
```

Understanding how to iterate
======
class: small-code

First step is always to understand how to iterate. So let us try it out!

```{python, eval=FALSE}
import requests
search_url = "https://icanhazdadjoke.com/search"
response = requests.get(search_url, 
                        headers={"Accept": "application/json"},
                        params={"iteration": 1})
joke_request = response.json()
print(joke_request)
```

__DO:__ 

- Try to add 1 to the `iteration` parameter to verify it works!
- Re-check the [documentation](https://icanhazdadjoke.com/api) for the endpoint `/search` -- and then debug the code snippet!

Looping through all pages of an endpoint
=====

- Let's first try constructing a loop that counts from 1 to the end of pages
- I always search for snippets - e.g., [here](https://www.google.com/search?client=firefox-b-d&q=loop+through+sequence+python)

```{python, eval=FALSE}
for x in range(6):
  print(x)
```

__Do:__ Modify the snippet below so that it loops through `joke_request['total_pages']`, starting with `1` (and not with `0`)!

```{python eval=FALSE, include=FALSE}
for x in range(joke_request['total_pages']):
  print(x+1)
```

Tying everything together
=============

We can now combine our learnings to build a function that extracts all jokes to new-line separated JSON files.

- Implement it step-by-step
- Also load packages!
- Save as `.py` file and test whether it runs from command prompt/terminal

__Solution on next slide...__


Solution: Tying everything together
=============
class: small-code

```{python, eval=FALSE}
import requests
import json

def get_jokes(term='cat'):
  search_url = "https://icanhazdadjoke.com/search"
  # get the number of jokes first
  response = requests.get(search_url, headers={"Accept": "application/json"},
                          params = {"term": term})
  joke_request = response.json()
  total_pages = joke_request['total_pages']
  print(f'Retrieving {total_pages} pages...')
  
  joke_collection = []
  
  for page in range(total_pages):
    print(f'  Getting data from page {page+1}')
    response = requests.get(search_url, 
                            headers={"Accept": "application/json"},
                            params = {"term": term,
                                      "page": page+1})
    joke_request = response.json()
    for joke in joke_request['results']:
      joke_collection.append(joke)
  return(joke_collection)
```

Do: Onwards to Reddit.com
=======================
class: small-code

- Reddit uses different headers -- use the example snippets from the tutorial
- If you want, you can change the name of the subreddit
- Learning how to iterate through JSON objects is required

```{python, eval=FALSE}
import requests
import json
url = 'https://www.reddit.com/r/marketing/about/.json' # subreddit is "marketing"
headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'user-agent': 'Mozilla/5.0'}
response = requests.get(url, headers=headers)
json_response = response.json()
```

1. Print the `title` of the subreddit!
2. Print the `public description` of the subreddit.
3. Store information (1) and (2) in a dictionary, and have it returned by a function.


Do: More reddit stuff: Looping (I)
======

```{python, eval=FALSE}
import requests
import json
url = 'https://www.reddit.com/r/marketing/.json'
headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'user-agent': 'Mozilla/5.0'}
response = requests.get(url, headers=headers)
json_response = response.json()
```

1) Loop through all posts, and add their text (`selftext`), timestamp (`created_utc`) and indicator whether it is a video (`is_video`) to a new list of dictionaries.

Do: More reddit stuff: Looping (II)
======

2) Parse this list of dictionaries to a csv file, using `pandas`.

```
import pandas as pd
df = pd.from_dict(your_dict)
df.to_csv('filename.csv')
```

Next steps
===========

- Reflection: wow - no "structuring" (THAT's the key difference compared to web scraping)
- Spend time trying to get your first data from an API -- authentication __will__ be an issue!

- Next week: Team coaching __on campus__
  - Presentations: check [activity #3](https://odcm.hannesdatta.com/docs/project/workplan/activity3/)
  - Afterwards: __coding camp__ (you work in teams, I'll give support)
  
- __You can proceed w/ tutorial now.__
  - I will be available for questions (if time is left)...
  - We could also go over some [legal issues](https://htmlpreview.github.io/?https://github.com/hannesdatta/legal-situation-of-web-scraping/blob/main/legal-status-of-web-scraping.html#/title-slide)

