{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Advanced (oDCM)\n",
    "\n",
    "*Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce pretium risus at ultricies egestas. Vivamus sit amet arcu sem. In hac habitasse platea dictumst. Nulla pharetra vitae mauris sed mollis. Pellentesque placerat mauris dui, in venenatis nisl posuere ac. Nunc vitae tincidunt risus, ut pellentesque odio. Donec quam neque, iaculis id eros et, condimentum vulputate nulla. Nullam sed ligula leo.*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to: \n",
    "* Understand the difference between headless and browser emulation and ability to apply both methods (using selenium)\n",
    "* Emulate user interaction with a site using timers, clicks, scrolling, and filling in forms \n",
    "* Access data that is hidden behind a login-screen\n",
    " \n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This course draws on a variety of online resources that can be retrieved from the [course website](https://odcm.hannesdatta.com/docs/about/).\n",
    "\n",
    "--- \n",
    "\n",
    "## Contact\n",
    "For technical issues outside of scheduled classes, please check the [support section](https://odcm.hannesdatta.com/docs/course/support) on the course website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://hannesdatta.github.io/course-jads2020/sessions/webscraper_socialblade.html\n",
    "\n",
    "* https://github.com/CU-ITSS/Web-Data-Scraping-S2019/blob/master/Class%2004%20-%20Selenium%2C%20Twitter%2C%20and%20Internet%20Archive/Class%2004%20-%20Selenium%2C%20Twitter%2C%20and%20Internet%20Archive.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Selenium \n",
    "\n",
    "### 1.1 Why Selenium? \n",
    "In the Web Scraping 101 tutorial, we used BeautifulSoup to turn HTML into a data structure that we could search and access using Python-like syntax. While it's easy to get started with this library, it has limitations when it comes to dynamic websites. That is, websites of which the content changes after each page refresh. Selenium can handle both static and dynamic websites and mimic user behavior (e.g., scrolling, clicking, logging in). It launches another web browser window in which all actions are visible which makes it feel more intuitive. For example, the video below launches a regular Google Chrome window and visits [`instagram.com`](https://www.instagram.com). This browser window behaves like normal, so you can click on buttons and fill out fields. Yet you can distinguish it from your normal web browser by the header that indicates that Chrome is being controlled by automated test software. Before you can try it out yourself, we need to install some additional software which we'll explain next. \n",
    "\n",
    "<img src=\"images/selenium_instagram.gif\" align=\"left\" width=70%/>\n",
    "\n",
    "\n",
    "<!-- \n",
    "\n",
    "With Selenium we use a standard called \"XPath\" to navigate through an HTML document: this is the official tutorial for working with XPath. The syntax is different, but the intuition is similar: we can find a parent node by its attribute (class, id, etc.) and then navigate down the tree to its children.\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installing Selenium\n",
    "You will need to (1) install the Python package for Selenium, (2) download a web driver to interface with a web browser, and (3) configure Selenium to recognize your web driver.\n",
    "1. Open Anaconda Prompt (Windows) or the Terminal (Mac), type the command `conda install selenium`, and agree to whatever the package manager wants to install or update (usually by pressing `y` to confirm your choice). \n",
    "2. Once we run the scraper, a Chrome browser launches which requires a web driver executable file. Download this file from [here](https://sites.google.com/a/chromium.org/chromedriver/downloads) (open [this](https://www.whatismybrowser.com/detect/what-version-of-chrome-do-i-have) site in Chrome to identify your current Chrome version). \n",
    "3. Unzip the file and move it to the same directory where you're running this notebook. Make a note of the path to this directory as you'll need to reference it later. The path to my Chrome driver looks like this (Mac): `/Users/royklaassebos/Google Drive/2020/Data_Scientist_Tilburg/oDCM/chromedriver`. On Windows, it may look like this: `E:/Google Drive/2020/Data_Scientist_Tilburg/oDCM/chromedriver.exe`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the `chrome_path` variable to the location where you've stored the driver and run the cell. It should open an empty Chrome window (don't close it until you're done with scraping!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.webdriver\n",
    "\n",
    "chrome_path = \"/Users/royklaassebos/Google Drive/2020/Data_Scientist_Tilburg/oDCM/chromedriver\"\n",
    "driver = selenium.webdriver.Chrome(executable_path = chrome_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Follow the steps and make sure it works properly on your machine. What happens once you run the cell above twice? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Access Sites Programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance**  \n",
    "Next, we're going to tell the browser to visit the Tilburg University Twitter account. We call the `driver` object we created above and use the `get` method, which we pass the URL of the website we'd like to extract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://twitter.com/TilburgU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/twitter_tilburgu.png\" align=\"left\" width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "As most information can only be obtained once you're signed in, manually login to your Twitter account through the driver page (create a new account if you don't have one yet). \n",
    "\n",
    "From this point, we can use BeautifulSoup as we learned previously, though we create the `res` object from the `driver` object this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you inspect the HTML code of the Twitter page you'll discover that the class names are more complex than the ones we looked at earlier. Take a look at the gigantic class name of the Twitter bio, for example..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/twitter_bio.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which we can extract using the class name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Follow @TilburgU and we will keep you up to date on our latest news! Our webcare team is more than happy to answer your questions on work days, 9 AM - 5 PM.'"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_ = \"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-qvutc0\")[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or by filtering on the `data-testid` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Follow @TilburgU and we will keep you up to date on our latest news! Our webcare team is more than happy to answer your questions on work days, 9 AM - 5 PM.'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(attrs={\"data-testid\": \"UserDescription\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**  \n",
    "Using the same approach as above, extract the (i) number of followers, (ii) the location, and the (iii) join date of the [TilburgU](https://twitter.com/tilburgU) Twitter account. Tip: use Google Inspector to determine an appropriate navigation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Followers: 13K \n",
      "Location: Tilburg, The Netherlands \n",
      "Join date: Joined June 2009\n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "followers = soup.find_all(class_ = \"css-901oao css-16my406 r-jwli3a r-1qd0xha r-b88u0q r-ad9z0x r-bcqeeo r-qvutc0\")[1].text \n",
    "location = soup.find(attrs={\"data-testid\": \"UserProfileHeader_Items\"}).find_all('span')[1].text\n",
    "join_date = soup.find(attrs = {\"data-testid\": \"UserProfileHeader_Items\"}).find_all('span')[3].text\n",
    "\n",
    "print(f\"Followers: {followers} \\nLocation: {location} \\nJoin date: {join_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Scroll Sites Programmatically\n",
    "\n",
    "**Importance**  \n",
    "In a similar way, we can scrape the content of the most recent tweet as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't have any plans for New Year's Eve yet? Join the Tilburg University pub quiz on December 31, 20:00 hrs. Make sure to register before December 31, 10:00 hrs. https://tilburguniversity.edu/current/events/cristmas-and-new-years-activities…\""
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st tweet\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[0].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for older tweets we simply increment the counter by one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We wish you Happy Holidays and a Wonderful New Year!  Our university will be closed for a couple of days: https://tilburguniversity.edu/contact/openinghours… \\nAs our webcare team is also celebrating the holidays, response time will be longer than usual. We will be back in full capacity on Jan 4, 2021!'"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd tweet\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[1].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy right? Not so fast.. From the 10th tweet onwards (in your case it may be a different figure; dependent on screen size, resolution, etc.), it returns an `IndexError: list index out of range`. This is because Twitter only pulls in new tweets once you scroll down the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Congratulations to @MicheleNuijten for winning the Young eScientist Award 2020 from the @eScienceCenter in the amount of 50,000 euro! She will further develop statcheck, an open-access tool for detecting statistical reporting errors. http://tilburguniversity.edu/current/news/more-news/michele-nuijten-escientist-award-meta-research-statcheck…'"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9th tweet\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[8].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-417-d5b04f8bca6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 10th tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data-testid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"dir\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 10th tweet\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[9].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we need to scroll down to the bottom of the page if we like to obtain more than a few tweets. Every time you run the cell below it loads another 5-10 tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Try running the cell above a couple of times. What happens to the most recent tweets? If you run the cells again that extract the 1st, 2nd, 9th, and 10th tweet, does the output change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we need to recreate the `res` object after each iteration because the HTML code changes once you scroll down (older tweets are added and newer ones are hidden). The number of tweets in the view deviates depending on the type of media (e.g., images take up more space than text). Therefore, we first determine the number of views in the current view to make sure we capture all tweets. After we stored the last tweet in the view, we scroll down the page and start all over again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "tweets = []\n",
    "\n",
    "for _ in range(5):\n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    \n",
    "    # total number of tweets in current view\n",
    "    num_tweets_view = len(soup.find_all(attrs={\"data-testid\": \"tweet\"}))\n",
    "        \n",
    "    # add tweets to list\n",
    "    for counter in range(num_tweets_view):\n",
    "        tweets.append(soup.find_all(attrs={\"data-testid\": \"tweet\"})[counter].find_all(attrs={\"dir\": \"auto\"})[4].text)\n",
    "    \n",
    "    # scroll down the page\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    \n",
    "    # pause for 5 seconds\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "1. What happens once you first scroll down the page and then run the cell above? Does `tweets` differ? Why? \n",
    "2. Estimate how many times you would need to scroll in order to capture all tweets (tip: you find the total number of tweets at the top). By the way, there's no need to collect all tweets!\n",
    "3. Write a function `process_tweets()` that returns a list of dictionaries in which each dictionary contains the original tweet, a list of mentions (e.g., `@gemeentetilburg`), and a list of hashtags (e.g., `#makeitintilburg`). Tip: you may first want to split each tweet into a list of words and work from there. When is a word considered a hashtag? And a mention? How about punctuation? Test your function with the list of `tweets` above. \n",
    "4. What's the most-used hashtag in the `tweets` dataset? Start with the output of `preprocess_tweets(tweets)`. Tip: use the [Counter](https://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item) module to easily determine the number of occurrences of each hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  \n",
    "1. The scraper will start from the current view. Since more recent tweets are hidden as you scroll down, the scraper would skip the first few tweets in that case. \n",
    "2. Scrolling down five times yielded 56 tweets, so it would take about 3986/56 = 71 times on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "def process_tweets(tweets):\n",
    "    output = []\n",
    "    \n",
    "    for tweet in tweets: \n",
    "        mentions = []\n",
    "        hashtags = []\n",
    "        \n",
    "        # a more elegant solution can be achieved using regular expressions (outside the scope of this course)\n",
    "        \n",
    "        # remove punctuation (to avoid #hashtag?, #hashtag!, etc.)\n",
    "        for character in [\"?\", \".\", \",\", \"!\"]:\n",
    "            tweet_clean = tweet.replace(character, \"\")\n",
    "            tweet = tweet_clean\n",
    "            \n",
    "        # separate words chained by an enter with a space (to avoid #hashtag\\nABCDEF)\n",
    "        tweet = tweet.replace(\"\\n\", \" \")\n",
    "        \n",
    "        for word in tweet.split(\" \"): \n",
    "            try: \n",
    "                if word[0] == \"@\" and word.count(\"@\") == 1: \n",
    "                    mentions.append(word)\n",
    "                if word[0] == \"#\" and word.count(\"#\") == 1: \n",
    "                    hashtags.append(word) \n",
    "            except: \n",
    "                pass\n",
    "            \n",
    "        output.append({\n",
    "            \"tweet\": tweet,\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": hashtags\n",
    "        })\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'#klimaatneutrale': 1,\n",
       "         '#economie': 1,\n",
       "         '#economische': 1,\n",
       "         '#groei': 1,\n",
       "         '#TilburgU': 12,\n",
       "         '#IkPasBattle': 1,\n",
       "         '#battle': 1,\n",
       "         '#HealthyCampus:': 1,\n",
       "         '#2': 1,\n",
       "         '#statcheck': 1,\n",
       "         '#Covid_19': 2,\n",
       "         '#SGTilburg': 1,\n",
       "         '#sgtilburg': 1,\n",
       "         '#patiënt': 1,\n",
       "         '#zorgverlening': 1,\n",
       "         '#needles': 1,\n",
       "         '#makeitintilburg': 1,\n",
       "         '#ondernemerschap': 1,\n",
       "         '#entrepreneurship': 1,\n",
       "         '#digitization': 1,\n",
       "         '#AI': 2,\n",
       "         '#corona': 1,\n",
       "         '#cancer': 1,\n",
       "         '#WDPD2020': 1,\n",
       "         '#RecognitionandRewards': 1,\n",
       "         '#remembering': 1,\n",
       "         '#MidpointBrabant': 1,\n",
       "         '#coronavirus': 1,\n",
       "         '#Gebarentaal': 1,\n",
       "         '#TrusTee': 1,\n",
       "         '#TilburgUniversityMagazine': 1,\n",
       "         '#CoronaMelder': 1,\n",
       "         '#NobelPeacePrize': 1})"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 4\n",
    "from collections import Counter\n",
    "\n",
    "hashtags = []\n",
    "df = process_tweets(tweets)\n",
    "\n",
    "# add all hashtags of all tweets to a list \n",
    "for tweet_dict in df: \n",
    "    hashtags.extend(tweet_dict['hashtags'])\n",
    "    \n",
    "# count frequencies of hashtags\n",
    "Counter(hashtags)  # so the answer is: #TilburgU (what a surprise..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "List of Twitter followers scrapen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Search Tweets\n",
    "* XXX\n",
    "* XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Wrap-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://twitter.com/search?f=tweets&q=from:WhiteHouse since:2017-01-20 until:2018-01-20&src=typd'"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the query params\n",
    "query_params = {}\n",
    "query_params['from'] = 'WhiteHouse'\n",
    "query_params['since'] = '2017-01-20'\n",
    "query_params['until'] = '2018-01-20'\n",
    "\n",
    "# Pass the params into a string and quote to format it properly\n",
    "query_params_quoted = f'from:{query_params[\"from\"]} since:{query_params[\"since\"]} until:{query_params[\"until\"]}'\n",
    "\n",
    "# Add the quoted query params into the URL\n",
    "query_url = \"https://twitter.com/search?f=tweets&q={0}&src=typd\".format(query_params_quoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hoe gaan landen om met een belast verleden? En hoe verhouden sociale ondernemers zich tot de verzorgingsstaat?\\n\\nA.s. zaterdag geven tien promovendi inzicht in hun onderzoeken, waaronder Marlies en Michiel \\n\\nMeer info: https://tilburguniversity.edu/nl/campus/studium-generale/wetenschap-de-maak…\\n@sgtilburg @bibliotheekmb'"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate soup object based on current HTML page\n",
    "res = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# extract first tweet in current view\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[0].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/scrolling.png\" align=\"left\" width=40%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wij wensen u fijne feestdagen en een gezond nieuwjaar! Wij hopen elkaar in 2021 weer in het echt te kunnen ontmoeten, maar voor nu deze virtuele kerstgroet, namens het hele VSNU-bureau en namens de universiteiten van Nederland. #kerstgroet #universiteiten'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[0].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following media includes potentially sensitive content. Change settingsView'"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How exceptional is #Covid_19? Can it strike again? Certainly, says Jonathan Verschuuren. He has explored the phenomenon of zoonosis, viruses that jump from animals to humans. What he discovered is not entirely reassuring. http://tilburguniversity.edu/magazine/how-exceptional-covid-19…'"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[4].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kan #klimaatneutrale #economie gecombineerd worden met #economische #groei? @TilburgU professor Gerlagh sprak over de klimaatverandering van de afgelopen 100 jaar en de verwachtingen van de komende 50 jaar op het Nederlands Gala van de Wetenschap 2020:\n",
      "Univers blikt met organisatiewetenschapper @mzelst van @C19RedTeam terug op een jaar waarin hij eigenlijk zijn proefschrift had willen afronden, maar dat uiteindelijk vooral in het teken stond van coronacijfers. #TilburgU\n",
      "This January @gem_Nijmegen & @gemeentetilburg go head to head in the #IkPasBattle. The goal is to recruit the most participants who are willing to go one month without alcohol. @TilburgU joins the #battle in the pursuit of a #HealthyCampus: https://ikpas.nl/bedrijf/tilburguniversity/…\n",
      "Dr. Martin Hoondert, Associate Professor @TilburgU, is researching the effects of the coronavirus crisis on rituals surrounding funerals. Rituals are changing, disappearing (temporarily or otherwise) and new ones arising. Find out more about these changeshttps://tilburguniversity.edu/magazine/spontaneity-more-modest-rituals…\n",
      "Congratulations! The researchers of the Tilburg School of Economics and Management traditionally do well in the Economists Top 40. @tilburgU ranks #2! Among others Bart Bronnenberg became second in the researchers top 40! http://tilburguniversity.edu/about/schools/economics-and-management/organization/tilburg-school-economics-and-management-ranks-high-economists-top-40-0…\n",
      "Also, congratulations to @willemsleegers, co-applicant on the grant #statcheck.\n",
      "Congratulations to @MicheleNuijten for winning the Young eScientist Award 2020 from the @eScienceCenter in the amount of 50,000 euro! She will further develop statcheck, an open-access tool for detecting statistical reporting errors. http://tilburguniversity.edu/current/news/more-news/michele-nuijten-escientist-award-meta-research-statcheck…\n",
      "Wij wensen u fijne feestdagen en een gezond nieuwjaar! Wij hopen elkaar in 2021 weer in het echt te kunnen ontmoeten, maar voor nu deze virtuele kerstgroet, namens het hele VSNU-bureau en namens de universiteiten van Nederland. #kerstgroet #universiteiten\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-ff5ebed2d085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data-testid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data-testid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"dir\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data-testid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"dir\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'window.scrollTo(0, document.body.scrollHeight);'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "for _ in range(4):\n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    for counter in range(len(soup.find_all(attrs={\"data-testid\": \"tweet\"}))):\n",
    "        tweets.append(soup.find_all(attrs={\"data-testid\": \"tweet\"})[counter].find_all(attrs={\"dir\": \"auto\"})[4].text)\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    \n",
    "#print(len(soup.find_all(attrs={\"data-testid\": \"tweet\"})))\n",
    "#soup.find_all(attrs={\"data-testid\": \"tweet\"})[8].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "temp = pd.DataFrame(tweets)\n",
    "temp.to_csv('tweets.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Congratulations! The researchers of the Tilburg School of Economics and Management traditionally do well in the Economists Top 40. @tilburgU ranks #2! Among others Bart Bronnenberg became second in the researchers top 40! http://tilburguniversity.edu/about/schools/economics-and-management/organization/tilburg-school-economics-and-management-ranks-high-economists-top-40-0…'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_=\"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")[6].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Meerdere keren runnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup.find_all(class_ = \"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")[0].text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't have any plans for New Year's Eve yet? Join the Tilburg University pub quiz on December 31, 20:00 hrs. Make sure to register before December 31, 10:00 hrs. https://tilburguniversity.edu/current/events/cristmas-and-new-years-activities…\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[0].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't have any plans for New Year's Eve yet? Join the Tilburg University pub quiz on December 31, 20:00 hrs. Make sure to register before December 31, 10:00 hrs. https://tilburguniversity.edu/current/events/cristmas-and-new-years-activities…\""
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(attrs={\"data-testid\": \"tweet\"}).find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tilburg University'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(attrs={\"role\": \"article\"}).find(attrs={\"dir\": \"auto\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the resolution of your display and size of the window, there may only be 5–10 tweets visible. Typically, we can only scrape the contents of the page that are visible in the current view. In other words, we need to scroll down to the bottom of the page if we like to obtain more than a few tweets. Every time you run the cell below it loads another 5-10 tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.find_all(class_='r-bnwqim r-qvutc0') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".r-bnwqim.r-qvutc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'price_color'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = driver.find_element_by_xpath('//*[@id=\"default\"]/div/div/div/div/section/div[2]/ol/li[1]/article/div[2]/p[1]').get_attribute('class')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default > div > div > div > div > section > div:nth-child(2) > ol > li:nth-child(1) > article > div.product_price > p.price_color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Controlling Programmable Web Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access website\n",
    "* Clicking \n",
    "* Scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('Twitter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Twitter Account Handle\n",
    "* Name of Twitter account\n",
    "* Bio\n",
    "\n",
    "* Meer dan 20 tweets binnenhalen middels scrolling\n",
    "    * hoe vaak moet je scrollen? \n",
    "\n",
    "* Oefening op tweet level\n",
    "    * text\n",
    "    * replies\n",
    "    * retweets\n",
    "    * favorites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Searching for Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Error Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Instagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_path = \"/Users/royklaassebos/Google Drive/2020/Data_Scientist_Tilburg/oDCM/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome(executable_path=mac_path)\n",
    "driver.get(\"https://www.instagram.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='username']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='password']\")))\n",
    "\n",
    "username.clear()\n",
    "password.clear()\n",
    "\n",
    "username.send_keys(\"onlineregistratie\")\n",
    "password.send_keys(\"jojnu4-vunpYf-qebqaz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_in = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_now = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now')]\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchbox = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//input[@placeholder='Search']\")))\n",
    "searchbox.clear()\n",
    "\n",
    "keyword = \"#cat\"\n",
    "searchbox.send_keys(keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchbox.send_keys(Keys.ENTER) # kan zijn dat je het 2x moet laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, 4000);\") # 4000 is around 4 times the size of a screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = driver.find_elements_by_tag_name(\"img\")\n",
    "images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image.get_attribute('src') for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(path, keyword[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path = os.path.join(path, keyword[1:])\n",
    "\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "counter = 0\n",
    "for image in images: \n",
    "    save_as = os.path.join(path, keyword[1:] + str(counter) + \".jpg\")\n",
    "    wget.download(image, save_as)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Advanced Web Scraping\n",
    "* Understand the difference between headless and browser emulation and ability to apply both methods (using selenium).\n",
    "* Emulate user interaction with a site using timers, clicks, scrolling, and filling in forms \n",
    "* Access data that is hidden behind a login-screen\n",
    "* Preprocess raw data with regular expressions (e.g., special characters, thousand separators, trailing and leading spaces)\n",
    "* Custom user agents\n",
    "* Throttling\n",
    "* Regular expressions\n",
    "* Feature engineering (date time, week, year, textblob sentiment)\n",
    "* Error handling (e.g., 404 pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/kimfetti/Conferences/tree/master/PyCon_2020\n",
    "* https://www.youtube.com/watch?v=RUQWPJ1T6Zc&t=190s\n",
    "* https://github.com/hancush/web-scraping-with-python/blob/master/session/web-scraping-with-python.ipynb#HTML-basics\n",
    "* https://www.udemy.com/course/the-modern-python3-bootcamp/learn/lecture/7991196#overview\n",
    "* https://campus.datacamp.com/courses/web-scraping-with-python/introduction-to-html?ex=1\n",
    "* https://realpython.com/python-web-scraping-practical-introduction/\n",
    "* https://github.com/CU-ITSS/Web-Data-Scraping-S2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "* Regex = regular expressions\n",
    "* Way of describing patterns within search strings \n",
    "* Not Python specific topic \n",
    "* Hideous and very difficult to understand (not Pythonic style) \n",
    "* There are a ton of regex symbols -> we're just going to cover the most important ones\n",
    "* Cheat sheet: https://www.rexegg.com/regex-quickstart.html\n",
    "* test regex: https://pythex.org\n",
    "\n",
    "Potential use cases\n",
    "* Credit card number validating\n",
    "* Phone number validating (website forms)\n",
    "* Advanced find/replacd in text\n",
    "    * Check if words are duplicated (one upon a time time)\n",
    "* Formatting text/output\n",
    "* Syntax highlighting (wat je ook in IDE ziet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validating emails (check: does it follow the right format?) \n",
    "    * letters + @ letters.letters\n",
    "    * gewoon checken of er een @ symobl in staat makkelijk (\"@\" in ...)\n",
    "    * maar @ mag niet op het begin of het einde zijn\n",
    "    * mag niet meer dan 1x @ zijn\n",
    "    * @ moet voor de .com zijn\n",
    "    * ingewikkeld want de \".\" kan ook op andere plekken voorkomen (roy.klaasse.bos@gmail.com)\n",
    "    * je zou hier normaal veel if-statements voor moeten schrijven\n",
    "\n",
    "\n",
    "* Formula\n",
    "    * Starts with 1 or more letter, number, +, _, -,. then\n",
    "    * A sigle @sign\n",
    "    * 1 or more letter, number or - then\n",
    "    * A single dot\n",
    "    * End with 1 or more letter, number, - or ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-ZA-Z0-9-.]+$)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Basic Syntax\n",
    "* Regular letters\n",
    "* Escape special characters (:\\) voor een smiley)\n",
    "* \\d\\d (for double digits) \n",
    "* Capitalize means NOT (\\s vs \\S)\n",
    "\n",
    "\n",
    "* \\d = digital 0-9\n",
    "* \\w = letter, digit or underscore (word character)\n",
    "    * zowel lowercase als uppercase letters\n",
    "    * accenten vallen hier niet onder!\n",
    "* \\s = whitespace character (maar ook een tab, newline) \n",
    "    * daarin verschilt het van zelf gewoon een spatie invullen -> handig voor (voor als iets het laatste woord van de zin is - gevolgd door een punt)\n",
    "* \\D = not a digit\n",
    "* \\W = not a word character\n",
    "* \\S = not a whitespace character\n",
    "* . = any character except line break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifiers\n",
    "\n",
    "* \\+ = one or more\n",
    "* {3} = exactly 3 times\n",
    "* {3,5} = 3 to 5 times\n",
    "* {4,} = 4 or more times\n",
    "* \\* = 0 or more times\n",
    "* ? = once or none\n",
    "\n",
    "\n",
    "Examples\n",
    "* `ab*c` = a[zero or more b's]c\n",
    "    * verwijst dus naar het teken wat er voorkomt (en niet naar ab)\n",
    "* `06-?12345678` = zowel 0612345678 en 06-12345678\n",
    "* `7{3}` slaat naast 777 ook terug op (4567777789)\n",
    "* `hi{2,}` is a single \"h\" and then \"i\" repeated two or more times (dus niet \"hi\" repeated two times\n",
    "* `0?\\d` = \"00\", \"9\", \"0\", \"03\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Classes and Sets\n",
    "* Any vowel eaiou werkt niet -> [eaiou]\n",
    "* Ranges of characters \n",
    "    * lowercase[a-z] - kan ook [a-f]\n",
    "    * [A-Z]\n",
    "* ^ within range\n",
    "    * [^A-Z] = not A-Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors and boundaries\n",
    "* ^ = start of string or line\n",
    "* $ = end of string or line\n",
    "* \\b = word boundary (bijv. eerste woord in een zin; geen spatie ervoor maar toch meenemen)\n",
    "\n",
    "Examples\n",
    "* probleem met `\\d{3} \\d{3}-?\\d{4}` voor een telefoonnummer -> kan zijn dat er voor of erna nog allemaal andere crap staat \n",
    "    * combinatie van `^` en `$` om andere tekst uit te sluiten \n",
    "    * '^\\d{3} \\d{3}-\\d{4}$' = beginnen met 3 digits en eindigen met 4 digits\n",
    "* `^\\d{3}$` matcht niet met `Yay I got 777`\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Or and Capture Groups\n",
    "\n",
    "* `|` = OR `\\(\\d{3}\\)|\\d{3}` = 3 digits with or without parentheses\n",
    "* () to capture groups \n",
    "    * `\\(\\d{3}\\)|\\d{3} \\d{3} \\d{4}` maakt de vergelijking 3 digits with parentheses or 10 digits without parentheses. Als je dus echt de 3 met en zonder apart wilt vergelijken met je om dat deel haakjes toevoegen\n",
    "    * Zelfs al heb je het niet per se nodig kan het alsnog handig zijn om `()` te gebruiken om een groep aan te maken -> scheiden van naam van voorvoegsel (Mr. / Ms.)\n",
    "    * Gebruik je heel veel zodat je het niet handmatig nog een keer hoeft te gaan splitten\n",
    "    \n",
    "Examples\n",
    "* `https?://([A-Za-z_-0-9]+\\.[A-Za-z_-0-9]+)`\n",
    "    * Alleen het deel na `http://` als groep opslaan\n",
    "* `Mr.|Mister Holmes` -> Mr. OR Mister Holmes (because of a lack of parentheses)\n",
    "* Escape group symbol \n",
    "    * Which regex would match both of the following strings (`cat(s)` AND `dog(s)`)\n",
    "    * `\\w{3}\\(s\\)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re Module\n",
    "* https://docs.python.org/3/library/re.html\n",
    "* `r` = raw string (otherwise you have to use double backslashes - avoids that \\t is seen as a TAB)\n",
    "* compiling it separately (`re.compile`) vs rechtstreeks\n",
    "    * if you're using it more than once -> via `re.compile()`\n",
    "* `search` = max 1 result\n",
    "* `findall` = return all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex module\n",
    "import re\n",
    "\n",
    "# define our phone number regex\n",
    "pattern = re.compile(r'\\d{3} \\d{3}-\\d{4}')\n",
    "\n",
    "# search a string with our regex\n",
    "result = pattern.search('Call me at 415 555-4242 or 310 234-9999!')\n",
    "print(result.group())\n",
    "\n",
    "\n",
    "result2 = pattern.findall('Call me at 415 555-4242 or 310 234-9999!')\n",
    "print(result2)\n",
    "\n",
    "\n",
    "# in plaats van een apart object aanmaken -> gelijk pattern in search\n",
    "print(re.findall(r'\\d{3} \\d{3}-\\d{4}', 'Call me at 415 555-4242 or 310 234-9999!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_phone(input):\n",
    "    phone_regex = re.compile(r'\\b\\d{3} \\d{3}-\\d{4}\\b')\n",
    "    match = phone_regex.search(input)\n",
    "    if match: \n",
    "        return match.group()\n",
    "    return None\n",
    "\n",
    "print(extract_phone(\"my number is 432 567-8976\"))\n",
    "print(extract_phone(\"my number is 432 567-897622\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing URLs\n",
    "* Breaking things up (`match.groups()`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = re.compile(r'(https?)://(www.[A-Za-z-]{2,256}\\.[a-z]{2,6})([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n",
    "match = url_regex.search(\"http://www.youtube.com/videos/asd/das/asd\")\n",
    "print(match.groups())\n",
    "print(match.groups()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "import re\n",
    "#define parse_date below\n",
    "\n",
    "def parse_date(input):\n",
    "    date_regex = re.compile(r'(\\d{2})[/.,](\\d{2})[/.,](\\d{4})')\n",
    "    match = date_regex.search(input)\n",
    "    return {\"d\": match.groups()[0], \n",
    "            \"m\": match.groups()[1], \n",
    "            \"y\": match.groups()[2], \n",
    "            }\n",
    "\n",
    "parse_date('12.04.2003')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation Flags\n",
    "* `IGNORECASE` = geen onderscheid meer tussen lower en upper case ([a-z]) pakt hierdoor ook hoofdletters op\n",
    "* `VEBOSE` = expand across multiple lines (als je hele lange regular expressions hebt) -> ignores white space\n",
    "* Meerdere compilation flags combineren met een pipe (|) symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"\"\"\n",
    "    ^([a-z0-9_\\.-]+)      # first part of email\n",
    "    @                     # single @sign\n",
    "    ([a-z0-9_\\.-]+)       # email provider\n",
    "    \\.                    # single period\n",
    "    ([a-z0-9_\\.-]{2,6})$  # com, org, net, etc.\n",
    "    \"\"\", re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "match = pattern.search(\"Thomas123@Yahoo.com\")\n",
    "print(match.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"\"\"\n",
    "    ^([a-z0-9_\\.-]+)      # first part of email\n",
    "    @                     # single @sign\n",
    "    ([a-z0-9_\\.-]+)       # email provider\n",
    "    \\.                    # single period\n",
    "    ([a-z0-9_\\.-]{2,6})$  # com, org, net, etc.\n",
    "    \"\"\", re.IGNORECASE)\n",
    "\n",
    "match = pattern.search(\"Thomas123@Yahoo.com\")\n",
    "print(match.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitutions\n",
    "* Privacy gevoelige informatie weglaten \n",
    "* Zinnen herstructureren: bijv. \n",
    "    * Significant Others (1987) \n",
    "    * Naar: 1987 - Significant Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove names from text (privacy)\n",
    "text = \"Last night Mrs. Daisy and Mr. White murdered Mr. Chow\"\n",
    "\n",
    "pattern = re.compile(r'(Mrs\\.|Mr\\.) ([a-z]+)', re.IGNORECASE)\n",
    "result = pattern.sub(\"REDACTED\", text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aparte groep maken als je bijvoorbeeld wel de eerste letter wilt laten zien\n",
    "# \\g<1> refers to group 1 (je hebt dus geen group 0)\n",
    "pattern = re.compile(r'(Mrs\\.|Mr\\.) ([a-z])([a-z]+)', re.IGNORECASE)\n",
    "result = pattern.sub(\"\\g<1> \\g<2>\", text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def censor(input):\n",
    "    censor_pattern = re.compile(r'frack\\w*', re.IGNORECASE)\n",
    "    return censor_pattern.sub(\"CENSORED\", input)\n",
    "    \n",
    "censor(\"Frack you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exam questions: \n",
    "* Which of the following strings will have matches in them? \n",
    "    * Syntax geven\n",
    "    * Meerdere voorbeeld zinnetjes\n",
    "* Write a function called `is_valid_time` that accepts a single string argument. It should return `True` if the string is formatted correctly as a time, like 3:15 or 12:48 and return `False` otherwise. Note that times can start with a single number (2:30) or two (11:18)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to import re!\n",
    "import re\n",
    "# Define is_valid_time below:\n",
    "def is_valid_time(input):\n",
    "    time_regex = re.compile(r'^[0-23]{1,2}:[0-5]{1}[0-9]{1}')\n",
    "    match = time_regex.search(input)\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "is_valid_time(\"23:59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
