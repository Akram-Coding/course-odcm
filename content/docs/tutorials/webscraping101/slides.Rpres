oDCM - Web Scraping 101 (Tutorial)
========================================================
author: Hannes Datta
date:
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

<!--#

https://support.rstudio.com/hc/en-us/articles/200486468
-->

```{r setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(reticulate)
py_install("requests")
py_install("bs4")
```

Welcome to oDCM!
========================================================

We're about to start with __today's tutorial ("web scraping 101")__.


- If you haven't done so, open slides at https://odcm.hannesdatta.com/docs/tutorials/webdata-101/
- Today's (live) code is available at [tiu.nu/livecoding]() (refresh needed)
- You will need to use __Jupyter Notebook on your laptops__


<!--
- Login at [http://pulse.tilburg-digital.com]() to explore this week's to do's
-->

Before we start
========================================================

- Recap coaching session for team activity #1
  - explore broadly! ("universe discovery!") --> check challenge #1.1
  - compare sources thoroughly --> e.g., comparison table + challenge #1.2
<br>

- Any other issues?


Agenda
=======
- In-class
  - Go through selected issues of "web scraping 101"
  - Will use different sites (e.g., Twitch)
- After class
  - Complete exercises yourselves

Making a connection with a site
=========
incremental: true

- So far, we have done this
  - `requests` (to get) --> `beautifulsoup` (to extract information, "parse")
  
- __DO:__ Run the snippet below and open `amazon.html`  
```{python, eval=FALSE}
import requests
header = 
f = open('amazon.html', 'w', encoding = 'utf-8')
f.write(requests.get('https://amazon.com', headers =  {'User-agent': 'Mozilla/5.0'}).text)
f.close()
```

__Can you explain what happened?__

<!--
But... what about dynamic websites?
  
  - Some sites block requests from the get-go, even w/ headers.
-->

Alternative ways to make connections
========

- Many dynamic sites require what I call "simulated browsing"

- Try this:
```{python, eval=FALSE}
!pip install webdriver_manager
!pip install selenium

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# Opening the site
driver = webdriver.Chrome(ChromeDriverManager().install())

url = "https://amazon.com/"
driver.get(url)
```

- See your browser opening up?
- Beware of rerunning code - a new instance will run each time!


Continuing with beautifulsoup
=========

- We can convert the site's source code (`page_source`) to `BeautifulSoup`, and proceed as always

```{python, eval=FALSE}
from bs4 import BeautifulSoup
soup = BeautifulSoup(driver.page_source)

for el in soup.find_all(class_ = 'a-section'):
    title = el.find('h2')
    if title is not None: print(title.text)
```


Do: Scraping Twitch.tv using `selenium`
=====

1. Use selenium to open `twitch.tv`

```{python, eval=FALSE}
import time
url = "https://twitch.tv/"
driver.get(url)
time.sleep(3) # wait for a few seconds for everything to be loaded
```

__Tip: Use the code snippet from above first to open the browser window!__.

2. Extend code to be able to use BeautifulSoup, and find all links (tag is `a`). Recall you can use `soup.find_all()`


Finding more specific links
===========

- So far, we only searched for tags (`a`), or classes (`class_='classname'`).
- We can also search for `attribute values`.
- Suppose we only want to obtain the links to __streams__, we can filter for values of a particular attribute, e.g., `"data-test-selector"` being `"TitleAndChannel"`.

```{python, eval = FALSE}

streams = soup.find_all('a', {'data-test-selector':"TitleAndChannel"})

```

__DO:__

- Write a code snippet to collect all stream names and corresponding URLs (`['href']`), and storing them in a list of dictionaries.


Framework
=======

![](https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/jmxa/2022/jmxa_86_5/00222429221100750/20220801/images/large/10.1177_00222429221100750-fig2.jpeg)

Today: zooming in more on __collection design__

Stage 2: How to sample?
=======

- Many pages on a website (here, Twitch) could serve as starting points for a data collection.
- For example, starting from the homepage, one samples from *currently active channels on Twitch*.
- But... such a sample is uninformative of *all* shows available on Twitch.
- In your web data projects, you need to think about *how to sample from the site*.

- __Do: Identifying sampling frames on Twitch__
  - Using the suggestions for challenge #2.2 (Table 3) in [tiu.nu/scraping](), identify potential sampling frames.
  

Stage 1: Which information to extract
========
incremental: true

- Let's open one of the currently running shows and explore what information we can see
- Does that trigger any "research ideas"?
- Let's now broaden our perspective, by considering some of the issues discussed in challenge #2.1 in Table 3.
  - e.g., live vs. archival data 
  - e.g., limits to iterating through pages
  - and many other validity, legal/ethical and technical issues

__DO:__

1. Can you develop a code snippet that extracts some of the desired information?
2. Let's wrap the code in a function which stores the desired information in a dictionary.


Stage 3: At which frequency
=======

- We can decide how often to capture information from a site
  - e.g., once, every 5 minutes, every day
- Potential gains in extracting data multiple times
- Extraction limits & technically feasible sample size!
- More issues explained in table 3, challenge #2.3



Stage 4: How to process data during collection
==========

- value of retaining raw data
- parsing data on-the-fly

Finalizing scraper
=======================

Let's wrap the data collection together in one executable file.


Next steps
===========

- Prepping for next coaching
   - get started w/ prototyping data extraction
   - consider selenium!
   - pitch your idea to friends - do they see value?
