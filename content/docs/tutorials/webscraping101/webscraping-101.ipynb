{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101 (in-class)\n",
    "\n",
    "*After finishing this tutorial, you can extract data from multiple pages on the web, and export such data to CSV files so that you can use it in an analysis. Plan a few hours to work through this notebook. Taking a few breaks inbetween keeps you sharp! Enjoy!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Understand the difference between headless and browser emulation and ability to apply both methods (using `requests` and `selenium`)\n",
    "* Generate seeds (‚Äúsampling‚Äù) \n",
    "* Navigating on a website using URLs and clicking\n",
    "* Implement timers and modularise extraction code\n",
    "* Write loops to execute data collections in bulk using functions\n",
    "* Store data in CSV or JSON files, and enrich with relevant metadata\n",
    "\n",
    "--- \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Support Needed?</b> \n",
    "    For technical issues outside of scheduled classes, please check the <a href=\"https://odcm.hannesdatta.com/docs/course/support\" target=\"_blank\">support section</a> on the course website.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Making different types of website requests\n",
    "\n",
    "In previous tutorials, you have used the `requests` library to retrieve web data. For example, re-run the following code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>Sharp Objects</h1>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "request = requests.get('https://books.toscrape.com/catalogue/sharp-objects_997/index.html', headers = user_agent)\n",
    "source_code = request.text\n",
    "\n",
    "# save website \n",
    "f=open('simple_website.html','w',encoding='utf-8')\n",
    "f.write(source_code)\n",
    "f.close()\n",
    "\n",
    "# parse some information\n",
    "soup=BeautifulSoup(source_code)\n",
    "soup.find('h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for relatively simple websites, but... try the same for the homepage of Twitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = requests.get('https://www.twitch.tv/', headers = user_agent)\n",
    "source_code = request.text\n",
    "soup=BeautifulSoup(source_code)\n",
    "\n",
    "# save website \n",
    "f=open('advanced_website.html','w',encoding='utf-8')\n",
    "f.write(source_code)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to open `advanced_website.html` in your browser, you quickly realize there is a problem. You can't see what's on the website when you manually open it using the URL. This mainly has to do with how advanced a website is: in the case of Twitch, you'd encounter quite a dynamic site with a video player, previews, real-time updates on the number of streams, etc. The normal request library isn't just able to handle it. \n",
    "\n",
    "So, we're resorting to an alternative way to retrieve data, using `selenium`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Installing Selenium and Chromedriver</b> \n",
    "\n",
    "To install Selenium and Chromedriver locally, please follow the <a href=\"https://tilburgsciencehub.com/configure/python-for-scraping/?utm_campaign=referral-short\">Tutorial on Tilburg Science Hub</a>.\n",
    "    \n",
    "You can also use the code snippet below to automate the installation. Running this snippet takes a little longer each time, but the benefit is that it almost always works!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver_manager in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (3.4.2)\n",
      "Requirement already satisfied: requests in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from webdriver_manager) (2.26.0)\n",
      "Requirement already satisfied: crayons in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from webdriver_manager) (0.4.0)\n",
      "Requirement already satisfied: configparser in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from webdriver_manager) (5.0.2)\n",
      "Requirement already satisfied: colorama in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from crayons->webdriver_manager) (0.4.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from requests->webdriver_manager) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from requests->webdriver_manager) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from requests->webdriver_manager) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from requests->webdriver_manager) (1.25.9)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/Users/hannesdatta/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: selenium in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /Users/hannesdatta/opt/anaconda3/lib/python3.8/site-packages (from selenium) (1.25.9)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/Users/hannesdatta/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 103.0.5060\n",
      "Get LATEST driver version for 103.0.5060\n",
      "Driver [/Users/hannesdatta/.wdm/drivers/chromedriver/mac64/103.0.5060.134/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Installing and starting up Chrome using Webdriver Manager\n",
    "!pip install webdriver_manager\n",
    "!pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Opening the Twitch site\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "url = \"https://twitch.tv/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went smooth, your computer opened a new Chrome window, and opened `twitch.tv`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Using Google Colab</b> \n",
    "\n",
    "If you're using Google Colab, you don't see your browser open up manually.\n",
    "    \n",
    "Whenever you switch pages, just manually open that page in your browser. Although this feels like a little less interactive, you will still be able to work through this tutorial!\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "From now onwards, you can use `driver.get('https://google.com')` to point to different websites (i.e., you don't need to install it over and over again, unless you open up a new instance of Jupyter Notebook).\n",
    "\n",
    "We can now also try to extract information. Note that we're converting the source code of the site to a `beautifulSoup` object (because you may have learnt how to use `BeautifulSoup` earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need the time package to wait a few seconds until the page is loaded\n",
    "import time\n",
    "url = \"https://twitch.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1: DROPS - BANDITS WITH DAD-  Answering New Player Questions -Lewpac\n",
      "Stream 2: ‚òëÔ∏èOMG ‚òëÔ∏èCLICK NOW TO HEAR ABOUT IT ‚òëÔ∏èCLICK NOW ‚òëÔ∏èTHIS IS CRAZY ‚òëÔ∏èINSANE ROLEPLAYER GOD ‚òëÔ∏èGAMEPLAY GOLEM ‚òëÔ∏èREACTOR! ‚òëÔ∏èxQc\n",
      "Stream 3: üî¥7/24 DROPS ONüî¥ !drops | discord.gg/kJDzdXdBKCMultiversusTR\n",
      "Stream 4: Berlin d√∂ner = best d√∂ner | !socials $7sr !vpn !maprobcdee\n",
      "Stream 5: Sipping coffee whilst providing an update on editing. Also featuring my pet ant.SovietWomble\n",
      "Stream 6: Tandem Cycling to Norway? - Day 36 - Lauwersoog, NL | !trip !bike !map !merchHitch\n",
      "Stream 7: IM BACK!! Lets see how top players perform on the new patch! !sellout to request a killer build! #ad !AWxNvidiaTrU3Ta1ent\n",
      "Stream 8: Wampus 4 sub hour grind | No Drama Zone !hellofreshMudda_tm\n",
      "Stream 9: NNO Gamer gamet um 21 Uhr mit anderen NNO Gamern gegen Alle f√ºr einen Gamer und hofft gut zu gamen um in den playoffs auch noch zu gamenTolkinLoL\n",
      "Stream 10: Lets Have Some Fun - !youtubeFarshadSilent\n",
      "Stream 11: I don't love StrayLimmy\n",
      "Stream 12: hey hey gm :)Harrie\n",
      "Stream 13: EUW soloq nightmare ;( | !EUWmidbeast\n",
      "Stream 14: RA VS. RNG | FPX VS. TT - Week 8 Day 1 | LPL Summer Split (2022)LPL\n",
      "Stream 15: GIGACHAD SUMMIT ON THE RIFT TAKING OVER - FPX VS TT !proguides !excel !social !discord !youtube !legion !essentials !streamscheduleCaedrel\n"
     ]
    }
   ],
   "source": [
    "soup=BeautifulSoup(driver.page_source)\n",
    "\n",
    "streams = soup.find_all('a', {'data-test-selector':\"TitleAndChannel\"})\n",
    "\n",
    "# print a list of stream names\n",
    "counter = 0\n",
    "for stream in streams:\n",
    "    counter = counter + 1\n",
    "    print('Stream ' + str(counter) + ': ' + stream.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - this is cool. You've just learnt a second way to open websites using `selenium`. The benefit of `selenium` is that you can work with highly dynamic websites (which also helps you to not getting blocked). The drawback is that `selenium` is slower than just using the `requests` library, and it may sometimes be buggy on computers without a screen (which matters when you scale up your data collection.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Awesome stuff with Selenium</b> \n",
    "\n",
    "Selenium is your best shot at navigating a dynamic website. It can do amazing things, such as \n",
    "    \n",
    "<ul>\n",
    "    <li>\"clicking\" on buttons</li>\n",
    "    <li>scrolling through a site</li>\n",
    "    <li>hovering over items and capturing information from popups,</li>\n",
    "    <li>starting to play a stream,</li>\n",
    "    <li>typing text and submitting it in the chat, and</li>\n",
    "    <li>so much more...!</li>\n",
    "</ul>\n",
    "    \n",
    "Note though that we won't cover the advanced functionality of Selenium in this tutorial, but the optional \"Web data advanced\" tutorial holds the necessary information.\n",
    "   \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "__Exercise 1.1__\n",
    "\n",
    "Please write code snippets to extract the following pieces of information. Do you choose `requests` or `selenium`?\n",
    "\n",
    "1. The titles of all `<h2>` tags from `https://odcm.hannesdatta.com/docs/course/`\n",
    "2. The titles of all available TV series from `https://www.bol.com/nl/nl/l/series/3133/30291/` (about 24)\n",
    "\n",
    "```\n",
    "soup.find('a', class_='product-title')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seed Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "So far, we've parsed some information (e.g., titles, product names, prices) from websites. What we haven't really done yet is decide for which products to obtain that information. Ideally, we would like to capture information for a sample of users, books, movies, series, etc. \n",
    "\n",
    "In web scraping, we typically refer to a \"seed\" as a starting point for a data collection. Without a seed, there's no data to collect.\n",
    "\n",
    "For example, before we can crawl through all books available on [this site](https://books.toscrape.com/catalogue/category/books_1/index.html), we first need to generate a *list of all books on the page*.\n",
    "\n",
    "One way to get there would be to:\n",
    "\n",
    "1. first scrape all book links (‚Äúseeds‚Äù) from the overview page, and \n",
    "2. then iterate over all links to scrape the product description (or anything else on that page). \n",
    "\n",
    "Note that the overview page allows us to \"navigate\" to the individual book pages, either by clicking on the book cover or the book title (see red boxes in the figure below). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/books_links.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Collecting Links to use as seeds\n",
    "\n",
    "Let's check out how the links from the book covers or book titles are encoded in the website's source code.\n",
    "\n",
    "Open the [book catalogue](https://books.toscrape.com/catalogue/category/books_1/index.html), and inspect the underlying HTML code with the Chrome Inspector (right click --> inspect element). \n",
    "\n",
    "The book covers (`<img>`) are surrounded by `<a>` tags, which contain a link (`href`) to the book. \n",
    "\n",
    "Also, the book titles (`<h3>`) are surrounded by `<a>` tags with the relevant links to the book pages.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/inspector_links.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we tell a computer to capture the links to the various books on the site?\n",
    "\n",
    "One simple way is to select *elements by their tags*. For example, to extract all links (`<a>` tags). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.1__\n",
    "\n",
    "Please run the code cell below, which extracts all links (the `a` tag!), and prints the URL (`href`) to the screen. Don't worry, you don't need need to understand the code yet, we'll go over it line by line shortly!\n",
    "\n",
    "If you look at these links more closely, you'll notice that we're not interested in many of these links... \n",
    "\n",
    "Make a list of all links we're *not* interested in (i.e., those *not* pointing to a book page). Which ones are those? Can you find out why they are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../index.html\n",
      "../../../index.html\n",
      "index.html\n",
      "../books/travel_2/index.html\n",
      "../books/mystery_3/index.html\n",
      "../books/historical-fiction_4/index.html\n",
      "../books/sequential-art_5/index.html\n",
      "../books/classics_6/index.html\n",
      "../books/philosophy_7/index.html\n",
      "../books/romance_8/index.html\n",
      "../books/womens-fiction_9/index.html\n",
      "../books/fiction_10/index.html\n",
      "../books/childrens_11/index.html\n",
      "../books/religion_12/index.html\n",
      "../books/nonfiction_13/index.html\n",
      "../books/music_14/index.html\n",
      "../books/default_15/index.html\n",
      "../books/science-fiction_16/index.html\n",
      "../books/sports-and-games_17/index.html\n",
      "../books/add-a-comment_18/index.html\n",
      "../books/fantasy_19/index.html\n",
      "../books/new-adult_20/index.html\n",
      "../books/young-adult_21/index.html\n",
      "../books/science_22/index.html\n",
      "../books/poetry_23/index.html\n",
      "../books/paranormal_24/index.html\n",
      "../books/art_25/index.html\n",
      "../books/psychology_26/index.html\n",
      "../books/autobiography_27/index.html\n",
      "../books/parenting_28/index.html\n",
      "../books/adult-fiction_29/index.html\n",
      "../books/humor_30/index.html\n",
      "../books/horror_31/index.html\n",
      "../books/history_32/index.html\n",
      "../books/food-and-drink_33/index.html\n",
      "../books/christian-fiction_34/index.html\n",
      "../books/business_35/index.html\n",
      "../books/biography_36/index.html\n",
      "../books/thriller_37/index.html\n",
      "../books/contemporary_38/index.html\n",
      "../books/spirituality_39/index.html\n",
      "../books/academic_40/index.html\n",
      "../books/self-help_41/index.html\n",
      "../books/historical_42/index.html\n",
      "../books/christian_43/index.html\n",
      "../books/suspense_44/index.html\n",
      "../books/short-stories_45/index.html\n",
      "../books/novels_46/index.html\n",
      "../books/health_47/index.html\n",
      "../books/politics_48/index.html\n",
      "../books/cultural_49/index.html\n",
      "../books/erotica_50/index.html\n",
      "../books/crime_51/index.html\n",
      "../../a-light-in-the-attic_1000/index.html\n",
      "../../a-light-in-the-attic_1000/index.html\n",
      "../../tipping-the-velvet_999/index.html\n",
      "../../tipping-the-velvet_999/index.html\n",
      "../../soumission_998/index.html\n",
      "../../soumission_998/index.html\n",
      "../../sharp-objects_997/index.html\n",
      "../../sharp-objects_997/index.html\n",
      "../../sapiens-a-brief-history-of-humankind_996/index.html\n",
      "../../sapiens-a-brief-history-of-humankind_996/index.html\n",
      "../../the-requiem-red_995/index.html\n",
      "../../the-requiem-red_995/index.html\n",
      "../../the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\n",
      "../../the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\n",
      "../../the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\n",
      "../../the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\n",
      "../../the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\n",
      "../../the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\n",
      "../../the-black-maria_991/index.html\n",
      "../../the-black-maria_991/index.html\n",
      "../../starving-hearts-triangular-trade-trilogy-1_990/index.html\n",
      "../../starving-hearts-triangular-trade-trilogy-1_990/index.html\n",
      "../../shakespeares-sonnets_989/index.html\n",
      "../../shakespeares-sonnets_989/index.html\n",
      "../../set-me-free_988/index.html\n",
      "../../set-me-free_988/index.html\n",
      "../../scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\n",
      "../../scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\n",
      "../../rip-it-up-and-start-again_986/index.html\n",
      "../../rip-it-up-and-start-again_986/index.html\n",
      "../../our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\n",
      "../../our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\n",
      "../../olio_984/index.html\n",
      "../../olio_984/index.html\n",
      "../../mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\n",
      "../../mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\n",
      "../../libertarianism-for-beginners_982/index.html\n",
      "../../libertarianism-for-beginners_982/index.html\n",
      "../../its-only-the-himalayas_981/index.html\n",
      "../../its-only-the-himalayas_981/index.html\n",
      "page-2.html\n"
     ]
    }
   ],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "res = requests.get(url, headers = user_agent)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "for link in soup.find_all(\"a\"): \n",
    "    print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__\n",
    "\n",
    "The links we want to ignore are...\n",
    "\n",
    "* \"Books to Scrape\" link at the top\n",
    "* \"Home\" breadcrumb link \n",
    "* Left sidebar with all book genres (e.g., Travel)\n",
    "* The next button at the bottom\n",
    "\n",
    "These links are present on the page, because they are used by users to navigate on the page. This can also be seen on the animation:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/books_overview.gif\" align=\"left\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Collecting *More Specific* Links\n",
    "\n",
    "__Importance__\n",
    "\n",
    "We've just discovered that selecting elements by their tags gives us many irrelevant links. But, how can we narrow down these links, or, in other words, __how can we scrape only the book links we're interested in?__.\n",
    "\n",
    "To answer this question, we need to briefly revisit the notion of __HTML classes__. \n",
    "\n",
    "A __class__ is often used as a reference in the code. For example, to make all text elements with a given class blue or increase the font size. In the Google Inspector screenshot shown earlier, you find an `<article>` tag with class `product_pod` in which a `<div>` is nested which contains the image and link attribute we're after. \n",
    "\n",
    "Every link to a book is *nested within this class* (nested = \"part of\"). The \"wrong links\" extracted above (i.e., the ones in the page's header and sidebar) are *not*. \n",
    "\n",
    "Thus, if we can tell our scraper that we're only interested in the `<a>` tags *within the `product_pod` class*, we end up with our desired selection of links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Like before, we'll use `.find_all()` to capture all matching elements on the page. The difference, however, is that we specify __a class (`class_=`)__, rather than an HTML tag. From the inspector, we know the class name (`product_pod`). \n",
    "\n",
    "This result is a list with __all 20 `product_pod` classes__ on the page (i.e., one for each book). \n",
    "\n",
    "Run the code below, in which we pick the __first book__ from the list (A Light in the Attic, element `[0]`), and extract the `<a>` tag nested within the `product_pod` class. \n",
    "\n",
    "Finally, we pull out the `href` attribute from the `<a>` tag which gives us the book link. Unlike the example above, we have selected only a single element (`[0]`) and therefore don't need to loop over all links with a `for`-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "soup.find_all(class_=\"product_pod\")[0].find(\"a\").attrs[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `../../` in front of the link which tells the browser: this tells the browser to go back two directories from the current URL:\n",
    "* Current URL: https://books.toscrape.com/catalogue/category/books_1/index.html\n",
    "* 1 step back: https://books.toscrape.com/catalogue/category/books_1\n",
    "* 2 steps back: https://books.toscrape.com/catalogue/category/\n",
    "\n",
    "Thereafter, it appends `a-light-in-the-attic_1000/index.html` to the URL which forms the full link to the [A Light in the Attic](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html) book. \n",
    "\n",
    "Pretty cool, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2\n",
    "1. Modify the script to extract the link from the *second book* (Tipping the Velvet), using BeautifulSoup.\n",
    "2. Create a new variable `book_url` that combines the base URL (` https://books.toscrape.com/catalogue/`) and the string you extracted in the previous exercise (`../../a-light-....`). You can remove the `../../` by using the `.replace('../../', '')` function on the URL. The final URL needs to be: `https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html` \n",
    "3. Write a function to collect all links (seeds) from this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tipping-the-velvet_999/index.html\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "url_book = soup.find_all(class_=\"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
    "print(url_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n"
     ]
    }
   ],
   "source": [
    "# Question 2 \n",
    "base_url = \"https://books.toscrape.com/catalogue/\" # gives a 403 error if you run the URL separately but works as expected once combined with the book url\n",
    "book_url = base_url + url_book.replace('../../', '')\n",
    "print(book_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       " 'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       " 'https://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       " 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       " 'https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-requiem-red_995/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-black-maria_991/index.html',\n",
       " 'https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',\n",
       " 'https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',\n",
       " 'https://books.toscrape.com/catalogue/set-me-free_988/index.html',\n",
       " 'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       " 'https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',\n",
       " 'https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',\n",
       " 'https://books.toscrape.com/catalogue/olio_984/index.html',\n",
       " 'https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',\n",
       " 'https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',\n",
       " 'https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3\n",
    "def get_all_links(url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'):\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = user_agent)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # return the href attribute in the <a> tag nested within the first product class element\n",
    "    urls = soup.find_all(class_=\"product_pod\")\n",
    "    \n",
    "    book_urls = []\n",
    "    for book in urls:\n",
    "        url_book = book.find(\"a\").attrs[\"href\"]\n",
    "        base_url = \"https://books.toscrape.com/catalogue/\"\n",
    "        book_url = base_url + url_book\n",
    "        book_url = book_url.replace('../', '')\n",
    "        book_urls.append(book_url)\n",
    "    \n",
    "    return(book_urls)\n",
    "\n",
    "get_all_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Page Navigation\n",
    "\n",
    "### 3.1. Using URLs\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Alright - what have we learnt up this point?\n",
    "\n",
    "We've learnt two ways to extract data (`requests` vs. `selenium`), and how to extract seeds from a page.\n",
    "\n",
    "So... what's missing?\n",
    "\n",
    "Exactly! The [`books.toscrape.com`](https://books.toscrape.com/catalogue/category/books_1/index.html) contains __1000 books__, spread across __50 pages__. \n",
    "\n",
    "So, the goal of this section is to navigate through the __entire book assortment__, not only the first 20 books!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Open [the website](https://books.toscrape.com/catalogue/category/books_1/index.html), and click on the \"next\" button at the bottom of the page.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/books.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Repeat this a couple of times, and observe how the URL in your navigation bar is changing...\n",
    "\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-1.html`\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-2.html`\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-3.html`\n",
    "\n",
    "Can you guess the next one...?\n",
    "\n",
    "Indeed! The URL can be divided into a __fixed base part__ (`https://books.toscrape.com/catalogue/category/books_1/`), and a __counter__ that is dependent on the page you're visiting (e.g., `page-1.html`). \n",
    "\n",
    "__Now let's create a list of all 50 URLs!__ \n",
    "\n",
    "First, we create a counter variable, which we now set to 1 (but it can take on any value later on). Then, we concatenate the `base_url` with the counter (note that we have to convert the integer counter to a string before we can do that, using the `str` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/page-1.html\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
    "print(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we generate a list of 50 `page_urls` with a for loop that starts at 1 and ends at 50 (not 51!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
    "page_urls = []\n",
    "\n",
    "for counter in range(1, 51):\n",
    "    full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
    "    page_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this gives a list of all page URLs that contain books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of page urls in the list is: 50\n"
     ]
    }
   ],
   "source": [
    "# print the last five page urls (btw, run print(page_urls) for yourself to see all page URLs!)\n",
    "print(\"The number of page urls in the list is: \" + str(len(page_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1\n",
    "In this exercise, we practice generating a seed for another website, [`quotes.toscrape.com`](https://quotes.toscrape.com/), which displays 100 famous quotes from GoodReads, categorized by tag. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/quotes.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make yourself comfortable with how the [site](https://quotes.toscrape.com) works and ask yourself questions such as: how does the navigation work, how many pages are there, what is the base URL, and how does it change if I move to the next page?\n",
    "2. Generate a list `quote_page_urls` that contains the page URLs we need if we'd like to scrape all 100 quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. The 100 quotes are evenly spread across 10 pages. The base URL is `https://quotes.toscrape.com/page/` followed by a page number between 1 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://quotes.toscrape.com/page/1', 'https://quotes.toscrape.com/page/2', 'https://quotes.toscrape.com/page/3', 'https://quotes.toscrape.com/page/4', 'https://quotes.toscrape.com/page/5', 'https://quotes.toscrape.com/page/6', 'https://quotes.toscrape.com/page/7', 'https://quotes.toscrape.com/page/8', 'https://quotes.toscrape.com/page/9', 'https://quotes.toscrape.com/page/10']\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "base_url = \"https://quotes.toscrape.com/page/\"\n",
    "quote_page_urls = []\n",
    "\n",
    "for counter in range(1, 11):\n",
    "    full_url = base_url + str(counter)\n",
    "    quote_page_urls.append(full_url)\n",
    "\n",
    "print(quote_page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using links contained in elements (e.g., buttons)\n",
    "\n",
    "__Importance__\n",
    "\n",
    "For now, the book link extraction has worked without problems. Yet, there's still one little improvement that we can make. *If the number of pages changes*, we need to manually update for how many pages we would like to retrieve seeds.\n",
    "\n",
    "A general solution is therefore to look up whether there is a `next` button on the page (see HTML code below). We can then either \"grab\" the URL and visit it (so, in essence, we're still using URLs to navigate), or - instead - \"click\" on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/next_page.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "So, let's write a snippet that \"captures\" the link of the next page button.\n",
    "\n",
    "We always proceed in small steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the website's source code and convert to BeautifulSoup object\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = user_agent)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"next\"><a href=\"page-2.html\">next</a></li>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Trying to locate the \"next\" class.\n",
    "soup.find(class_='next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Trying to locate the <a> tag within the \"next\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"page-2.html\">next</a>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(class_='next').find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'page-2.html'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Trying to extract the link ('href' attribute)\n",
    "soup.find(class_='next').find('a')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, we can observe how we're getting closer to the information we need.\n",
    "\n",
    "Now, we only need to combine the base URL with the page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/category/books_1/page-2.html'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://books.toscrape.com/catalogue/category/books_1/'\n",
    "next_page = soup.find(class_='next').find('a')['href']\n",
    "base_url + next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.2__\n",
    "\n",
    "Please first load the snippet below, which has wrapped the \"next page\" capturing in a function. Observe the use of `try` and `except`, which accounts for the last page NOT having a next page button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://books.toscrape.com/catalogue/category/books_1/'\n",
    "\n",
    "def next_page(url):\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = user_agent)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    try:\n",
    "        next_page = soup.find(class_='next').find('a')['href']\n",
    "    except:\n",
    "        next_page = 'no next page'\n",
    "    return(base_url + next_page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Pass `https://books.toscrape.com/catalogue/page-49.html` to `next_page()` and observe the output. Then, use  `https://books.toscrape.com/catalogue/page-50.html`. Is that what you expected? \n",
    "\n",
    "2. Write a while loop that assembles a list of all product pages for the book category (`'https://books.toscrape.com/catalogue/category/books_1/'`), by extracting next page URLs from each page and appending them to an array/list called `urls`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-3.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-4.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-5.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-6.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-7.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-8.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-9.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-10.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-11.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-12.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-13.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-14.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-15.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-16.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-17.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-18.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-19.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-20.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-21.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-22.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-23.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-24.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-25.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-26.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-27.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-28.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-29.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-30.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-31.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-32.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-33.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-34.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-35.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-36.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-37.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-38.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-39.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-40.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-41.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-42.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-43.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-44.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-45.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-46.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-47.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-48.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-49.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-50.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-5.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-6.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-7.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-8.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-9.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-10.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-11.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-12.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-13.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-14.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-15.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-16.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-17.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-18.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-19.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-20.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-21.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-22.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-23.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-24.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-25.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-26.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-27.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-28.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-29.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-30.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-31.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-32.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-33.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-34.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-35.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-36.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-37.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-38.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-39.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-40.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-41.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-42.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-43.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-44.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-45.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-46.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-47.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-48.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-49.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-50.html']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "url = base_url\n",
    "\n",
    "while True:\n",
    "    print('Trying to get next page URL from ' + url)\n",
    "    next_url = next_page(url)\n",
    "    if 'no next page' in next_url: break\n",
    "    url = next_url\n",
    "    urls.append(url)\n",
    "    \n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Using interactive elements (e.g., by clicking buttons)\n",
    "\n",
    "__Importance__\n",
    "\n",
    "For more dynamic websites, we may have to click on certain elements (rather than extracting some URL).\n",
    "\n",
    "__Try it out__\n",
    "\n",
    "If you haven't done so, rerun the installation code for `selenium` from above. Then, proceed by running the following cell and observe what happens in your browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://books.toscrape.com/catalogue/category/books_1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, your browser will have loaded the website in Chrome. Now, run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"b91f5618425843d7f2a25da8f50a3651\", element=\"92256b27-cc69-4967-8aa5-158572f78c15\")>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Let's try location the element\n",
    "from selenium.webdriver.common.by import By\n",
    "driver.find_element(By.CLASS_NAME, 'next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"b91f5618425843d7f2a25da8f50a3651\", element=\"b044c417-c0b2-4de2-9249-011a4eaa9475\")>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Finding the link within the `next` class\n",
    "driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clicking the link!\n",
    "driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! In step 3, we finally clicked on the link. Just try rerunning this cell with step 3 over and over again. Does iterating through the pages work?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.3__\n",
    "\n",
    "Iterate through the entire set of pages, until there are no new pages left. This time, use `selenium` and click on the next page button. You can start on page 47 (`https://books.toscrape.com/catalogue/category/books_1/page-47.html`) to speed up this exercise a bit.\n",
    "\n",
    "Make use of the `time.sleep(2)` function to make the code wait a bit after each page load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "urls = []\n",
    "driver.get('https://books.toscrape.com/catalogue/category/books_1/page-47.html')\n",
    "time.sleep(1)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a').click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Collecting all seeds\n",
    "\n",
    "Up to this moment, we have defined what seeds are (crucially important for sampling!), and introduced several ways through which you can navigate on a site. The only thing that's missing is combining these two things: navigating through all of the available pages, and collecting seeds for which we can later extract data.\n",
    "\n",
    "__Exercise 3.4__\n",
    "\n",
    "Using the solution from exercise 3.2, write code that navigates through all pages of the book category and stores product URLs in a list of dictionaries, containing the following data points:\n",
    "- product URL\n",
    "- URL from which page the product URL was captured\n",
    "- current time stamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-3.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-4.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'product_url': 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-requiem-red_995/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-black-maria_991/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/set-me-free_988/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/olio_984/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1658753809},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/in-her-wake_980/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/how-music-works_979/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/foolproof-preserving-a-guide-to-small-batch-jams-jellies-pickles-condiments-and-more-a-foolproof-guide-to-making-small-batch-jams-jellies-pickles-condiments-and-more_978/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/chase-me-paris-nights-2_977/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/black-dust_976/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/birdsong-a-story-in-pictures_975/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/americas-cradle-of-quarterbacks-western-pennsylvanias-football-factory-from-johnny-unitas-to-joe-montana_974/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/aladdin-and-his-wonderful-lamp_973/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/worlds-elsewhere-journeys-around-shakespeares-globe_972/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/wall-and-piece_971/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-four-agreements-a-practical-guide-to-personal-freedom_970/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-five-love-languages-how-to-express-heartfelt-commitment-to-your-mate_969/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-elephant-tree_968/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-bear-and-the-piano_967/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sophies-world_966/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/penny-maybe_965/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/maude-1883-1993she-grew-up-with-the-country_964/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/behind-closed-doors_962/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/you-cant-bury-them-all-poems_961/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/slow-states-of-collapse-poems_960/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/reasons-to-stay-alive_959/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/private-paris-private-10_958/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/higherselfie-wake-up-your-life-free-your-soul-find-your-tribe_957/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/without-borders-wanderlove-1_956/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/when-we-collided_955/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/we-love-you-charlie-freeman_954/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/untitled-collection-sabbath-poems-2014_953/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unseen-city-the-majesty-of-pigeons-the-discreet-charm-of-snails-other-wonders-of-the-urban-wilderness_952/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unicorn-tracks_951/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unbound-how-eight-technologies-made-us-human-transformed-society-and-brought-our-world-to-the-brink_950/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/tsubasa-world-chronicle-2-tsubasa-world-chronicle-2_949/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/throwing-rocks-at-the-google-bus-how-growth-became-the-enemy-of-prosperity_948/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/this-one-summer_947/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/thirst_946/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-torch-is-passed-a-harding-family-story_945/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-secret-of-dreadwillow-carse_944/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-pioneer-woman-cooks-dinnertime-comfort-classics-freezer-food-16-minute-meals-and-other-delicious-ways-to-solve-supper_943/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-natural-history-of-us-the-fine-art-of-pretending-2_941/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1658753810},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-nameless-city-the-nameless-city-1_940/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-murder-that-never-was-forensic-instincts-5_939/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-most-perfect-thing-inside-and-outside-a-birds-egg_938/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-mindfulness-and-acceptance-workbook-for-anxiety-a-guide-to-breaking-free-from-anxiety-phobias-and-worry-using-acceptance-and-commitment-therapy_937/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-life-changing-magic-of-tidying-up-the-japanese-art-of-decluttering-and-organizing_936/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-inefficiency-assassin-time-management-tactics-for-working-smarter-not-longer_935/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-gutsy-girl-escapades-for-your-life-of-epic-adventure_934/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-electric-pencil-drawings-from-inside-state-hospital-no-3_933/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-death-of-humanity-and-the-case-for-life_932/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-bulletproof-diet-lose-up-to-a-pound-a-day-reclaim-energy-and-focus-upgrade-your-life_931/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-art-forger_930/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-age-of-genius-the-seventeenth-century-and-the-birth-of-the-modern-mind_929/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-activists-tao-te-ching-ancient-advice-for-a-modern-revolution_928/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/spark-joy-an-illustrated-master-class-on-the-art-of-organizing-and-tidying-up_927/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/soul-reader_926/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/security_925/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/saga-volume-6-saga-collected-editions-6_924/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/saga-volume-5-saga-collected-editions-5_923/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/reskilling-america-learning-to-labor-in-the-twenty-first-century_922/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/rat-queens-vol-3-demons-rat-queens-collected-editions-11-15_921/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1658753811}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "seeds = []\n",
    "base_url = 'https://books.toscrape.com/catalogue/category/books_1/'\n",
    "url = base_url #initialize for first page\n",
    "counter = 0 #initialize counter so that you can break earlier from this loop when needed\n",
    "\n",
    "while True:\n",
    "    counter+=1\n",
    "    \n",
    "    #if (counter>4): break # deactivate this comment if you want to break after x iterations for prototyping\n",
    "    \n",
    "    print('Trying to get next page URL from ' + url)\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    # extract information\n",
    "    urls = soup.find_all(class_=\"product_pod\")\n",
    "    for book in urls:\n",
    "        url_book = book.find(\"a\").attrs[\"href\"]\n",
    "        book_url = \"https://books.toscrape.com/catalogue/\" + url_book\n",
    "        book_url = book_url.replace('../', '')\n",
    "        seeds.append({'product_url': book_url,\n",
    "                      'page_url': url,\n",
    "                      'timestamp': int(time.time())})\n",
    "    # next page available?\n",
    "    try:\n",
    "        url = base_url + soup.find(class_='next').find('a')['href']\n",
    "    except:\n",
    "        break # no next page present\n",
    "    \n",
    "seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Extraction\n",
    "\n",
    "\n",
    "### 4.1 Timers\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Before we started running some of the cells above, you may have observed the usage of the `time.sleep` function. Sending many requests at the same time can overload a server. Therefore, it's highly recommended to pause between requests rather than sending them all simultaneously. This avoids that your IP address (i.e., numerical label assigned to each device connected to the internet) gets blocked, and you can no longer visit (and scrape) the website. \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "In Python, you can import the `time` module, which pauses the execution of future commands for a given amount of time. For example, the print statement after `time.sleep(5)` will only be executed after 3 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll be printed to the console after 3 seconds!\n"
     ]
    }
   ],
   "source": [
    "# run this cell again to see the timer in action yourself!\n",
    "import time\n",
    "time.sleep(3)\n",
    "print(\"I'll be printed to the console after 3 seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.1__\n",
    "\n",
    "Modify the code above to sleep for 2 minutes. Go grab a coffee inbetween. Did it take you longer than 2 minutes?\n",
    "\n",
    "(if you want to abort the running code, just select the cell and push the \"stop\" button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "time.sleep(2*60)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Modularization\n",
    "\n",
    "**Importance**  \n",
    "\n",
    "In scraping, many things have to be executed *multiple times*. For example, whenever we open a new page on books.toscrape.com, we would like to extract all the available book links.\n",
    "\n",
    "To help us execute things over and over again, we will \"modularize\" our code into functions. We can then call these functions whenever we need them. Another benefit from using functions is that we can improve the readability and reusability of our code. If you need a quick refresher on functions, please revisit section 4 of the [Python Bootcamp](https://odcm.hannesdatta.com/docs/tutorials/pythonbootcamp/).\n",
    "\n",
    "**Let's try it out**\n",
    "\n",
    "Let's finish up our book URL scraper by putting together everything we have learned thus far.\n",
    "\n",
    "1. We need a function that extracts all seeds, given a category URL. We would like to store these seeds in a JSON file.\n",
    "2. We need a function that opens this JSON file, and captures all of the relevant product information (for now, let's use the title and price).\n",
    "\n",
    "__Exercise 4.2__\n",
    "\n",
    "Write a function to accomplish (1) above? (capturing the seeds and storing them in a JSON file)? Start with the solution in 3.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-3.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-4.html\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_seeds(base_url = 'https://books.toscrape.com/catalogue/category/books_1/'):\n",
    "    seeds = []\n",
    "    url = base_url #initialize for first page\n",
    "    counter = 0 #initialize counter so that you can break earlier from this loop when needed\n",
    "\n",
    "    while True:\n",
    "        counter+=1\n",
    "\n",
    "        if (counter>4): break # deactivate this comment if you want to break after x iterations for prototyping\n",
    "\n",
    "        print('Trying to get next page URL from ' + url)\n",
    "        user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers = user_agent)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # extract information\n",
    "        urls = soup.find_all(class_=\"product_pod\")\n",
    "        for book in urls:\n",
    "            url_book = book.find(\"a\").attrs[\"href\"]\n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + url_book\n",
    "            book_url = book_url.replace('../', '')\n",
    "            seeds.append({'product_url': book_url,\n",
    "                          'page_url': url,\n",
    "                          'timestamp': int(time.time())})\n",
    "        # next page available?\n",
    "        try:\n",
    "            url = base_url + soup.find(class_='next').find('a')['href']\n",
    "        except:\n",
    "            break # no next page present\n",
    "\n",
    "    return(seeds)\n",
    "\n",
    "data = get_seeds('https://books.toscrape.com/catalogue/category/books_1/')\n",
    "\n",
    "import json\n",
    "f = open('seeds.json','w',encoding = 'utf-8')\n",
    "for item in data:\n",
    "        f.write(json.dumps(item))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.3__\n",
    "\n",
    "Now, let's write some code that loads `seeds.json`, and visits each of the websites to extract the product title and price. Remember to build in a little timer (e.g., waiting for 1 second). The prototype/starting code below stops automatically after 5 iterations to minimize server load. Try removing the prototyping condition using the comment character `#` when you think you're done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n",
      "https://books.toscrape.com/catalogue/soumission_998/index.html\n",
      "https://books.toscrape.com/catalogue/sharp-objects_997/index.html\n",
      "https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n"
     ]
    }
   ],
   "source": [
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    print(obj['product_url'])\n",
    "    \n",
    "    # eventually sleep for a second\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n",
      "https://books.toscrape.com/catalogue/soumission_998/index.html\n",
      "https://books.toscrape.com/catalogue/sharp-objects_997/index.html\n",
      "https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n"
     ]
    }
   ],
   "source": [
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    \n",
    "    counter = counter + 1\n",
    "    if counter>5: break\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    print(obj['product_url'])\n",
    "    \n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    req = requests.get(obj['product_url'], headers = user_agent)\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    \n",
    "    retrieved_data = {'title': soup.find('h1').get_text(),\n",
    "                      'price': soup.find(class_='price_color').get_text(),\n",
    "                      'timestamp_retrieval': int(time.time())}\n",
    "        \n",
    "    f = open('book_data.json', 'a', encoding = 'utf-8')\n",
    "    f.write(json.dumps(retrieved_data))\n",
    "    f.write('\\n')\n",
    "    f.close() \n",
    "    \n",
    "    # eventually sleep for a second\n",
    "    time.sleep(1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp_retrieval</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:45</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Product Description</td>\n",
       "      <td>2022-07-25 13:44:48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>2022-07-25 13:45:26</td>\n",
       "      <td>√Ç¬£51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>2022-07-25 13:45:27</td>\n",
       "      <td>√Ç¬£53.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>2022-07-25 13:45:29</td>\n",
       "      <td>√Ç¬£50.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>2022-07-25 13:45:30</td>\n",
       "      <td>√Ç¬£47.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>2022-07-25 13:45:31</td>\n",
       "      <td>√Ç¬£54.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title timestamp_retrieval    price\n",
       "0                     Product Description 2022-07-25 13:44:09      NaN\n",
       "1                     Product Description 2022-07-25 13:44:11      NaN\n",
       "2                     Product Description 2022-07-25 13:44:12      NaN\n",
       "3                     Product Description 2022-07-25 13:44:14      NaN\n",
       "4                     Product Description 2022-07-25 13:44:15      NaN\n",
       "5                     Product Description 2022-07-25 13:44:42      NaN\n",
       "6                     Product Description 2022-07-25 13:44:44      NaN\n",
       "7                     Product Description 2022-07-25 13:44:45      NaN\n",
       "8                     Product Description 2022-07-25 13:44:46      NaN\n",
       "9                     Product Description 2022-07-25 13:44:48      NaN\n",
       "10                   A Light in the Attic 2022-07-25 13:45:26  √Ç¬£51.77\n",
       "11                     Tipping the Velvet 2022-07-25 13:45:27  √Ç¬£53.74\n",
       "12                             Soumission 2022-07-25 13:45:29  √Ç¬£50.10\n",
       "13                          Sharp Objects 2022-07-25 13:45:30  √Ç¬£47.82\n",
       "14  Sapiens: A Brief History of Humankind 2022-07-25 13:45:31  √Ç¬£54.23"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data in pandas\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_json('book_data.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Wrap-up\n",
    "\n",
    "At the beginning of this tutorial, we set out the promise of writing multi-page scrapers from start to finish. Although the examples we have studied are relatively simple, the same principles (seed definition, data extraction plan, page-level data collection) apply to any other website you'd like to scrape. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After-class exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Extending the code written for exercise 4.3 in \"Web data 101\", please collect seeds from ten self-chosen product categories and store them in a file called `all_seeds.json`.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Please use the code written in exercise 4.4 in \"Web Data 101\" and extend it so capture more information (e.g., not only title and price, but also as other attributes/data points you are interested in. In particular, try getting the product description!\n",
    "\n",
    "Try running your code and store the product data in a JSON dictionary called `all_books.json`.\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Please complete an entire data collection project in a `.py` file, capturing data for 10 product categories and all products contained on all of the pages. You can proceed in two steps: first collect the seeds, then obtain all data. In addition, parse all retrieved data to a CSV file (with rows and columns), using `pd.read_json(filename, lines = True)` for reading in the JSON data, and `pd.to_csv(filename)` for saving the data in tabular format.\n",
    "\n",
    "Run your data collection from the terminal.\n",
    "\n",
    "The final deliverable is\n",
    "- `all_seeds.json`\n",
    "- `all_books.json`\n",
    "- `all_books.csv`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup: Executing Python Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebooks versus Spyder\n",
    "\n",
    "Jupyter Notebooks are ideal for combining programming and markdown (e.g., text, plots, equations), making it the default choice for sharing and presenting reproducible data analyses. Since we can execute code blocks one by one, it's suitable for developing and debugging code on the fly. \n",
    "\n",
    "That said, Jupyter Notebooks also have some severe limitations when using them in production environments. That's where an \"Integrated Development Environment\" (IDE) comes in, such as Spyder or PyCharm. A fancy word, we know. So, let's revisit the most important differences.\n",
    "\n",
    "First, the order in which you run cells within a notebook may affect the results. While prototyping, you may lose sight of the top-down hierarchy, which can cause problems once you restart the kernel (e.g., a library is imported after it is being used). Second, there is no easy way to browse through directories and files within a Jupyter Notebook. Third, notebooks cannot handle large codebases nor big data remarkably well. \n",
    "\n",
    "That's why we recommend starting in Jupyter Notebooks, moving code into functions along the way, and once all seems to be running well, copy-paste all necessary code into Spyder. From there, you can save it as a Python file (`.py`) - rather than a notebook (`.ipynb`) - and execute the file from the command line. In this tutorial, we introduce you to the Spyder IDE and learn how to run Python files from the command line. The reason we choose for the Spyder IDE instead of PyCharm, for example, is because Spyder is already installed with Anaconda. In the future, you can always use PyCharm or another text editor to write your python scripts if you prefer! \n",
    "\n",
    "### Introduction to Spyder\n",
    "The first time you need to click on the green \"Install\" button in Anaconda Navigator, after which you start Spyder by clicking on the blue \"Launch\" button (alternatively, type `spyder` in the terminal). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/anaconda_navigator.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interface consists of three panels: \n",
    "1. **Code editor** = where you write Python code (i.e., the content of code cells in a notebook)\n",
    "2. **Variable / files** = depending on which tab you choose either an overview of all declared variables (e.g. look up their type or change their values) or a file explorer (e.g., to open other Python files)\n",
    "3. **Console** = the output of running the Python script from the code editor (what normally appears below each cell in a notebook)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/spyder.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**     \n",
    "Copy the solution from exercise 4.3 to a new file, called `webscraping_101.py`. To run the script you can\n",
    "\n",
    "- click on the green play button to run all code, or\n",
    "- highlight the parts of the script you want to execute and then click the run selection button.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/toolbar.png\" width=40% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the script is running, you may need to interrupt the execution because it is simply taking too long or you spotted a bug somewhere. Click on the red rectangular in the console to stop the execution. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/interrupt.gif\" width=80% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Python Files \n",
    "\n",
    "__For Mac and Linux users__\n",
    "\n",
    "1. Open the terminal and navigate to the folder in which the `.py` file has been saved (use `cd` to change directories and `ls` to list all files).\n",
    "2. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/running_python.gif\" width=60% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For Windows users__\n",
    "\n",
    "1. Open Windows explorer and navigate to the folder in which the `.py` file has been saved. Type `cmd` to open the command prompt. Alternatively, open the command prompt from the start menu (and use `cd` to change directories and `dir` to list files).\n",
    "2. Activate Anaconda by typing `conda activate`.\n",
    "3. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
