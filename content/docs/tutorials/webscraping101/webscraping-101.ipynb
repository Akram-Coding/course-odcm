{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101 (in-class)\n",
    "\n",
    "*After finishing this tutorial, you can extract data from multiple pages on the web, and export such data to CSV files so that you can use it in an analysis. Plan a few hours to work through this notebook. Taking a few breaks inbetween keeps you sharp!*\n",
    "\n",
    "*Just starting out with web scraping? Then make sure to have followed the [\"webdata for dummies\" tutorial](https://odcm.hannesdatta.com/docs/tutorials/webdata-for-dummies/) first.*\n",
    "\n",
    "*Enjoy!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Identifying a strategy to generating seeds (“sampling”)\n",
    "* Navigating on a website using URLs\n",
    "* Extracting multiple elements at once \n",
    "    * using the `.find_all()` function\n",
    "    * preventing array misalignment\n",
    "* Improving extraction design\n",
    "    * Storing data in CSV or JSON files, and enrich with relevant metadata\n",
    "    * Writing loops to execute data collections in bulk using functions\n",
    "    * Implementing timers and modularizing extraction code\n",
    "* Scraping more advanced, dynamic websites\n",
    "    * Understanding the difference between headless and browser emulation \n",
    "    * Learn when to apply one of the two methods (using `requests` and `selenium`)\n",
    "\n",
    "--- \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Support Needed?</b> \n",
    "    For technical issues outside of scheduled classes, please check the <a href=\"https://odcm.hannesdatta.com/docs/course/support\" target=\"_blank\">support section</a> on the course website.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating seeds (\"sampling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "So far, we've extracted (=parsed) some information (e.g., titles, product names, prices) from products' individual *product pages*. What we haven't done yet is decide for __which products to obtain that information__. Ideally, we would like to capture information for a *sample of books* (or users, movies, series, etc.).\n",
    "\n",
    "In web scraping, we typically refer to a \"seed\" as a starting point for a data collection. Without a seed, there's no data to collect.\n",
    "\n",
    "For example, before we can crawl through all books available on [this site](https://books.toscrape.com/catalogue/category/books_1/index.html), we first need to generate a *list of all books on the page*.\n",
    "\n",
    "One way to get there would be to:\n",
    "\n",
    "1. first scrape all book links (“seeds”) from the overview page, and \n",
    "2. then iterate over all links to scrape the product description (or anything else on that page; we have done this in the webdata for dummies tutorial).\n",
    "\n",
    "Note that the overview page allows us to \"navigate\" to the individual book pages, either by clicking on the book cover or the book title (see red boxes in the figure below). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/books_links.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Collecting links to use as seeds\n",
    "\n",
    "Let's check out how the links from the book covers or book titles are encoded in the website's source code.\n",
    "\n",
    "Open the [book catalogue](https://books.toscrape.com/catalogue/category/books_1/index.html), and inspect the underlying HTML code with the Chrome Inspector (right click --> inspect element). \n",
    "\n",
    "The book covers (`<img>`) are surrounded by `<a>` tags, which contain a link (`href`) to the book. \n",
    "\n",
    "Also, the book titles (`<h3>`) are surrounded by `<a>` tags with the relevant links to the book pages.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/inspector_links.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we tell a computer to capture the links to the various books on the site?\n",
    "\n",
    "One simple way is to select *elements by their tags*. For example, to extract all links (`<a>` tags). \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>How to extract multiple elements at once?</b>\n",
    "    <br>\n",
    "    \n",
    "- By working through other tutorials, you may already be familiar with the <code>.find()</code> function of BeautifulSoup. The <code>.find()</code> function returns the <b>first element</b> that matches your particular \"search query\". <br>\n",
    "- If you want to extract <b>all elements</b> that match a particular search pattern (say, a class name), you can use BeautifulSoup's <code>.find_all()</code> function.<br>\n",
    "- Note that the \"result\" of the <code>.find_all()</code> option is a list of results that you need to iterate through.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.1__\n",
    "\n",
    "Please run the code cell below, which extracts all links (the `a` tag!), and prints the URL (`href`) to the screen. Don't worry, you don't need need to understand the code yet, we'll go over it line by line shortly!\n",
    "\n",
    "If you look at these links more closely, you'll notice that we're not interested in many of these links... \n",
    "\n",
    "Make a list of all links we're *not* interested in (i.e., those *not* pointing to a book page). Which ones are those? Can you find out why they are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib3.connectionpool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-83a0ddfecaeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run this code now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# make a get request to the books overview page (see Webdata for Dummies tutorial)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \"\"\"\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconnectionpool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTTPConnectionPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHTTPSConnectionPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection_from_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'urllib3.connectionpool'"
     ]
    }
   ],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "for link in soup.find_all(\"a\"): \n",
    "    print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__\n",
    "\n",
    "The links we want to ignore are...\n",
    "\n",
    "* \"Books to Scrape\" link at the top\n",
    "* \"Home\" breadcrumb link \n",
    "* Left sidebar with all book genres (e.g., Travel)\n",
    "* The next button at the bottom\n",
    "\n",
    "These links are present on the page, because they are used by users to navigate on the page. This can also be seen on the animation:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/books_overview.gif\" align=\"left\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Collecting *More Specific* Links\n",
    "\n",
    "__Importance__\n",
    "\n",
    "We've just discovered that selecting elements by their tags gives us many irrelevant links. But, how can we narrow down these links, or, in other words, __how can we scrape only the book links we're interested in?__.\n",
    "\n",
    "To answer this question, we need to briefly revisit the notion of how an HTML code is structured. Open your browser's inspect mode again and hover over the product pictures on the site.\n",
    "\n",
    "After inspecting, you'd probably notice that the page is generated according to a rigid structure: all product links are contained in a `<div>` tag, with the class name `product_pod`. The \"wrong links\" extracted above (i.e., the ones in the page's header and sidebar) are *not* part of these elements. \n",
    "\n",
    "So, if we can tell our scraper that we're only interested in the `<a>` tags *within the `product_pod` class*, we end up with our desired selection of links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Like before, we'll use `.find_all()` to capture all matching elements on the page. The difference, however, is that we do not directly try to extract the __links__ with the tag `a`, but first try to obtain a __list with product containers__ identified by the classname `product_pod`.\n",
    "\n",
    "Run the code below, in which we first try to capture all book containers using the `product_pod` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers=header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# return all book containers\n",
    "books = soup.find_all(class_=\"product_pod\")\n",
    "len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we retrieve 20 book containers. You can now also use the books object to look at the data for the first, second, third, ... book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<article class=\"product_pod\">\n",
       "<div class=\"image_container\">\n",
       "<a href=\"../../a-light-in-the-attic_1000/index.html\"><img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"../../../media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/></a>\n",
       "</div>\n",
       "<p class=\"star-rating Three\">\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "</p>\n",
       "<h3><a href=\"../../a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the ...</a></h3>\n",
       "<div class=\"product_price\">\n",
       "<p class=\"price_color\">£51.77</p>\n",
       "<p class=\"instock availability\">\n",
       "<i class=\"icon-ok\"></i>\n",
       "    \n",
       "        In stock\n",
       "    \n",
       "</p>\n",
       "<form>\n",
       "<button class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\" type=\"submit\">Add to basket</button>\n",
       "</form>\n",
       "</div>\n",
       "</article>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...to subsequently try to extract the link for the first book..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0].find('a')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the second book..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../tipping-the-velvet_999/index.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[1].find('a')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or all books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../a-light-in-the-attic_1000/index.html',\n",
       " '../../tipping-the-velvet_999/index.html',\n",
       " '../../soumission_998/index.html',\n",
       " '../../sharp-objects_997/index.html',\n",
       " '../../sapiens-a-brief-history-of-humankind_996/index.html',\n",
       " '../../the-requiem-red_995/index.html',\n",
       " '../../the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',\n",
       " '../../the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',\n",
       " '../../the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',\n",
       " '../../the-black-maria_991/index.html',\n",
       " '../../starving-hearts-triangular-trade-trilogy-1_990/index.html',\n",
       " '../../shakespeares-sonnets_989/index.html',\n",
       " '../../set-me-free_988/index.html',\n",
       " '../../scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       " '../../rip-it-up-and-start-again_986/index.html',\n",
       " '../../our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',\n",
       " '../../olio_984/index.html',\n",
       " '../../mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',\n",
       " '../../libertarianism-for-beginners_982/index.html',\n",
       " '../../its-only-the-himalayas_981/index.html']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "for book in books:\n",
    "    links.append(book.find('a')['href'])\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `../../` in front of the link which tells the browser: this tells the browser to go back two directories from the current URL:\n",
    "* Current URL: https://books.toscrape.com/catalogue/category/books_1/index.html\n",
    "* 1 step back: https://books.toscrape.com/catalogue/category/books_1\n",
    "* 2 steps back: https://books.toscrape.com/catalogue/category/\n",
    "\n",
    "Thereafter, it appends `a-light-in-the-attic_1000/index.html` to the URL which forms the full link to the [A Light in the Attic](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html) book. \n",
    "\n",
    "Pretty cool, right? So let's proceed with some exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2\n",
    "1. Modify the loop (`for book in books`) above to extract the *absolute URLs* rather than the relative URLs. Specifically, combine the website's URL (`https://books.toscrape.com/catalogue/`) and the string you extracted in the previous code snippet (`../../a-light-....`). You can remove the `../../` by using the `.replace('../../', '')` function on the URL. The final URL needs to be: `https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html` \n",
    "2. Write a function to collect all links (seeds) from this page, i.e., including loading packages, making the HTTP request, and returning the information as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       " 'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       " 'https://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       " 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       " 'https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-requiem-red_995/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-black-maria_991/index.html',\n",
       " 'https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',\n",
       " 'https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',\n",
       " 'https://books.toscrape.com/catalogue/set-me-free_988/index.html',\n",
       " 'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       " 'https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',\n",
       " 'https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',\n",
       " 'https://books.toscrape.com/catalogue/olio_984/index.html',\n",
       " 'https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',\n",
       " 'https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',\n",
       " 'https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1 \n",
    "links = []\n",
    "for book in books:\n",
    "    extracted_link = book.find('a')['href'].replace('../../','')\n",
    "    combined_link = \"https://books.toscrape.com/catalogue/\" + extracted_link\n",
    "    links.append(combined_link)\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting links from page https://books.toscrape.com/catalogue/category/books_1/index.html.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       " 'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       " 'https://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       " 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       " 'https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-requiem-red_995/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-black-maria_991/index.html',\n",
       " 'https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',\n",
       " 'https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',\n",
       " 'https://books.toscrape.com/catalogue/set-me-free_988/index.html',\n",
       " 'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       " 'https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',\n",
       " 'https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',\n",
       " 'https://books.toscrape.com/catalogue/olio_984/index.html',\n",
       " 'https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',\n",
       " 'https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',\n",
       " 'https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_all_links(url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'):\n",
    "    # make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "    print(f'Getting links from page {url}.')\n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers=header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "\n",
    "    links = []\n",
    "    for book in books:\n",
    "        extracted_link = book.find('a')['href'].replace('../../','')\n",
    "        combined_link = \"https://books.toscrape.com/catalogue/\" + extracted_link\n",
    "        links.append(combined_link)\n",
    "    return(links) # to return all links\n",
    "\n",
    "get_all_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Preventing array misalignment\n",
    "\n",
    "So far, we have only extracted *one* piece of information (the URL) from the product overview pages. But, what if we want to use the product overview page to extract multiple data points (say, about the price and the review valence)?\n",
    "\n",
    "A simple solution may be to just use multiple `.find_all()` commands.\n",
    "\n",
    "__Example__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib3.connectionpool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8fb64d9a5f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run this code now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'User-agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \"\"\"\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconnectionpool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTTPConnectionPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHTTPSConnectionPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection_from_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'urllib3.connectionpool'"
     ]
    }
   ],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "res = requests.get(url, headers=header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# getting the titles\n",
    "book_titles = []\n",
    "for title in soup.find_all('h3'): book_titles.append(title.get_text())\n",
    "\n",
    "# getting the valence\n",
    "stars = []\n",
    "for star in soup.find_all(class_='star-rating'): stars.append(star.attrs['class'][1])\n",
    "\n",
    "# book titles\n",
    "print(book_titles)\n",
    "\n",
    "# stars\n",
    "print(stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this approach seems easily implemented, it is __highly error-prone and needs to be avoided.__\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>What's an array misalignment?</b>\n",
    "    <br>\n",
    "    \n",
    "<ul>\n",
    "<li>\n",
    "When extracting information from the web, we sometimes are prone to \"ripping apart\" the website's original structure by putting data points into individual arrays (e.g., lists such as one list for book titles and another for stars). </li>\n",
    "<li>In so doing, we violate the data's original structure: we should store information on books, and <b>each book</b> has a title and rating.</li>\n",
    "    <li>The <b>correct way of organizing the data</b> is to create a list of books (e.g., in a dictionary) and then list each attribute (i.e., the title, the valence, etc.) within these objects. <b>Only if we store data this way</b> can we be sure to store everything correctly. </li>\n",
    "<br>\n",
    "<li>When we do not adhere to this practice, we run the risk of \"array misalignment\". For example, if only ONE data point were missing for a book, then the (independent) book_titles array (say, with 20 items) wouldn't be \"1:1 aligned\" with the valence array (say, with only 19 items).</li>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So, how to do it correctly?__\n",
    "\n",
    "We will first have to iterate through each __book__, and within each book extract the information.\n",
    "\n",
    "Storing the information in a list of dictionaries corresponds most to this solution (see the example below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib3.connectionpool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3594f36a069f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'User-agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://books.toscrape.com/catalogue/category/books_1/index.html'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \"\"\"\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconnectionpool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTTPConnectionPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHTTPSConnectionPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection_from_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'urllib3.connectionpool'"
     ]
    }
   ],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "res = requests.get(url, headers=header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# loop through the books\n",
    "books = []\n",
    "for book in soup.find_all(class_='image_container'):\n",
    "    title = book.find('h3').get_text()\n",
    "    valence = book.find(class_='star-rating').attrs['class'][1]\n",
    "    \n",
    "    obj = {'title': title,\n",
    "           'valence': valence}\n",
    "    \n",
    "    books.append(obj)\n",
    "    \n",
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Page Navigation\n",
    "\n",
    "### 2.1. Using URLs\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Alright - what have we learnt up this point?\n",
    "\n",
    "We've learnt how to extract seeds from __one page.__\n",
    "\n",
    "So... what's missing?\n",
    "\n",
    "Exactly! The [`books.toscrape.com`](https://books.toscrape.com/catalogue/category/books_1/index.html) contains many books, spread across __50 pages__. \n",
    "\n",
    "So, the goal of this section is to navigate through the __entire book assortment__, not only the first 20 books!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Open [the website](https://books.toscrape.com/catalogue/category/books_1/index.html), and click on the \"next\" button at the bottom of the page.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/books.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Repeat this a couple of times, and observe how the URL in your navigation bar is changing...\n",
    "\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-1.html`\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-2.html`\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-3.html`\n",
    "\n",
    "Can you guess the next one...?\n",
    "\n",
    "Indeed! The URL can be divided into a __fixed base part__ (`https://books.toscrape.com/catalogue/category/books_1/`), and a __counter__ that is dependent on the page you're visiting (e.g., `page-1.html`). \n",
    "\n",
    "__Now let's create a list of all 50 URLs!__ \n",
    "\n",
    "First, we create a counter variable, which we now set to 1 (but it can take on any value later on). Then, we append the site's URL to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/page-1.html',\n",
       " 'https://books.toscrape.com/catalogue/page-2.html',\n",
       " 'https://books.toscrape.com/catalogue/page-3.html',\n",
       " 'https://books.toscrape.com/catalogue/page-4.html',\n",
       " 'https://books.toscrape.com/catalogue/page-5.html',\n",
       " 'https://books.toscrape.com/catalogue/page-6.html',\n",
       " 'https://books.toscrape.com/catalogue/page-7.html',\n",
       " 'https://books.toscrape.com/catalogue/page-8.html',\n",
       " 'https://books.toscrape.com/catalogue/page-9.html',\n",
       " 'https://books.toscrape.com/catalogue/page-10.html',\n",
       " 'https://books.toscrape.com/catalogue/page-11.html',\n",
       " 'https://books.toscrape.com/catalogue/page-12.html',\n",
       " 'https://books.toscrape.com/catalogue/page-13.html',\n",
       " 'https://books.toscrape.com/catalogue/page-14.html',\n",
       " 'https://books.toscrape.com/catalogue/page-15.html',\n",
       " 'https://books.toscrape.com/catalogue/page-16.html',\n",
       " 'https://books.toscrape.com/catalogue/page-17.html',\n",
       " 'https://books.toscrape.com/catalogue/page-18.html',\n",
       " 'https://books.toscrape.com/catalogue/page-19.html',\n",
       " 'https://books.toscrape.com/catalogue/page-20.html',\n",
       " 'https://books.toscrape.com/catalogue/page-21.html',\n",
       " 'https://books.toscrape.com/catalogue/page-22.html',\n",
       " 'https://books.toscrape.com/catalogue/page-23.html',\n",
       " 'https://books.toscrape.com/catalogue/page-24.html',\n",
       " 'https://books.toscrape.com/catalogue/page-25.html',\n",
       " 'https://books.toscrape.com/catalogue/page-26.html',\n",
       " 'https://books.toscrape.com/catalogue/page-27.html',\n",
       " 'https://books.toscrape.com/catalogue/page-28.html',\n",
       " 'https://books.toscrape.com/catalogue/page-29.html',\n",
       " 'https://books.toscrape.com/catalogue/page-30.html',\n",
       " 'https://books.toscrape.com/catalogue/page-31.html',\n",
       " 'https://books.toscrape.com/catalogue/page-32.html',\n",
       " 'https://books.toscrape.com/catalogue/page-33.html',\n",
       " 'https://books.toscrape.com/catalogue/page-34.html',\n",
       " 'https://books.toscrape.com/catalogue/page-35.html',\n",
       " 'https://books.toscrape.com/catalogue/page-36.html',\n",
       " 'https://books.toscrape.com/catalogue/page-37.html',\n",
       " 'https://books.toscrape.com/catalogue/page-38.html',\n",
       " 'https://books.toscrape.com/catalogue/page-39.html',\n",
       " 'https://books.toscrape.com/catalogue/page-40.html',\n",
       " 'https://books.toscrape.com/catalogue/page-41.html',\n",
       " 'https://books.toscrape.com/catalogue/page-42.html',\n",
       " 'https://books.toscrape.com/catalogue/page-43.html',\n",
       " 'https://books.toscrape.com/catalogue/page-44.html',\n",
       " 'https://books.toscrape.com/catalogue/page-45.html',\n",
       " 'https://books.toscrape.com/catalogue/page-46.html',\n",
       " 'https://books.toscrape.com/catalogue/page-47.html',\n",
       " 'https://books.toscrape.com/catalogue/page-48.html',\n",
       " 'https://books.toscrape.com/catalogue/page-49.html',\n",
       " 'https://books.toscrape.com/catalogue/page-50.html']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 1\n",
    "page_urls = []\n",
    "while counter <= 50:\n",
    "    page_urls.append(f'https://books.toscrape.com/catalogue/page-{counter}.html')\n",
    "    counter+=1\n",
    "page_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this gives a list of all page URLs that contain books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of page urls in the list is: 50\n"
     ]
    }
   ],
   "source": [
    "# print the last five page urls (btw, run print(page_urls) for yourself to see all page URLs!)\n",
    "print(\"The number of page urls in the list is: \" + str(len(page_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1\n",
    "\n",
    "Let's take a step back again, and practice getting seeds from *another website*: [`quotes.toscrape.com`](https://quotes.toscrape.com/) displays 100 famous quotes from GoodReads, categorized by tag. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/quotes.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make yourself comfortable with how the [site](https://quotes.toscrape.com) works and ask yourself questions such as: how does the navigation work, how many pages are there, what is the base URL, and how does it change if I move to the next page?\n",
    "2. Generate a list `quote_page_urls` that contains the page URLs we need if we'd like to scrape all 100 quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. The 100 quotes are evenly spread across 10 pages. The base URL is `https://quotes.toscrape.com/page/` followed by a page number between 1 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://quotes.toscrape.com/page/1',\n",
       " 'https://quotes.toscrape.com/page/2',\n",
       " 'https://quotes.toscrape.com/page/3',\n",
       " 'https://quotes.toscrape.com/page/4',\n",
       " 'https://quotes.toscrape.com/page/5',\n",
       " 'https://quotes.toscrape.com/page/6',\n",
       " 'https://quotes.toscrape.com/page/7',\n",
       " 'https://quotes.toscrape.com/page/8',\n",
       " 'https://quotes.toscrape.com/page/9',\n",
       " 'https://quotes.toscrape.com/page/10']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 1\n",
    "quote_page_urls = []\n",
    "while counter <= 10:\n",
    "    quote_page_urls.append(f'https://quotes.toscrape.com/page/{counter}')\n",
    "    counter+=1\n",
    "quote_page_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, one of the big disadvantages of this \"manual\" link building is that we need to \"know\" how many pages to extract information from. This may vastly differ by category. \n",
    "\n",
    "We turn towards this issue next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using links contained in elements (e.g., buttons)\n",
    "\n",
    "__Importance__\n",
    "\n",
    "For now, the book link extraction has worked without problems. Yet, there's still one little improvement that we can make. *If the number of pages changes*, we need to manually update for how many pages we would like to retrieve seeds.\n",
    "\n",
    "A general solution is therefore to look up whether there is a `next` button on the page (see HTML code below). We can then either \"grab\" the URL and visit it (so, in essence, we're still using URLs to navigate), or - instead - \"click\" on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/next_page.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "So, let's write a snippet that \"captures\" the link of the next page button on the [books page](https://books.toscrape.com).\n",
    "\n",
    "We always proceed in small steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the website's source code and convert to BeautifulSoup object\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"next\"><a href=\"page-2.html\">next</a></li>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Trying to locate the \"next\" class.\n",
    "soup.find(class_='next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Trying to locate the <a> tag within the \"next\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"page-2.html\">next</a>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(class_='next').find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'page-2.html'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Trying to extract the link ('href' attribute)\n",
    "soup.find(class_='next').find('a')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, we can observe how we're getting closer to the information we need.\n",
    "\n",
    "Now, we only need to combine the base URL with the page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/category/books_1/page-2.html'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_page = soup.find(class_='next').find('a')['href']\n",
    "'https://books.toscrape.com/catalogue/category/books_1/' + next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.2__\n",
    "\n",
    "Please first load the snippet below, which has wrapped the \"next page\" capturing in a function. Observe the use of `try` and `except`, which accounts for the last page NOT having a next page button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://books.toscrape.com/catalogue/category/books_1/'\n",
    "\n",
    "def next_page(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    try:\n",
    "        next_page = soup.find(class_='next').find('a')['href']\n",
    "    except:\n",
    "        next_page = 'no next page'\n",
    "    return(base_url + next_page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Pass `https://books.toscrape.com/catalogue/page-49.html` to `next_page()` and observe the output. Then, use  `https://books.toscrape.com/catalogue/page-50.html`. Is that what you expected? \n",
    "\n",
    "2. Write a while loop that assembles a list of all product pages for the book category (`'https://books.toscrape.com/catalogue/category/books_1/'`), by extracting next page URLs from each page and appending them to an array/list called `urls`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/category/books_1/page-50.html'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "next_page('https://books.toscrape.com/catalogue/page-49.html') # works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/category/books_1/no next page'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_page('https://books.toscrape.com/catalogue/page-50.html') # returns \"no next page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-3.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-4.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-5.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-6.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-7.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-8.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-9.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-10.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-11.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-12.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-13.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-14.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-15.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-16.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-17.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-18.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-19.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-20.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-21.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-22.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-23.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-24.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-25.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-26.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-27.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-28.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-29.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-30.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-31.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-32.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-33.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-34.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-35.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-36.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-37.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-38.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-39.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-40.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-41.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-42.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-43.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-44.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-45.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-46.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-47.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-48.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-49.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-50.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-5.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-6.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-7.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-8.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-9.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-10.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-11.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-12.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-13.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-14.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-15.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-16.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-17.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-18.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-19.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-20.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-21.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-22.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-23.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-24.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-25.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-26.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-27.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-28.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-29.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-30.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-31.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-32.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-33.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-34.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-35.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-36.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-37.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-38.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-39.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-40.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-41.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-42.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-43.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-44.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-45.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-46.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-47.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-48.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-49.html',\n",
       " 'https://books.toscrape.com/catalogue/category/books_1/page-50.html']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "urls = []\n",
    "\n",
    "# define first URL to start from\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/'\n",
    "\n",
    "while True:\n",
    "    print('Trying to get next page URL from ' + url)\n",
    "    next_url = next_page(url)\n",
    "    if 'no next page' in next_url: break\n",
    "    url = next_url\n",
    "    urls.append(url)\n",
    "    \n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Collecting all seeds\n",
    "\n",
    "Up to this moment, we have defined what seeds are (crucially important for sampling!), and introduced several ways through which you can navigate on a site. The only thing that's missing is combining these two things: navigating through all of the available pages, and collecting seeds for which we can later extract data.\n",
    "\n",
    "__Exercise 2.3__\n",
    "\n",
    "Using the solution from exercise 2.2, write code that navigates through all pages of the book category and stores product URLs in a list of dictionaries, containing the following data points:\n",
    "- product URL\n",
    "- URL from which page the product URL was captured\n",
    "- current time stamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-3.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-4.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-5.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-6.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-7.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-8.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-9.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-10.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-11.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-12.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-13.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-21eb307a454d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'User-agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapparent_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mapparent_encoding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    726\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapparent_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[1;34m\"\"\"The apparent encoding, provided by the chardet library.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_unicode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\__init__.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(byte_str)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[0;32m    213\u001b[0m                                    \u001b[1;34m'confidence'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\charsetgroupprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\sbcharsetprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keep_english_letter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mbyte_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_international_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\chardet\\charsetprober.py\u001b[0m in \u001b[0;36mfilter_international_words\u001b[1;34m(buf)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# the end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         words = re.findall(b'[a-zA-Z]*[\\x80-\\xFF]+[a-zA-Z]*[^a-zA-Z\\x80-\\xFF]?',\n\u001b[1;32m---> 87\u001b[1;33m                            buf)\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "seeds = []\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/' #initialize for first page\n",
    "counter = 0 #initialize counter so that you can break earlier from this loop when needed\n",
    "\n",
    "while True:\n",
    "    counter+=1\n",
    "    \n",
    "    #if (counter>4): break # deactivate this comment if you want to break after x iterations for prototyping\n",
    "    \n",
    "    print(f'Trying to get next page URL from {url}')\n",
    "    \n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers=header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    \n",
    "    # extract information\n",
    "    urls = soup.find_all(class_=\"product_pod\")\n",
    "    for book in urls:\n",
    "        url_book = book.find(\"a\").attrs[\"href\"]\n",
    "        book_url = \"https://books.toscrape.com/catalogue/\" + url_book\n",
    "        book_url = book_url.replace('../', '')\n",
    "        seeds.append({'product_url': book_url,\n",
    "                      'page_url': url,\n",
    "                      'timestamp': int(time.time())})\n",
    "    \n",
    "    # next page available?\n",
    "    try:\n",
    "        url = 'https://books.toscrape.com/catalogue/category/books_1/' + soup.find(class_='next').find('a')['href']\n",
    "    except:\n",
    "        break # no next page present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the collected seeds\n",
    "seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve product information, you could now loop through this list of links and obtain the respective product information (see webdata for dummies tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction\n",
    "\n",
    "\n",
    "### 3.1 Timers\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Before we started running some of the cells above, you may have observed the usage of the `time.sleep` function. Sending many requests at the same time can overload a server. Therefore, it's highly recommended to pause between requests rather than sending them all simultaneously. This avoids that your IP address (i.e., numerical label assigned to each device connected to the internet) gets blocked, and you can no longer visit (and scrape) the website. \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "In Python, you can import the `time` module, which pauses the execution of future commands for a given amount of time. For example, the print statement after `time.sleep(3)` will only be executed after 3 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll be printed to the console after 3 seconds!\n"
     ]
    }
   ],
   "source": [
    "# run this cell again to see the timer in action yourself!\n",
    "import time\n",
    "pause = 3\n",
    "time.sleep(pause)\n",
    "print(f\"I'll be printed to the console after {pause} seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1__\n",
    "\n",
    "Modify the code above to sleep for 2 minutes. Go grab a coffee inbetween. Did it take you longer than 2 minutes?\n",
    "\n",
    "(if you want to abort the running code, just select the cell and push the \"stop\" button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "time.sleep(2*60)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modularization\n",
    "\n",
    "**Importance**  \n",
    "\n",
    "In scraping, many things have to be executed *multiple times*. For example, whenever we open a new page on books.toscrape.com, we would like to extract all the available book links.\n",
    "\n",
    "To help us execute things over and over again, we will \"modularize\" our code into functions. We can then call these functions whenever we need them. Another benefit from using functions is that we can improve the readability and reusability of our code. If you need a quick refresher on functions, please revisit section 4 of the [Python Bootcamp](https://odcm.hannesdatta.com/docs/tutorials/pythonbootcamp/).\n",
    "\n",
    "**Let's try it out**\n",
    "\n",
    "Let's finish up our book URL scraper by putting together everything we have learned thus far.\n",
    "\n",
    "1. We need a function that extracts all seeds, given a category URL. We would like to store these seeds in a JSON file and save it to the disk. This will consititute our \"sample\" going forward.\n",
    "2. We need a function that opens this JSON file, and captures all of the relevant product information (for now, let's use the title and price).\n",
    "\n",
    "__Exercise 3.2__\n",
    "\n",
    "Write a function to accomplish (1) above? (capturing the seeds and storing them in a JSON file)? Start with the solution in 2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_seeds(start_url = 'https://books.toscrape.com/catalogue/category/books_1/'):\n",
    "    seeds = []\n",
    "    url = start_url\n",
    "    counter = 0 #initialize counter so that you can break earlier from this loop when needed\n",
    "\n",
    "    while True:\n",
    "        counter+=1\n",
    "\n",
    "        if (counter>4): break # (de)activate this comment if you want to break after x iterations for prototyping\n",
    "\n",
    "        print(f'Trying to get next page URL from {url}')\n",
    "\n",
    "        header = {'User-agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers=header)\n",
    "        res.encoding = res.apparent_encoding\n",
    "        soup = BeautifulSoup(res.text)\n",
    "\n",
    "        # extract information\n",
    "        urls = soup.find_all(class_=\"product_pod\")\n",
    "        for book in urls:\n",
    "            url_book = book.find(\"a\").attrs[\"href\"]\n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + url_book\n",
    "            book_url = book_url.replace('../', '')\n",
    "            seeds.append({'product_url': book_url,\n",
    "                          'page_url': url,\n",
    "                          'timestamp': int(time.time())})\n",
    "        \n",
    "        # next page available?\n",
    "        try:\n",
    "            url = 'https://books.toscrape.com/catalogue/category/books_1/' + soup.find(class_='next').find('a')['href']\n",
    "        except:\n",
    "            break # no next page present\n",
    "            \n",
    "    return(seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-3.html\n",
      "Trying to get next page URL from https://books.toscrape.com/catalogue/category/books_1/page-4.html\n"
     ]
    }
   ],
   "source": [
    "data = get_seeds('https://books.toscrape.com/catalogue/category/books_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'product_url': 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-requiem-red_995/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-black-maria_991/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/set-me-free_988/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/olio_984/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/',\n",
       "  'timestamp': 1676365693},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/in-her-wake_980/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/how-music-works_979/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/foolproof-preserving-a-guide-to-small-batch-jams-jellies-pickles-condiments-and-more-a-foolproof-guide-to-making-small-batch-jams-jellies-pickles-condiments-and-more_978/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/chase-me-paris-nights-2_977/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/black-dust_976/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/birdsong-a-story-in-pictures_975/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/americas-cradle-of-quarterbacks-western-pennsylvanias-football-factory-from-johnny-unitas-to-joe-montana_974/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/aladdin-and-his-wonderful-lamp_973/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/worlds-elsewhere-journeys-around-shakespeares-globe_972/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/wall-and-piece_971/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-four-agreements-a-practical-guide-to-personal-freedom_970/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-five-love-languages-how-to-express-heartfelt-commitment-to-your-mate_969/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-elephant-tree_968/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-bear-and-the-piano_967/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/sophies-world_966/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/penny-maybe_965/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/maude-1883-1993she-grew-up-with-the-country_964/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/behind-closed-doors_962/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/you-cant-bury-them-all-poems_961/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-2.html',\n",
       "  'timestamp': 1676365694},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/slow-states-of-collapse-poems_960/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/reasons-to-stay-alive_959/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/private-paris-private-10_958/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/higherselfie-wake-up-your-life-free-your-soul-find-your-tribe_957/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/without-borders-wanderlove-1_956/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/when-we-collided_955/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/we-love-you-charlie-freeman_954/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/untitled-collection-sabbath-poems-2014_953/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unseen-city-the-majesty-of-pigeons-the-discreet-charm-of-snails-other-wonders-of-the-urban-wilderness_952/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unicorn-tracks_951/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/unbound-how-eight-technologies-made-us-human-transformed-society-and-brought-our-world-to-the-brink_950/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/tsubasa-world-chronicle-2-tsubasa-world-chronicle-2_949/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/throwing-rocks-at-the-google-bus-how-growth-became-the-enemy-of-prosperity_948/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/this-one-summer_947/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/thirst_946/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-torch-is-passed-a-harding-family-story_945/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-secret-of-dreadwillow-carse_944/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-pioneer-woman-cooks-dinnertime-comfort-classics-freezer-food-16-minute-meals-and-other-delicious-ways-to-solve-supper_943/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-natural-history-of-us-the-fine-art-of-pretending-2_941/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-3.html',\n",
       "  'timestamp': 1676365695},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-nameless-city-the-nameless-city-1_940/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-murder-that-never-was-forensic-instincts-5_939/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-most-perfect-thing-inside-and-outside-a-birds-egg_938/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-mindfulness-and-acceptance-workbook-for-anxiety-a-guide-to-breaking-free-from-anxiety-phobias-and-worry-using-acceptance-and-commitment-therapy_937/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-life-changing-magic-of-tidying-up-the-japanese-art-of-decluttering-and-organizing_936/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-inefficiency-assassin-time-management-tactics-for-working-smarter-not-longer_935/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-gutsy-girl-escapades-for-your-life-of-epic-adventure_934/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-electric-pencil-drawings-from-inside-state-hospital-no-3_933/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-death-of-humanity-and-the-case-for-life_932/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-bulletproof-diet-lose-up-to-a-pound-a-day-reclaim-energy-and-focus-upgrade-your-life_931/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-art-forger_930/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-age-of-genius-the-seventeenth-century-and-the-birth-of-the-modern-mind_929/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/the-activists-tao-te-ching-ancient-advice-for-a-modern-revolution_928/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/spark-joy-an-illustrated-master-class-on-the-art-of-organizing-and-tidying-up_927/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/soul-reader_926/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/security_925/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/saga-volume-6-saga-collected-editions-6_924/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/saga-volume-5-saga-collected-editions-5_923/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/reskilling-america-learning-to-labor-in-the-twenty-first-century_922/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696},\n",
       " {'product_url': 'https://books.toscrape.com/catalogue/rat-queens-vol-3-demons-rat-queens-collected-editions-11-15_921/index.html',\n",
       "  'page_url': 'https://books.toscrape.com/catalogue/category/books_1/page-4.html',\n",
       "  'timestamp': 1676365696}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data in new-line separated JSON files\n",
    "\n",
    "import json\n",
    "f = open('seeds.json','w',encoding = 'utf-8')\n",
    "for item in data:\n",
    "        f.write(json.dumps(item))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.3__\n",
    "\n",
    "Now, let's write some code that loads `seeds.json`, and visits each of the websites to extract the product title and price. Remember to build in a little timer (e.g., waiting for 1 second). The prototype/starting code below stops automatically after 5 iterations to minimize server load. Try removing the prototyping condition using the comment character `#` when you think you're done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n",
      "https://books.toscrape.com/catalogue/soumission_998/index.html\n",
      "https://books.toscrape.com/catalogue/sharp-objects_997/index.html\n",
      "https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n"
     ]
    }
   ],
   "source": [
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    print(obj['product_url'])\n",
    "    \n",
    "    # eventually sleep for a second\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tips</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            Use the function <code>parse_website</code> from exercise 1.6 in the \"webdata for dummies\" tutorial and remove the file saving part.\n",
    "        </li>\n",
    " \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the parse_website() function here from an earlier tutorial. Remember also using the import statements!\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_website(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'} # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "    request = requests.get(url, headers = header)\n",
    "    request.encoding = request.apparent_encoding # set encoding to UTF-8\n",
    "    source_code = request.text\n",
    "\n",
    "    # make information \"extractable\" using BeautifulSoup\n",
    "    soup = BeautifulSoup(source_code)\n",
    "    \n",
    "    # title\n",
    "    title = soup.find('h1').get_text()\n",
    "    price = soup.find(class_='price_color').get_text()\n",
    "    instock = soup.find(class_='instock availability').get_text().strip()\n",
    "    stars = soup.find(class_='star-rating').attrs['class'][1]\n",
    "\n",
    "    data = {'title': title,\n",
    "            'price': price,\n",
    "            'instock': instock,\n",
    "            'stars': stars}\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Set Me Free',\n",
       " 'price': '£17.46',\n",
       " 'instock': 'In stock (19 available)',\n",
       " 'stars': 'Five'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether the function works (I just randomly picked a book)\n",
    "parse_website('https://books.toscrape.com/catalogue/set-me-free_988/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data for https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html.\n",
      "Retrieving data for https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html.\n",
      "Retrieving data for https://books.toscrape.com/catalogue/soumission_998/index.html.\n",
      "Retrieving data for https://books.toscrape.com/catalogue/sharp-objects_997/index.html.\n",
      "Retrieving data for https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html.\n"
     ]
    }
   ],
   "source": [
    "# now start from the code above and \"use\" the function\n",
    "\n",
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    url = obj['product_url']\n",
    "    print(f'Retrieving data for {url}.')\n",
    "    \n",
    "    retrieved_data = parse_website(url)\n",
    "    retrieved_data['timestamp_retrieval'] = int(time.time())\n",
    "    # store data\n",
    "    f = open('book_data.json', 'a', encoding = 'utf-8')\n",
    "    f.write(json.dumps(retrieved_data))\n",
    "    f.write('\\n')\n",
    "    f.close() \n",
    "    \n",
    "    # eventually sleep for a second\n",
    "    time.sleep(1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>timestamp_retrieval</th>\n",
       "      <th>instock</th>\n",
       "      <th>stars</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>Â£51.77</td>\n",
       "      <td>2023-02-14 09:11:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>Â£53.74</td>\n",
       "      <td>2023-02-14 09:11:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>Â£50.10</td>\n",
       "      <td>2023-02-14 09:11:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (22 available)</td>\n",
       "      <td>Three</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Four</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Five</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (22 available)</td>\n",
       "      <td>Three</td>\n",
       "      <td>2023-02-14 09:14:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>2023-02-14 09:14:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>2023-02-14 09:14:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Four</td>\n",
       "      <td>2023-02-14 09:14:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Five</td>\n",
       "      <td>2023-02-14 09:14:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>2023-02-14 09:14:40</td>\n",
       "      <td>In stock (22 available)</td>\n",
       "      <td>Three</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>2023-02-14 09:14:41</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>2023-02-14 09:14:43</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>One</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>2023-02-14 09:14:45</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Four</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>2023-02-14 09:14:46</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>Five</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title    price timestamp_retrieval  \\\n",
       "0                    A Light in the Attic  Â£51.77 2023-02-14 09:11:42   \n",
       "1                      Tipping the Velvet  Â£53.74 2023-02-14 09:11:43   \n",
       "2                              Soumission  Â£50.10 2023-02-14 09:11:45   \n",
       "3                    A Light in the Attic   £51.77                 NaT   \n",
       "4                      Tipping the Velvet   £53.74                 NaT   \n",
       "5                              Soumission   £50.10                 NaT   \n",
       "6                           Sharp Objects   £47.82                 NaT   \n",
       "7   Sapiens: A Brief History of Humankind   £54.23                 NaT   \n",
       "8                    A Light in the Attic   £51.77                 NaT   \n",
       "9                      Tipping the Velvet   £53.74                 NaT   \n",
       "10                             Soumission   £50.10                 NaT   \n",
       "11                          Sharp Objects   £47.82                 NaT   \n",
       "12  Sapiens: A Brief History of Humankind   £54.23                 NaT   \n",
       "13                   A Light in the Attic   £51.77 2023-02-14 09:14:40   \n",
       "14                     Tipping the Velvet   £53.74 2023-02-14 09:14:41   \n",
       "15                             Soumission   £50.10 2023-02-14 09:14:43   \n",
       "16                          Sharp Objects   £47.82 2023-02-14 09:14:45   \n",
       "17  Sapiens: A Brief History of Humankind   £54.23 2023-02-14 09:14:46   \n",
       "\n",
       "                    instock  stars           timestamp  \n",
       "0                       NaN    NaN                 NaT  \n",
       "1                       NaN    NaN                 NaT  \n",
       "2                       NaN    NaN                 NaT  \n",
       "3   In stock (22 available)  Three                 NaT  \n",
       "4   In stock (20 available)    One                 NaT  \n",
       "5   In stock (20 available)    One                 NaT  \n",
       "6   In stock (20 available)   Four                 NaT  \n",
       "7   In stock (20 available)   Five                 NaT  \n",
       "8   In stock (22 available)  Three 2023-02-14 09:14:30  \n",
       "9   In stock (20 available)    One 2023-02-14 09:14:31  \n",
       "10  In stock (20 available)    One 2023-02-14 09:14:33  \n",
       "11  In stock (20 available)   Four 2023-02-14 09:14:34  \n",
       "12  In stock (20 available)   Five 2023-02-14 09:14:36  \n",
       "13  In stock (22 available)  Three                 NaT  \n",
       "14  In stock (20 available)    One                 NaT  \n",
       "15  In stock (20 available)    One                 NaT  \n",
       "16  In stock (20 available)   Four                 NaT  \n",
       "17  In stock (20 available)   Five                 NaT  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data in pandas\n",
    "import pandas as pd\n",
    "pd.read_json('book_data.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Summary\n",
    "\n",
    "At the beginning of this tutorial, we set out the promise of writing multi-page scrapers from start to finish. Although the examples we have studied are relatively simple, the same principles (seed definition, data extraction plan, page-level data collection) apply to any other website you'd like to scrape. \n",
    "\n",
    "But... then, there are more *advanced websites*, which we address next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Making different types of website requests\n",
    "\n",
    "In previous tutorials, you have used the `requests` library to retrieve web data. For example, re-run the following code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>Sharp Objects</h1>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "request = requests.get('https://books.toscrape.com/catalogue/sharp-objects_997/index.html', headers = header)\n",
    "request.encoding = request.apparent_encoding\n",
    "source_code = request.text\n",
    "\n",
    "# save website \n",
    "f=open('simple_website.html','w',encoding='utf-8')\n",
    "f.write(source_code)\n",
    "f.close()\n",
    "\n",
    "# parse some information\n",
    "soup=BeautifulSoup(source_code)\n",
    "soup.find('h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for relatively simple websites, but... try the same for the homepage of Twitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = requests.get('https://www.twitch.tv/', headers = header)\n",
    "request.encoding = request.apparent_encoding\n",
    "source_code = request.text\n",
    "soup=BeautifulSoup(source_code)\n",
    "\n",
    "# save website \n",
    "f=open('advanced_website.html','w',encoding='utf-8')\n",
    "f.write(source_code)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to open `advanced_website.html` in your browser, you quickly realize there is a problem. You can't see what's on the website when you manually open it using the URL. This mainly has to do with how advanced a website is: in the case of Twitch, you'd encounter quite a dynamic site with a video player, previews, real-time updates on the number of streams, etc. The normal request library isn't just able to handle it. \n",
    "\n",
    "So, we're resorting to an alternative way to retrieve data, using `selenium`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Making a connection to a website using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Installing Selenium and Chromedriver</b> \n",
    "\n",
    "To install Selenium and Chromedriver locally, please follow the <a href=\"https://tilburgsciencehub.com/configure/python-for-scraping/?utm_campaign=referral-short\">Tutorial on Tilburg Science Hub</a>.\n",
    "    \n",
    "You can also use the code snippet below to automate the installation. Running this snippet takes a little longer each time, but the benefit is that it almost always works!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver_manager in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.23.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver_manager) (4.44.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from requests->webdriver_manager) (2022.9.24)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.14 which is incompatible.\n",
      "ERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.26.14 which is incompatible.\n",
      "ERROR: botocore 1.12.228 has requirement docutils<0.16,>=0.10, but you'll have docutils 0.16 which is incompatible.\n",
      "ERROR: botocore 1.12.228 has requirement urllib3<1.26,>=1.20; python_version >= \"3.4\", but you'll have urllib3 1.26.14 which is incompatible.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\urllib3\\\\contrib\\\\_securetransport\\\\__pycache__'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from selenium) (0.22.0)\n",
      "Collecting urllib3[socks]~=1.26\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.10 in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from trio-websocket~=0.9->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.9)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9; python_version < \"3.11\" in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from trio~=0.17->selenium) (1.0.0rc9)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (19.3.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hdatta\\appdata\\roaming\\python\\python37\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.4.0)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|████████████████████████████████████████████████████████| 6.79M/6.79M [00:00<00:00, 81.8MB/s]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Installing and starting up Chrome using Webdriver Manager\n",
    "!pip install webdriver_manager\n",
    "!pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Opening the Twitch site\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "url = \"https://twitch.tv/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went smooth, your computer opened a new Chrome window, and opened `twitch.tv`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Using Google Colab</b> \n",
    "\n",
    "If you're using Google Colab, you don't see your browser open up manually.\n",
    "    \n",
    "Whenever you switch pages, just manually open that page in your browser. Although this feels like a little less interactive, you will still be able to work through this tutorial!\n",
    "\n",
    "</div>\n",
    "\n",
    "From now onwards, you can use `driver.get('https://google.com')` to point to different websites (i.e., you don't need to install it over and over again, unless you open up a new instance of Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using BeautifulSoup with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now also try to extract information. Note that we're converting the source code of the site to a `BeautifulSoup` object (because you may have learnt how to use `BeautifulSoup` earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need the time package to wait a few seconds until the page is loaded\n",
    "import time\n",
    "url = \"https://twitch.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using the \"source code\" obtained with the `requests` library, we can now convert the source code of the Selenium website to a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and start experimenting with querying the site, such as retrieving the titles of the currently active streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1: 🟦MAFIATHON DAY 13🟦BLACK HISTORY MONTH🟦CLICK HERE🟦THE BIGGEST DWARF🟦!subathonKaiCenat\n",
      "Stream 2: [!DROPS ON] Hogwarting with Tyr, maybe some OW later!Fextralife\n",
      "Stream 3: 🔴YSKM VS ROOKIE WTFFFFFFF VALENTINES DAY HAHAHAHAH IM GONNA DIE ALONE🔴 !discord !youtube !socials !vodchannel !pobox !reddit🔴Caedrel\n",
      "Stream 4: chilin. - !gfuel !merch - @summit1gsummit1g\n",
      "Stream 5: drunk before V-DAY | DROPS ENABLED | HOGWARTS LEGACY | COSPLAAY |  PS5 | Lets Get Sorted!!AdrianaChechik_\n",
      "Stream 6: DROPS ✨ HOGWARTS LEGACY ✨ !socials -> social links ✨eugeniacooney\n",
      "Stream 7: Happy Valentine's Day with Lucy! | TTS: $2/100 BITS | !discord !menu !socialsWolfsbanee\n",
      "Stream 8: [18+] SENSUAL SINGLES FT KennGotGame 💕 | !donate !discord !pissmacadaamian\n",
      "Stream 9: [DROPS ENABLED] 100% Hufflepuff hard run! - !fnaticLoeya\n",
      "Stream 10: Hogwarts op Hard in Harnas op HartjesdagCohony\n",
      "Stream 11: 🎁Subathon Dag 41🎁💗Heb jij een VALENTIJN?💗 !subathon !doneren !planningPotgrond_\n",
      "Stream 12: 🟢24/7🟢[FREEGAMES]Interactive Trivia, Giveaways, Mini Gameshitsquadgodfather\n",
      "Stream 13: 🔴IG VS TES LPL CoStream 🔴 LEC Week 5 Day 2 #LPLCoStreamIWDominate\n",
      "Stream 14: Speedrunning Challenger on smurf account (Day 2 of 5) -- Champion requests are more than welcome -- !facecheck #adBroxah\n",
      "Stream 15: Skelly Bois !HelloFresh !merch !sneak !primeBeardageddon\n",
      "Stream 16: sinatraa | late night !call | !discord !newvid !gorilla !blitz !ads !keyboardsinatraa\n",
      "Stream 17: METROID PRIME 100% ITEM COLLECTION | !ggsupps @calebhart42 IG/TwitterCalebhart42\n",
      "Stream 18: Youtube Binge @16:00 CET !Subathon Day 33 Midnight CLub 3 any% HD !Textures !PCSX2 | !ig !mmKuruHS\n",
      "Stream 19: 🔴 V-DAY SUBATHON 🔴 KFC DATING SIM 🔴 HUGE DAY !socials !giveawayTakaGGuwu\n",
      "Stream 20: Rerun: 14:30 RMR OPEN QUALIFY!!! - Siga @gaules nas redes sociaisGaules\n",
      "Stream 21: NEW UPDATE (Most Wanted) PRO SOLOS | Use Code \"Oatley\" #ad | !socials !youtube !giveawayOatley\n",
      "Stream 22: Replay Relay!SmiteGame\n",
      "Stream 23: !DROPS | !fiona Wedding Skin Tomorrow | Still Dropping DC Keys Randomly | !tof !server !relic !movement !gear !mira !gg !NordVPN !HelloFreshgateoo\n",
      "Stream 24: 2-YEAR ANNIVERSARY SUBATHON | DAY 3 | !discord !spotify !suppsAriannaFoxton\n",
      "Stream 25: 24/7 the best of A State of Trance #Trance #Progressive #Dance #HouseASOT\n",
      "Stream 26: Non Stop Music - Monstercat TV 🎶Monstercat\n",
      "Stream 27: We Won, bounty timeBen_\n",
      "Stream 28: !vote HBOX SUMMIT SUBATHON DAY 1!! MODCAST!! | !subathonHungrybox\n",
      "Stream 29: FIGHT TIME !vote !weight !newvideo !merchnesua\n",
      "Stream 30: Playing Tekken and Then a Surprise Game (De..)TheMainManSWE\n",
      "Stream 31: hafu -- bgs! 9.2k startitsHafu\n",
      "Stream 32: I will be your Valentine Chat 🌹superjj102\n",
      "Stream 33: ↪ Rerun - Super Mario Land 2: 6 Golden Coins [Glitchless] by elecman, ninja_SRC, Oh_DeeR - #ESASummer22ESAMarathon\n",
      "Stream 34: 120 Star speedrun (WR incoming)Liam\n",
      "Stream 35: click here to be my valentine ♡♡♡ [NZ] !birthday !wishlist !poboxMeowSparky\n",
      "Stream 36: 2023 S1 W10- Daytona WeekRenceTheFence\n",
      "Stream 37: (#1 World) 100 PP! HERE WE GOBugs\n",
      "Stream 38: ⚠️CLICK NOW⚠️ICON PLAYER PICKS⚠️HecticTKS\n"
     ]
    }
   ],
   "source": [
    "streams = soup.find_all('a', attrs = {'data-test-selector':\"TitleAndChannel\"})\n",
    "\n",
    "# print a list of stream names\n",
    "counter = 0\n",
    "for stream in streams:\n",
    "    counter = counter + 1\n",
    "    print('Stream ' + str(counter) + ': ' + stream.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - this is cool. You've just learnt a second way to open websites using `selenium`. The benefit of `selenium` is that you can work with highly dynamic websites (which also helps you to not getting blocked). The drawback is that `selenium` is slower than just using the `requests` library, and it may sometimes be buggy on computers without a screen (which matters when you scale up your data collection.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Awesome stuff with Selenium</b> \n",
    "\n",
    "Selenium is your best shot at navigating a dynamic website. It can do amazing things, such as \n",
    "    \n",
    "<ul>\n",
    "    <li>\"clicking\" on buttons</li>\n",
    "    <li>scrolling through a site</li>\n",
    "    <li>hovering over items and capturing information from popups,</li>\n",
    "    <li>starting to play a stream,</li>\n",
    "    <li>typing text and submitting it in the chat, and</li>\n",
    "    <li>so much more...!</li>\n",
    "</ul>\n",
    "    \n",
    "Note though that we won't cover the advanced functionality of Selenium in this tutorial, but the optional \"Web data advanced\" tutorial holds the necessary information.\n",
    "   \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "__Exercise 4.1__\n",
    "\n",
    "Please write code snippets to extract the following pieces of information. Do you choose `requests` or `selenium`?\n",
    "\n",
    "1. The titles of all `<h2>` tags from `https://odcm.hannesdatta.com/docs/course/`\n",
    "2. The titles of all available TV series from `https://www.bol.com/nl/nl/l/series/3133/30291/` (about 24)\n",
    "\n",
    "```\n",
    "soup.find_all('a', class_='product-title')\n",
    "```\n",
    "\n",
    "\n",
    "We also need the time package to wait a few seconds until the page is loaded.\n",
    "\n",
    "```\n",
    "import time\n",
    "url = \"https://twitch.tv/\" # some example URL\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructor\n",
      "Course description\n",
      "Prerequisites\n",
      "Teaching format\n",
      "Assessment\n",
      "Code of Conduct\n",
      "Structure of the course\n",
      "More links\n"
     ]
    }
   ],
   "source": [
    "# Solution to question 1:\n",
    "header = {'User-agent': 'Mozilla/5.0'} # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "request = requests.get('https://odcm.hannesdatta.com/docs/course/', headers = header)\n",
    "request.encoding = request.apparent_encoding # set encoding to UTF-8\n",
    "soup = BeautifulSoup(request.text)\n",
    "for title in soup.find_all('h2'): print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"product-title px_list_page_product_click list_page_product_tracking_target\" data-bltgh=\"rnrtRTDOZc-0bkD-mIhNQw.3_15.16.ProductTitle\" data-list-page-product-click-location=\"title\" data-no-translate=\"true\" data-test=\"product-title\" href=\"/nl/nl/p/midsomer-murders-seizoen-19-deel-2/9200000119833762/\">Midsomer Murders - Seizoen 19 Deel 2</a>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution to question 2:\n",
    "driver.get('https://www.bol.com/nl/nl/l/series/3133/30291/')\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/nl/nl/p/midsomer-murders-seizoen-19-deel-2/9200000119833762/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-18-deel-1/9200000132010326/',\n",
       " '/nl/nl/p/ncis-seizoen-19/9300000135569426/',\n",
       " '/nl/nl/p/fawlty-towers/9300000087454356/',\n",
       " '/nl/nl/p/sisi-seizoen-2/9300000139818897/',\n",
       " '/nl/nl/p/game-of-thrones-seizoen-1-8/9300000045366009/',\n",
       " '/nl/nl/p/chicago-fire-seizoen-10/9300000123634169/',\n",
       " '/nl/nl/p/star-trek-discovery-seizoen-4/9300000127973053/',\n",
       " '/nl/nl/p/house-of-the-dragon-seizoen-1/9300000127606162/',\n",
       " '/nl/nl/p/ncis-los-angeles-s12/9300000058801046/',\n",
       " '/nl/nl/p/flikken-maastricht-seizoen-16/9300000096688928/',\n",
       " '/nl/nl/p/game-of-thrones-seizoen-1-8/9300000045366024/',\n",
       " '/nl/nl/p/nachtwacht-het-donkere-spiegelbeeld/9300000128499338/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-17/9200000132010294/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-18-deel-2/9200000132010306/',\n",
       " '/nl/nl/p/columbo-complete-collection/9200000096426621/',\n",
       " '/nl/nl/p/star-trek-picard-seizoen-2/9300000123707493/',\n",
       " '/nl/nl/p/outlander-seizoen-6-blu-ray-import-met-nl-ondertiteling/9300000137849257/',\n",
       " '/nl/nl/p/dragon-ball-super-complete-series/9300000015744662/',\n",
       " '/nl/nl/p/ncis-los-angeles-seizoen-13-dvd-import-zonder-nl-ondertiteling/9300000140350227/',\n",
       " '/nl/nl/p/murder-she-wrote-complete-collection/9200000095914989/',\n",
       " '/nl/nl/p/north-south-complete-collection/1002004005993287/',\n",
       " '/nl/nl/p/the-sommerdahl-murders-seizoen-3/9300000139818898/',\n",
       " '/nl/nl/p/ncis-seizoen-18/9300000091148747/',\n",
       " '/nl/nl/p/knutby/9300000139818896/',\n",
       " '/nl/nl/p/friends-complete-collection/1002004007073112/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-19-deel-1/9200000119833764/',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-14-deel-1/9200000132010292/',\n",
       " '/nl/nl/p/call-the-midwife-series-10/9300000043667535/',\n",
       " '/nl/nl/p/star-trek-picard-seizoen-2/9300000123707486/']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for url in soup.find_all('a', class_='product-title'):\n",
    "    urls.append(url.attrs['href'])\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Using interactive elements (e.g., by clicking buttons)\n",
    "\n",
    "__Importance__\n",
    "\n",
    "For more dynamic websites, we may have to click on certain elements (rather than extracting some URL).\n",
    "\n",
    "__Try it out__\n",
    "\n",
    "If you haven't done so, rerun the installation code for `selenium` from above. Then, proceed by running the following cell and observe what happens in your browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://books.toscrape.com/catalogue/category/books_1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, your browser will have loaded the website in Chrome. Now, run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"f8d53a0fc3ac874f2f517e6cb934bdd8\", element=\"b8fad5d2-856f-4fd0-ab98-e9aac7976552\")>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Let's try location the element\n",
    "from selenium.webdriver.common.by import By\n",
    "driver.find_element(By.CLASS_NAME, 'next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"f8d53a0fc3ac874f2f517e6cb934bdd8\", element=\"613b3931-0404-478a-b687-6bf3f6fe6527\")>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Finding the link within the `next` class\n",
    "driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clicking the link!\n",
    "driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! In step 3, we finally clicked on the link. Just try rerunning this cell with step 3 over and over again. Does iterating through the pages work?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.2__\n",
    "\n",
    "Iterate through the entire set of pages, until there are no new pages left. This time, use `selenium` and click on the next page button. You can start on page 47 (`https://books.toscrape.com/catalogue/category/books_1/page-47.html`) to speed up this exercise a bit.\n",
    "\n",
    "Make use of the `time.sleep(2)` function to make the code wait a bit after each page load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "urls = []\n",
    "driver.get('https://books.toscrape.com/catalogue/category/books_1/page-47.html')\n",
    "time.sleep(1)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, 'next').find_element(By.TAG_NAME, 'a').click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After-class exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Extending the code written for exercise 3.2 in \"Web data 101\", please collect seeds from ten self-chosen product categories and store them in a file called `all_seeds.json`.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Please use the code written in exercise 3.3 in \"Web Data 101\" and extend it so capture more information (e.g., not only title and price, but also as other attributes/data points you are interested in. In particular, try getting the product description!\n",
    "\n",
    "Try running your code and store the product data in a JSON dictionary called `all_books.json`.\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Please complete an entire data collection project in a `.py` file, capturing data for 10 product categories and all products contained on all of the pages. You can proceed in two steps: first collect the seeds, then obtain all data. In addition, parse all retrieved data to a CSV file (with rows and columns), using `pd.read_json(filename, lines = True)` for reading in the JSON data, and `pd.to_csv(filename)` for saving the data in tabular format.\n",
    "\n",
    "Run your data collection from the terminal.\n",
    "\n",
    "The final deliverable is\n",
    "- `all_seeds.json`\n",
    "- `all_books.json`\n",
    "- `all_books.csv`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup: Executing Python Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebooks versus editors such as Visual Studio Code, PyCharm, or Spyder\n",
    "\n",
    "Jupyter Notebooks are ideal for combining programming and markdown (e.g., text, plots, equations), making it the default choice for sharing and presenting reproducible data analyses. Since we can execute code blocks one by one, it's suitable for developing and debugging code on the fly. \n",
    "\n",
    "That said, Jupyter Notebooks also have some severe limitations when using them in production environments. That's where an \"Integrated Development Environment\" (IDE) comes in, such as Visual Studio Code or PyCharm. Let's revisit the most important differences.\n",
    "\n",
    "First, the order in which you run cells within a notebook may affect the results. While prototyping, you may lose sight of the top-down hierarchy, which can cause problems once you restart the kernel (e.g., a library is imported after it is being used). Second, there is no easy way to browse through directories and files within a Jupyter Notebook. Third, notebooks cannot handle large codebases nor big data remarkably well. \n",
    "\n",
    "That's why we recommend starting in Jupyter Notebooks, moving code into functions along the way, and once all seems to be running well, copy-paste all necessary code into Visual Studio Code or PyCharm. From there, you can save it as a Python file (`.py`) - rather than a notebook (`.ipynb`) - and execute the file from the command line. Below, we introduce you to the IDE (here, Spyder, but VS Code looks very similar), and show you how to run Python files from the command line. \n",
    "\n",
    "### Introduction to Spyder\n",
    "The first time you need to click on the green \"Install\" button in Anaconda Navigator, after which you start Spyder by clicking on the blue \"Launch\" button (alternatively, type `spyder` in the terminal). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/anaconda_navigator.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interface consists of three panels: \n",
    "1. **Code editor** = where you write Python code (i.e., the content of code cells in a notebook)\n",
    "2. **Variable / files** = depending on which tab you choose either an overview of all declared variables (e.g. look up their type or change their values) or a file explorer (e.g., to open other Python files)\n",
    "3. **Console** = the output of running the Python script from the code editor (what normally appears below each cell in a notebook)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/spyder.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**     \n",
    "Copy the solution from exercise 3.3 to a new file, called `webscraping_101.py`. To run the script you can\n",
    "\n",
    "- click on the green play button to run all code, or\n",
    "- highlight the parts of the script you want to execute and then click the run selection button.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/toolbar.png\" width=40% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the script is running, you may need to interrupt the execution because it is simply taking too long or you spotted a bug somewhere. Click on the red rectangular in the console to stop the execution. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/interrupt.gif\" width=80% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Python Files \n",
    "\n",
    "__For Mac and Linux users__\n",
    "\n",
    "1. Open the terminal and navigate to the folder in which the `.py` file has been saved (use `cd` to change directories and `ls` to list all files).\n",
    "2. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/running_python.gif\" width=60% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For Windows users__\n",
    "\n",
    "1. Open Windows explorer and navigate to the folder in which the `.py` file has been saved. Type `cmd` to open the command prompt. Alternatively, open the command prompt from the start menu (and use `cd` to change directories and `dir` to list files).\n",
    "2. Activate Anaconda by typing `conda activate`.\n",
    "3. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
