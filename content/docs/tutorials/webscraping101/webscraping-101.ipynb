{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101 (oDCM)\n",
    "\n",
    "*After finishing this tutorial, you can extract data from multiple pages on the web, and export such data to CSV files so that you can use it in an analysis.*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Generate lists of entities to scrape data from\n",
    "* Map navigation path on a website using URLs, and understand how to use parameters to modify results\n",
    "* Select data for extraction on a website using CSS selectors\n",
    "* Write data to CSV file, and enrich with relevant metadata\n",
    "* Bundle data capture in Python functions and modularize extraction code\n",
    "* Loop through a list of URLs to capture data in bulk, using functions\n",
    "* Understand the difference between Jupyter Notebooks and “raw” Python files, and run collection via the command line/terminal\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This tutorial has been inspired by various open-access online resources, which we list for further reference at the [course website](https://odcm.hannesdatta.com/docs/about). \n",
    "\n",
    "--- \n",
    "\n",
    "## Support Needed?\n",
    "For technical issues outside of scheduled classes, please check the [support section](https://odcm.hannesdatta.com/docs/course/support) on the course website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Seed Generation\n",
    "\n",
    "\n",
    "### 1.1 Collecting Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "In web scraping, we typically refer to a \"seed\" as a starting point for a data collection. Without a seed, there's no data to collect.\n",
    "\n",
    "For example, before we can crawl through all books available on [this site](https://books.toscrape.com/catalogue/category/books_1/index.html), we first need to generate a *list of all books on the page*.\n",
    "\n",
    "One way to get there would be to:\n",
    "\n",
    "1. first scrape all book links (“seeds”) from the overview page, and \n",
    "2. then iterate over all links to scrape the product description (or anything else on that page). \n",
    "\n",
    "Note that the overview page allows us to \"navigate\" to the individual book pages, either by clicking on the book cover or the book title (see red boxes in the figure below). \n",
    "\n",
    "<img src=\"images/books_links.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Let's now check out how the links from the book covers or book titles are encoded in the website's source code.\n",
    "\n",
    "Open the [book catalogue](https://books.toscrape.com/catalogue/category/books_1/index.html), and inspect the underlying HTML code with the Chrome Inspector (right click --> inspect element). \n",
    "\n",
    "The book covers (`<img>`) are surrounded by `<a>` tags, which contain a link (`href`) to the book. \n",
    "\n",
    "Also, the book titles (`<h3>`) are surrounded by `<a>` tags with the relevant links to the book pages.\n",
    "\n",
    "<img src=\"images/inspector_links.png\" align=\"left\" width=80%/>\n",
    "\n",
    "How could we tell a computer to capture the links to the various books on the site?\n",
    "\n",
    "One simple way is to select *elements by their tags*. For example, to extract all links (`<a>` tags). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__\n",
    "\n",
    "Please run the code cell below, which extracts all links (the `a` tag!), and prints the URL (`href`) to the screen. \n",
    "\n",
    "If you look at these links more closely, you'll notive that we're not interested in many of these links... \n",
    "\n",
    "Make a list of all links we're *not* interested in (i.e., those *not* pointing to a book page). Which ones are those? Can you find out why they are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "for link in soup.find_all(\"a\"): print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__\n",
    "\n",
    "The links we want to ignore are...\n",
    "\n",
    "* \"Books to Scrape\" link at the top\n",
    "* \"Home\" breadcrumb link \n",
    "* Left sidebar with all book genres (e.g., Travel)\n",
    "* The next button at the bottom\n",
    "\n",
    "These links are present on the page, because they are used by users to navigate on the page. This can also be seen on the animation:\n",
    "\n",
    "<img src=\"images/books_overview.gif\" align=\"left\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Collecting *More Specific* Links\n",
    "\n",
    "__Importance__\n",
    "\n",
    "We've just discovered that selecting elements by their tags gives us many irrelevant links. But, how can we narrow down these links, or, in other words, __how can we scrape only the book links we're interested in?__.\n",
    "\n",
    "To answer this question, we need to briefly revisit the notion of __HTML classes__. \n",
    "\n",
    "A __class__ is often used as a reference in the code. For example, to make all text elements with a given class blue or increase the font size. In the Google Inspector screenshot shown earlier, you find an `<article>` tag with class `product_pod` in which a `<div>` is nested which contains the image and link attribute we're after. \n",
    "\n",
    "Every link to a book is *nested within this class* (nested = \"part of\"). The \"wrong links\" extracted above (i.e., the ones in the page's header and sidebar) are *not*. \n",
    "\n",
    "Thus, if we can tell our scraper that we're only interested in the `<a>` tags *within the `product_pod` class*, we end up with our desired selection of links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Like before, we'll use `.find_all()` to capture all matching elements on the page. The difference, however, is that we specify __a class (`class_=`)__, rather than an HTML tag. From the inspector, we know the class name (`product_pod`). \n",
    "\n",
    "This result is a list with __all 20 `product_pod` classes__ on the page (i.e., one for each book). \n",
    "\n",
    "Run the code below, in which we pick the __first book__ from the list (A Light in the Attic, element `[0]`), and extract the `<a>` tag nested within the `product_pod` class. \n",
    "\n",
    "Finally, we pull out the `href` attribute from the `<a>` tag which gives us the book link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "soup.find_all(class_=\"product_pod\")[0].find(\"a\").attrs[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `../../` in front of the link which tells the browser: this tells the browser to go back two directories from the current URL:\n",
    "* Current URL: https://books.toscrape.com/catalogue/category/books_1/index.html\n",
    "* 1 step back: https://books.toscrape.com/catalogue/category/books_1\n",
    "* 2 steps back: https://books.toscrape.com/catalogue/category/\n",
    "\n",
    "Thereafter, it appends `a-light-in-the-attic_1000/index.html` to the URL which forms the full link to the [A Light in the Attic](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html) book. \n",
    "\n",
    "Pretty cool, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "1. Modify the script to extract the link from the *second book* (Tipping the Velvet), using BeautifulSoup.\n",
    "2. Create a new variable `book_url` that concatenates the base URL (` https://books.toscrape.com/catalogue/`) and the string you extracted in the previous exercise 1.2 (`../../a-light-....`). Use *slicing* to remove the `../../` part inbetween. The final output should be: `https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html` \n",
    "3. The `replace` functions offers a more convenient way to \"search and replace\" in a string. The syntax is: `my_string = old_string.replace('text-to-replace', 'replace-by-text')`. Implement the `replace` function for the previous exercise 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tipping-the-velvet_999/index.html\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "url_book = soup.find_all(class_=\"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
    "print(url_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n"
     ]
    }
   ],
   "source": [
    "# Question 2 \n",
    "base_url = \"https://books.toscrape.com/catalogue/\"\n",
    "book_url = base_url + url_book[6:]\n",
    "print(book_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "base_url = \"https://books.toscrape.com/catalogue/\"\n",
    "book_url = base_url + url_book\n",
    "book_url = book_url.replace('../', '')\n",
    "print(book_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Iterating over items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "Ideally, we'd like our code to extract the URL from *every* book on the page, not just *one* product.\n",
    "\n",
    "In other words, we need a way to *iterate*/*loop* through the entire page to assemble a list of links (product pages) to scrape.\n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "Let's set up this exercise.\n",
    "\n",
    "1. We have a BeautifulSoup object, holding all of the book previews (`soup.find_all(class_=\"product_pod\")`)\n",
    "2. We have an empty array of `book_urls`, that we would like to fill\n",
    "3. We write a loop, which iterates through 1. and fills in 2.\n",
    "\n",
    "Run the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../a-light-in-the-attic_1000/index.html', '../../tipping-the-velvet_999/index.html', '../../soumission_998/index.html', '../../sharp-objects_997/index.html', '../../sapiens-a-brief-history-of-humankind_996/index.html']\n"
     ]
    }
   ],
   "source": [
    "# list of all books on the overview page\n",
    "books = soup.find_all(class_=\"product_pod\")\n",
    "book_urls = []\n",
    "\n",
    "for book in books: \n",
    "    book_url = book.find(\"a\").attrs[\"href\"]\n",
    "    book_urls.append(book_url)\n",
    "    \n",
    "# print the first five urls\n",
    "print(book_urls[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it may be more convenient to create a *dictionary* in which the `book_title` is the key and the `book_url` the value. This way it is more intuitive to look up the URL from a given book because you don't have to remember the exact position in the list but can simply pass it the title of the book. \n",
    "\n",
    "In the Google Inspector screenshot at the beginning of this section, you can see that the book title is stored in the `alt` attribute of the `<img>` tag (as well as in the `title` attribute from the second `<a>` tag). Using a similar approach as above, we collect the `book_title` and `book_url` of each book, and use these records to update `book_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = {}\n",
    "\n",
    "for book in books: \n",
    "    book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "    book_url = book.find(\"a\").attrs[\"href\"]\n",
    "    book_dict[book_title] = book_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we can simply pass the book title (mind the capitals!) to the dictionary to obtain the corresponding URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../a-light-in-the-attic_1000/index.html\n"
     ]
    }
   ],
   "source": [
    "print(book_dict['A Light in the Attic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "1. Like exercise 2.2, write code that transforms the relative URLs (`../..`) in `book_dict` into full URLs. Tip: you can use `for key, value in book_dict.items():` to iterate over the key-value pairs in the dictionary and update URLs accordingly. \n",
    "2. One of the books on `books.toscrape.com` is [Black Dust](https://books.toscrape.com/catalogue/black-dust_976/index.html). What happens once you pass this title as a key to `book_dict`? Why is that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "for key, value in book_dict.items():\n",
    "    book_dict[key] = (base_url + value).replace('../','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Black Dust'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f1e67d97bbf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Question 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbook_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Black Dust\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# it throws an error because the key does not exist (this book is on shown on the 2nd page and we only scraped the first one!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Black Dust'"
     ]
    }
   ],
   "source": [
    "# Question 2 \n",
    "book_dict[\"Black Dust\"] # it throws an error because the key does not exist (this book is on shown on the 2nd page and we only scraped the first one!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Page Navigation\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Alright - what have we learnt up this point?\n",
    "\n",
    "- Section 1.1 taught us how to extract links from a page, \n",
    "- Section 1.2 taught us how to extract *more specific links* from a page, and finally\n",
    "- Section 1.3 taught us how to assemble a list of *links* to *all* books listed on a specific page.\n",
    "\n",
    "So... what's missing?\n",
    "\n",
    "Exactly! The [`books.toscrape.com`](https://books.toscrape.com/catalogue/category/books_1/index.html) contains __1000 books__, spread across __50 pages__. \n",
    "\n",
    "So, the goal of this section is to navigate through the __entire book assortment__, not only the first 20 books.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Open [the website](https://books.toscrape.com/catalogue/category/books_1/index.html), and click on the \"next\" button at the bottom of the page.\n",
    "\n",
    "<img src=\"images/books.png\" align=\"left\" width=60%/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Repeat this a couple of times, and observe how the URL in your navigation bar is changing...\n",
    "\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-1.html`\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-2.html`\n",
    "- `https://books.toscrape.com/catalogue/category/books_1/page-3.html`\n",
    "\n",
    "Can you guess the next one...?\n",
    "\n",
    "Indeed! The URL can be divided into a __fixed base part__ (`https://books.toscrape.com/catalogue/category/books_1/`), and a __counter__ that is dependent on the page you're visiting (e.g., `page-1.html`). \n",
    "\n",
    "__Now let's create a list of all 50 URLs!__ \n",
    "\n",
    "First, we create a counter variable, which we now set to 1 (but it can take on any value later on). Then, we concatenate the `base_url` with the counter (note that we have to convert the integer counter to a string before we can do that, using the `str` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/page-1.html\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
    "print(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we generate a list of 50 `page_urls` with a for loop that starts at 1 and ends at 50 (not 51!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
    "page_urls = []\n",
    "\n",
    "for counter in range(1, 51):\n",
    "    full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
    "    page_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this gives a list of all page URLs that contain books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of page urls in the list is: 50\n"
     ]
    }
   ],
   "source": [
    "# print the last five page urls\n",
    "print(\"The number of page urls in the list is: \" + str(len(page_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "In this exercise, we practice generating a seed for another website, [`quotes.toscrape.com`](https://quotes.toscrape.com/), which displays 100 famous quotes from GoodReads, categorized by tag. \n",
    "\n",
    "<img src=\"images/quotes.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make yourself comfortable with how the [site](https://quotes.toscrape.com) works and ask yourself questions such as: how does the navigation work, how many pages are there, what is the base URL, and how does it change if I move to the next page?\n",
    "2. Generate a list `quote_page_urls` that contains the page URLs we need if we'd like to scrape all 100 quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. The 100 quotes are evenly spread across 10 pages. The base URL is `https://quotes.toscrape.com/page/` followed by a page number between 1 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://quotes.toscrape.com/page/1', 'https://quotes.toscrape.com/page/2', 'https://quotes.toscrape.com/page/3', 'https://quotes.toscrape.com/page/4', 'https://quotes.toscrape.com/page/5', 'https://quotes.toscrape.com/page/6', 'https://quotes.toscrape.com/page/7', 'https://quotes.toscrape.com/page/8', 'https://quotes.toscrape.com/page/9', 'https://quotes.toscrape.com/page/10']\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "base_url = \"https://quotes.toscrape.com/page/\"\n",
    "quote_page_urls = []\n",
    "\n",
    "for counter in range(1, 11):\n",
    "    full_url = base_url + str(counter)\n",
    "    quote_page_urls.append(full_url)\n",
    "\n",
    "print(quote_page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have defined our seed and thought about a data extraction strategy to obtain the book links on a page. Since there are multiple pages, we needed to generate a list of URLs as an input for our scraper, which we'll further refine in the next chapter.  \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction\n",
    "\n",
    "\n",
    "### 2.1 Timers\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Before we start running the scraper, we need to realize that sending many requests at the same time can overload a server. Therefore, it's highly recommended to pause between requests rather than sending them all simultaneously. This avoids that your IP address (i.e., numerical label assigned to each device connected to the internet) gets blocked, and you can no longer visit (and scrape) the website. \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "In Python, you can import the `sleep` module, which pauses the execution of future commands for a given amount of time. For example, the print statement after `sleep(5)` will only be executed after 5 seconds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll be printed to the console after 5 seconds!\n"
     ]
    }
   ],
   "source": [
    "# run this cell again to see the timer in action yourself!\n",
    "from time import sleep\n",
    "sleep(5)\n",
    "print(\"I'll be printed to the console after 5 seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 5__\n",
    "\n",
    "Modify the code above to sleep for 2 minutes. Go grab a coffee inbetween. Did it take you longer than 2 minutes?\n",
    "\n",
    "(if you want to abort the running code, just select the cell and push the \"stop\" button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-221186e284c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "sleep(2*60)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Modularization\n",
    "\n",
    "With this addition to our toolkit, let's finish up our book URL scraper by putting together everything we have learned thus far. Following Python conventions, let's modularize our  code into functions to improve readability and reusability. \n",
    "\n",
    "First, we define a function `generate_page_urls()` that takes a base URL and an upper limit of the number of pages (50) as input parameters. This way we can easily update our scraper if more books are added or if the base URL changes. \n",
    "\n",
    "Second, the `extract_book_urls()` function takes a list of page URLs as input and returns a dictionary of book titles and URLs. Note the two-step structure of the for-loops: on every page, we create a `books` object which we subsequently loop over by extracting the `book_title` and `book_url` from each book. These records are added to the dictionary `book_dict` which is eventually returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_page_urls(base_url, num_pages):\n",
    "    '''generate a list of full page urls from a base url and counter that has takes on the values between 1 and num_pages'''\n",
    "    page_urls = []\n",
    "    \n",
    "    for counter in range(1, num_pages + 1):\n",
    "        counter_url = f\"page-{counter}.html\"\n",
    "        full_url = base_url + counter_url \n",
    "        page_urls.append(full_url)\n",
    "        \n",
    "    return page_urls\n",
    "    \n",
    "def extract_book_urls(page_urls):\n",
    "    '''collect the book title and url for every book on all page urls'''\n",
    "    book_dict = {}\n",
    "    \n",
    "    for page_url in page_urls: \n",
    "        res = requests.get(page_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        books = soup.find_all(class_=\"product_pod\")\n",
    "\n",
    "        for book in books: \n",
    "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"][6:]\n",
    "            book_dict[book_title] = book_url\n",
    "            \n",
    "        sleep(1)  # pause 1 second after each request\n",
    "            \n",
    "    return book_dict\n",
    "    \n",
    "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
    "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
    "book_dict = extract_book_urls(page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this code works without problems, there is one little improvement that we can make. If the number of pages changes, we need to manually update the `num_pages` parameter. For example, we may miss out once new books are added which appear on page 51 and further. \n",
    "\n",
    "\n",
    "### 2.3 Next Page Button\n",
    "\n",
    "A general solution is therefore to look up whether there is a `next` button on the page (HTML code below). If so, it means a next page exists and we keep on incrementing the page counter by 1. If not, it means we have reached the last page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/next_page.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, we write the function `check_next_page()` which takes an URL as an input and returns the outgoing link of the next button (if present):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next page is: page-2.html\n"
     ]
    }
   ],
   "source": [
    "def check_next_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    next_btn = soup.find(class_= \"next\")\n",
    "    return next_btn.find(\"a\").attrs[\"href\"] if next_btn else None\n",
    "\n",
    "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "print(f\"The next page is: {check_next_page(page_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the function the first page of the bookshop, and it returns the link to the second page (note that `page-2.html` is a relative path from the current URL). Now let's check what happens once we pass it the 50th page!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "1. Pass `https://books.toscrape.com/catalogue/page-50.html` to `check_next_page()` and observe the output. Is that what you expected? \n",
    "2. Write a function `next_page_url()` that that checks whether the output of `check_next_page()` is not equal to `None` (i.e., anything but `None`). If so, it should return a new variable `page_url` that concatenates the base URL and the relative path to the next page. If not, it should print the statement `This is already the last page!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Question 1 \n",
    "output = check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\")\n",
    "print(output) # the output is None because page 50 is the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is already the last page!\n"
     ]
    }
   ],
   "source": [
    "# Question 2 \n",
    "def next_page_url(url):\n",
    "    base_url = \"https://books.toscrape.com/catalogue/\"\n",
    "    if url != None: \n",
    "        page_url = base_url + url \n",
    "        return page_url \n",
    "    else: \n",
    "        print(\"This is already the last page!\")\n",
    "        \n",
    "next_page_url(check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As a last step, we have revised the `extract_book_urls()` function. Instead of generating the list of page URLs up front, we now use a `while` loop that remains `True` as long as there is another new page. At the end of each loop, we update the `page_url` according to the link of the next button (using `check_next_page()`). On the last page, there is no new page URL and thus we break out of the while loop. \n",
    "\n",
    "All in all, we have modularized our code into functions, made it future-proof (e.g. if new books are added), and reduced the number of lines of code to get the job done! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_book_urls(page_url):\n",
    "    '''collect the book title and url for every book on all page urls'''\n",
    "    book_dict = {}\n",
    "\n",
    "    while page_url: \n",
    "        res = requests.get(page_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        books = soup.find_all(class_=\"product_pod\")\n",
    "\n",
    "        for book in books: \n",
    "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"][6:]\n",
    "            book_dict[book_title] = book_url\n",
    "\n",
    "        sleep(1)  # pause 1 second after each request\n",
    "        \n",
    "        if check_next_page(page_url) != None: \n",
    "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
    "        else: \n",
    "            break\n",
    "        \n",
    "    return book_dict\n",
    "\n",
    "book_dict = extract_book_urls(\"https://books.toscrape.com/catalogue/page-1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "Try to run the function, inspect the output for yourself, and answer the following questions (you may need to wait for a bit as the scraper loops through all 50 pages!).\n",
    "1. How many books are there in `book_dict`? Does this align with your initial expectations? Can you come up with a plausible explanation? (p.s. this is a tricky one, you have been warned!)\n",
    "2. Your good friend recommended the book `The Activist's Tao Te Ching: Ancient Advice for a Modern Revolution`. After looking up the reviews on [GoodReads](https://www.goodreads.com/book/show/25986790-the-activist-s-tao-te-ching?ac=1&from_search=true&qid=jpcvOsxKfP&rank=1) you decide to look for a copy of the book online. Does [books.toscrape.com](books.toscrape.com) offer a copy in their store? If so, do they have enough stock currently? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. There are 999 books (use: `len(book_dict)` to get the answer). The reason you find 999 books - rather than 1000 as listed on the homepage - is because there is one duplicate (Do Androids Dream of Electric Sheep? (Blade Runner #1)). Dictionaries can only store distinct keys and thus duplicate keys will be overwritten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/the-activists-tao-te-ching-ancient-advice-for-a-modern-revolution_928/index.html'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "# first we check whether the book is present in the dictionary (mind the capitals!)\n",
    "\"The Activist's Tao Te Ching: Ancient Advice for a Modern Revolution\" in book_dict\n",
    "\n",
    "# then we look up the corresponding URL -> on the page we find a stock level of 16 which is more than sufficient\n",
    "book_dict[\"The Activist's Tao Te Ching: Ancient Advice for a Modern Revolution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Page-Level Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember trying to obtain the URL of the [Black Bust](https://books.toscrape.com/catalogue/black-dust_976/index.html) book in exercise 2? Let's see whether it works this time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/black-dust_976/index.html\n"
     ]
    }
   ],
   "source": [
    "print(book_dict[\"Black Dust\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, it works flawlessly! But why did we need the book URLs in the first place? It forms the seed for other web scraping efforts. For example, the product descriptions can only be obtained from the book pages themselves which means we need to loop over all book URLs to extract the right information. In the follow-up exercise, we'll look at how to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "1. We'd like to extract the product description of our 3 favorite books. Fill in the blanks below to finish the `get_book_description()` function. Each `#` represents a single missing character (e.g. `####` means the solution requires 4 characters). \n",
    "2. Run the function and inspect the output. If you look carefully, you may spot `â\\x80\\x99t` symbols throughout the product description. Look up the original text on the book pages and compare it side-by-side with the output of `book_dict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_description(books):\n",
    "    book_descriptions = {}\n",
    "\n",
    "    for book in books: \n",
    "        page_url = book_dict[####]\n",
    "\n",
    "        res = requests.get(########)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # tip: look at the Google Inspector screenshot below \n",
    "        description = soup.find(id=\"content_inner\").find_all(\"p\")[#].get_text()\n",
    "        book_descriptions[####] = ###########\n",
    "\n",
    "    return book_descriptions\n",
    "\n",
    "favorite_books = [\"Black Dust\", \"The Grand Design\", \"Twenties Girl\"]\n",
    "book_descriptions = get_book_description(##############)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_description(books):\n",
    "    book_descriptions = {}\n",
    "\n",
    "    for book in books: \n",
    "        page_url = book_dict[book]\n",
    "\n",
    "        res = requests.get(page_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # tip: look at the Google Inspector screenshot below \n",
    "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
    "        book_descriptions[book] = description\n",
    "\n",
    "    return book_descriptions\n",
    "\n",
    "favorite_books = [\"Black Dust\", \"The Grand Design\", \"Twenties Girl\"]\n",
    "book_descriptions = get_book_description(favorite_books) # â\\x80\\x99t refers to an apostrophe (' - e.g. can't)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/black_dust.png\" align=\"left\" width=70% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.5 CSV Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we convert the dictionary into a Comma Separated Values (CSV) file which you can open up in any spreadsheet program (e.g., Excel). More specifically, we'd like to have a file with three columns: one for the book title, one for the product description, and another one for the current date and time. The latter helps you to distinguish between data from scrapers you run repeatedly. For example, you may run the book scraper at the beginning of every month to keep track of price changes of any of the books. Although you could store the data of each extraction moment into a separate file (e.g., `2021_01_01_book_prices.csv` for January 2021, `2021_02_01_book_prices.csv` for February 2021), we recommend always including a timestamp column to your scraped datasets. After all, losing or overwriting data can be disastrous (especially for scrapers) as you may never be able to obtain historical data (e.g., the price of a book 2 months ago)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that light, we import the `datetime` library which contains a function `now()` that automatically determines the current date and time which we'll incorporate into our final dataset. Run the cell again, and you'll see how the values update to the current time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-14 21:25:06.955207\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, CSV-files are simply text files with symbols that indicate the beginning of a new column (i.e., delimiter). Below you find a screenshot of the `book_descriptions.csv` file opened in a basic text editor. Every `;` and enter (empty line) indicate the start of a new column and row, respectively.\n",
    "\n",
    "<img src=\"images/csv_files.png\" align=\"left\" width=50% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel then applies this logic - converting semicolons and empty lines - to assign the data points to their respective cells: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/excel.png\" align=\"left\" width=50% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets more complicated once the delimiter has been embodied into data. For example, a comma is sometimes used as a delimiter but that would not work here because the product description also contains commas (e.g., `No matter how busy he keeps himself, successful Broadway...`). In that case, the part after the comma (`successful Broadway...`) would be regarded as a new column whereas it actually still belongs to the product description. For that reason, setting the delimiter to `;` is a safer choice here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write to a text file with the `csv` library. The first row is the header and contains the three column names (`\"title\", \"description\", \"date_time\"`). Thereafter, we iterate over the key-value pairs in the dictionary and add the current date time to it. Importantly, the `w` flag in the `with` statement indicates that the file will be overwritten every time the cell is executed. If you, however, want to append data to an existing file and avoid losing historical data, you can swap `w` for `a`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "with open(\"book_descriptions.csv\", \"w\") as csv_file: \n",
    "    writer = csv.writer(csv_file, delimiter = \";\")\n",
    "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
    "    now = datetime.now()\n",
    "    for title, description in book_descriptions.items(): \n",
    "        writer.writerow([title, description, now])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7 \n",
    "1. Run the cell above and look at the `book_descriptions.csv` file in Excel. Make sure it looks like the screenshot above (3 columns x 4 rows). Depending on the language settings on your machine, the data may not be correctly distributed over the columns. In that case, go to the \"Data\" tab in Excel, click the \"Text to Columns\" button in the ribbon, choose \"Delimited\", put a checkmark in front of \"Semicolon\", and choose \"Finish\".\n",
    "\n",
    "<img src=\"images/text_to_column.gif\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Close Excel, change the flag to `a`, and run the cell again. Open the `book_descriptions.csv` file again (and repeat the Text to Columns procedure if necessary). How does the output differ from the previous step? Why is that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "2. It shows the same data, including the header, twice (below one another). It goes beyond the scope of this course to define better alternatives (e.g., save data to a database).\n",
    "---\n",
    "\n",
    "At the beginning of this tutorial, we set out the promise of writing multi-page scrapers from start to finish. Although the examples we have studied are relatively simple, the same principles (seed definition, data extraction plan, page-level data collection) apply to any other website you'd like to scrape. \n",
    "\n",
    "Now that you have hopefully got the hang of using Jupyter notebooks, we're going to introduce you to an alternative that goes hand in hand with what you have learned thus far but overcomes some of its limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Executing Python Files\n",
    "\n",
    "### 3.1 Jupyter Notebooks vs Spyder\n",
    "Jupyter Notebooks are ideal for combining programming and markdown (e.g., text, plots, equations) which makes it the default choice for sharing and presenting data analyses from a reproducibility standpoint. Since we can execute code blocks one by one it's suitable for developing and debugging code on the fly. There are, however, some limitations to Jupyter Notebooks which makes us consider other Integrated Development Environments (IDEs) such as Spider. First, the order in which you run cells within a notebook may affect the results. While prototyping you may lose sight of the top-down hierarchy which can cause problems once you restart the kernel (e.g., a library is imported after is being used). Second, there is no easy way to browse through directories and files within a Jupyter Notebook. Third, notebooks cannot handle large codebases nor big data particularly well. For those reasons, we recommend starting out in Jupyter Notebooks, moving code into functions along the way, and once all seems to be running well, copy-paste all necessary code into Spider. From there you can save it as a Python file (`.py`) - rather than a notebook (`.ipynb`) - and execute the file from the command line. Next, we introduce you to the Spider IDE, and learn how to run Python files from the command line. \n",
    "\n",
    "### 3.2 Introduction to Spyder\n",
    "The first time you need to click on the green \"Install\" button in Anaconda Navigator after which you start Spyder by clicking on the blue \"Launch\" button (alternatively, type `spyder` in the terminal). \n",
    "\n",
    "<img src=\"images/anaconda_navigator.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interface consists of three panels: \n",
    "1. **Code editor** = where you write Python code (i.e., the content of code cells in a notebook)\n",
    "2. **Variable / files** = depending on which tab you choose either an overview of all declared variables (e.g. look up their type or change their values) or a file explorer (e.g., to open other Python files)\n",
    "3. **Console** = the output of running the Python script from the code editor (what normally appears below each cell in a notebook)\n",
    "\n",
    "<img src=\"images/spyder.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `webscraping_101.py` file above, we have put together all code snippets from this notebook needed to scrape and store the URLs of all books. To run the script you either click on the green play button to run all code (from line 1 to 46). As an alternative, you can highlight the parts of the script you want to execute and then click the run selection button.\n",
    "\n",
    "<img src=\"images/toolbar.png\" width=40% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the script is running, you may need to interrupt the execution because it is simply taking too long or you spotted a bug somewhere. Click on the red rectangular in the console to stop the execution. \n",
    "\n",
    "<img src=\"images/interrupt.gif\" width=80% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "1. Start (and install) Spyder and open `webscraping101.py` (`File` > `Open`). Compare this notebook and the Python script in Spyder side-by-side: which do you find clearer? \n",
    "2. Run the script and then open the `book_urls.csv` file in Excel. Where is the file stored on your computer? How many records are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. It remains a personal opinion but I'd say the `.py` looks neater because all the code is in the same view (e.g., all import statements below each other rather than spreading them throughout your notebook)\n",
    "2. Exported files appear in the same working directory (unless specified differently). The `book_urls.csv` file contains 1000 rows (999 records and 1 header row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run Python Files \n",
    "* *Mac*\n",
    "    1. Open the terminal and navigate to the folder in which the `.py` file has been saved (use `cd` to change directories and `ls` to list all files).\n",
    "    2. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`).\n",
    "\n",
    "<img src=\"images/running_python.gif\" width=60% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Windows*\n",
    "    1. Open Windows explorer and navigate to the folder in which the `.py` file has been saved. Type `cmd` to open the command prompt. Alternatively, open the command prompt from the start menu (and use `cd` to change directories and `dir` to list files).\n",
    "    2. Activate Anaconda by typing `conda activate`.\n",
    "    3. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Wrap-up\n",
    "\n",
    "Although we barely scratched the surface of Spyder and the command line, it's already the end of this tutorial. Keep up the good work!+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
