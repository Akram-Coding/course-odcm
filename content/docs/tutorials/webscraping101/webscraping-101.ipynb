{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101 (oDCM)\n",
    "\n",
    "*This tutorial is a comprehensive take on extracting data via webscraping from multiple pages, and exporting that data into CSV files so that you can work with the data after collection.*\n",
    "In this tutorial, you get to know the art of collecting data from mul\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to: \n",
    "* Generate a seed by scraping and parsing URLs from a parent object\n",
    "* Select data for extraction on a website using CSS selectors\n",
    "* Loop through list of URLs to capture data in bulk using functions\n",
    "* Generate a seed by scraping and parsing URLs from a parent object\n",
    "* Write a dictionary of data to a CSV file (enriched with metadata)\n",
    "* Run data entire data collection from a Python script on the command line\n",
    "\n",
    "HD:\n",
    "* Obtain list of entities to scrape (internal seeds)\n",
    "* Map navigation path on a website using URLs, and understand how to use parameters to modify result sets\n",
    "* Select data for extraction on a website using XPATHs and CSS selectors\n",
    "* Write data to CSV file, and enrich with relevant metadata\n",
    "* Bundle data capture in Python functions, and modularize extraction code\n",
    "* Loop through list of URLs to capture data in bulk, using functions\n",
    "* Understand difference between Jupyter Notebooks and “raw” Python files, and port extraction code to Python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This course draws on online resources built by Brian Keegan, Colt Steele, David Amos, Hannah Cushman Garland, Kimberly Fessel, and Thomas Laetsch.  @ROY LINKS\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Contact\n",
    "For technical issues try to be as specific as possible (e.g., include screenshots, your notebook, errors) so that we can help you better.\n",
    "\n",
    "**WhatsApp**  \n",
    "+31 13 466 8938\n",
    "\n",
    "**Email**  \n",
    "odcm@uvt.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generating Seeds\n",
    "\n",
    "\n",
    "### 1.1 Collecting Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In web scraping we typically refer to a \"seed\" as a starting point for data collection. For example, we can first scrape all book links from the overview page and then iterate over all links to scrape the product description (or anything else on that page). In this case we pick the [book catalogue](https://books.toscrape.com/catalogue/category/books_1/index.html) as our seed. From this page there are two ways to move towards a book page: either by clicking on the book cover or on the title of the book (figure below). \n",
    "\n",
    "<img src=\"images/books_links.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also becomes clear once we inspect the underlying HTML code with Google Chrome Inspector. The thumbnails (`<img>`) are surrounded by `<a>` tags which contain a link (`href`) to the book. Also, within the book headers (`<h3>`) we find nested links (`<a>`) to the book pages:\n",
    "\n",
    "<img src=\"images/inspector_links.png\" align=\"left\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we got away with selecting elements by tag (e.g., `<h2>`) but this time we'll run into problems if we try to filter down on `<a>` tags (i.e., links). Why? Because the overview page also contains `<a>` elements we are not interested in: \n",
    "\n",
    "* \"Books to Scrape\" link at the top\n",
    "* \"Home\" breadcrumb link \n",
    "* Left sidebar with all book genres (e.g., Travel)\n",
    "* The next button at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that reason, we need to be more specific so that we only scrape the links to the book pages and ignore all other `<a>` tags. Let's briefly revisit the notion of HTML classes. A class is often used as a reference in the code. For example, to make all text elements with a given class blue or increase the font size. In the screenshot above you find an `<article>` tag with class `product_pod` in which a `<div>` is nested which contains the image and link attribute we're after. Every link to a book is nested within this class, but aforementioned `<a>` tags on other parts of the page are not. Thus, if we can tell our scraper that we're only interested in the `<a>` tags within the `product_pod` class, we end up with our desired selection of links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use `.find_all()` to capture all matching elements on the page. The difference, however, is that we specifiy it's a class (`class_=`), rather than a HTML tag, and the class name we need (`product_pod`). This returns a list with all 20 `product_pod` classes on the page (i.e., one for each book). In this example, we pick the first book from the list (A Light in the Attic, element `[0]` from the list) and extract the `<a>` tag nested within the `product_pod` class. Finally, we pull out the `href` attribute from the `<a>` tag which gives us the book link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "soup.find_all(class_=\"product_pod\")[0].find(\"a\").attrs[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `../../` in front of the link which tells the browser: go back two directories from the current URL:\n",
    "* Current URL: https://books.toscrape.com/catalogue/category/books_1/index.html\n",
    "* 1 step back: https://books.toscrape.com/catalogue/category/books_1\n",
    "* 2 steps back: https://books.toscrape.com/catalogue/category/\n",
    "\n",
    "Thereafter, it appends `a-light-in-the-attic_1000/index.html` to the URL which forms the full link to the [A Light in the Attic](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html) book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "1. Extract the link from the second book (Tipping the Velvet) using BeautifulSoup.\n",
    "2. Create a new variable `book_url` that concatenates the base URL (` https://books.toscrape.com/catalogue/category/`) and the string you extracted in the previous exercise. Use slicing to remove the `../../` part in between. The final output should be: `https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Iterating over items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd like our code to extract the URL from every book (not just a single one). Hence, we store the temporary results in a variable `books` and loop over each book of which the link is stored in a list `book_urls`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../a-light-in-the-attic_1000/index.html', '../../tipping-the-velvet_999/index.html', '../../soumission_998/index.html', '../../sharp-objects_997/index.html', '../../sapiens-a-brief-history-of-humankind_996/index.html']\n"
     ]
    }
   ],
   "source": [
    "# list of all books on the overview page\n",
    "books = soup.find_all(class_=\"product_pod\")\n",
    "book_urls = []\n",
    "\n",
    "for book in books: \n",
    "    book_url = book.find(\"a\").attrs[\"href\"]\n",
    "    book_urls.append(book_url)\n",
    "    \n",
    "# print the first five urls\n",
    "print(book_urls[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, it may be more convenient to create a dictionary in which the `book_title` is the key and the `book_url` the value. This way it is more intuitive to look up the URL from a given book because you don't have to remember the exact position in the list but can simply pass it the title of the book. \n",
    "\n",
    "In the Google Inspector screenshot in the beginning of this section, you can see that the book title is stored in the `alt` attribute from the `<img>` tag (as well as in the `title` attribute from the second `<a>` tag). Using a similar approach as above, we collect the `book_title` and `book_url` of each book and use these records to update `book_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = {}\n",
    "\n",
    "for book in books: \n",
    "    book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "    book_url = book.find(\"a\").attrs[\"href\"]\n",
    "    book_dict[book_title] = book_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we can simply pass the book title (mind the capitals!) to the dictionary to obtain the corresponding URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../a-light-in-the-attic_1000/index.html\n"
     ]
    }
   ],
   "source": [
    "print(book_dict['A Light in the Attic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "1. Like exercise 1.2, write a program that transforms the relative URLs (`../..`) in `book_dict` into a full URL. Tip: you can use `for key, value in book_dict.items():` to iterate over the key value pairs in the dcitionary and update URLs accordingly. \n",
    "2. One of the books on `books.toscrape.com` is [Black Dust](https://books.toscrape.com/catalogue/black-dust_976/index.html). What happens once you pass this title as a key to `book_dict`? Why is that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Page Navigation\n",
    "The [`books.toscrape.com`](https://books.toscrape.com/catalogue/category/books_1/index.html) contains 1000 books which are spread across 50 pages. At the bottom of the page, you can click on \"next\" to move to the next page. \n",
    "\n",
    "<img src=\"images/books.png\" align=\"left\" width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you repeat this a couple of times, you get the following pattern of URLs:\n",
    "\n",
    "`https://books.toscrape.com/catalogue/category/books_1/page-1.html`\n",
    "`https://books.toscrape.com/catalogue/category/books_1/page-2.html`\n",
    "`https://books.toscrape.com/catalogue/category/books_1/page-3.html`\n",
    "\n",
    "Can you guess the next one? Indeed, the URL can be divided into a fixed base url (`https://books.toscrape.com/catalogue/category/books_1/`) and a counter that is dependent on the page you're visiting (e.g., `page-1.html`). Now let's create a list of all 50 URLs! First, we create a f-string `counter_url` variable of which we can change the `counter` variable. Next, we concatenate the `base_url` and the `counter_url` to get the `full_url`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/category/books_1/page-1.html\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "counter_url = f\"page-{counter}.html\"\n",
    "full_url = base_url + counter_url \n",
    "print(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we generate a list of 50 `page_urls` with a for loop that starts at 1 and ends at 50 (not 51!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
    "page_urls = []\n",
    "\n",
    "for counter in range(1, 51):\n",
    "    counter_url = f\"page-{counter}.html\"\n",
    "    full_url = base_url + counter_url \n",
    "    page_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this gives a list of all page urls that contain books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of page urls in the list is: 50\n"
     ]
    }
   ],
   "source": [
    "# print the last five page urls\n",
    "print(f\"The number of page urls in the list is: {len(page_urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "In this exercise, we practise with generating a seed for another website, [`quotes.toscrape.com`](https://quotes.toscrape.com/), which displays 100 famous quotes from GoodReads categorized by tag. \n",
    "\n",
    "<img src=\"images/quotes.png\" align=\"left\" width=70% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make yourself comfortable with how the [site](https://quotes.toscrape.com) works and ask yourself questions such as: how does the nagiation work, how many pages are there, what is the base url, and how does it change if I move to the next page?\n",
    "2. Generate a list `quote_pager_urls` that contains the page urls we need if we'd like to scrape all 100 quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction\n",
    "\n",
    "\n",
    "### 2.1 Timers\n",
    "Next, we combine the concepts from section 1.2 and 1.3 to extract the book urls across different pages. But before we start implementing it, we need to realize that sending many requests at the same time can overload a server. Therefore, it's highly recommended to pause between requests rather than sending them simultaneously. This avoids that your IP address (i.e., numerical label assigned to each device connected to the internet) gets blocked and you can no longer visit (and scrape) the website. \n",
    "\n",
    "In Python you can import the `sleep` module which pauses the execution of future commands for a given amount of time. For example, the print statement after `sleep(5)` will only be executed after 5 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll be printed to the console after 5 seconds!\n"
     ]
    }
   ],
   "source": [
    "# run this cell again to see the timer in action yourself!\n",
    "from time import sleep\n",
    "sleep(5)\n",
    "print(\"I'll be printed to the console after 5 seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this addition to our toolkit, let's finish up our book URL scraper by putting together everything we have learned thus far. Following Python conventions, let's modularize our  code into functions to improve readability and reusabilty. \n",
    "\n",
    "First, we define a function `generate_page_urls()` that takes a base url and an upper limit of the number of pages (50) as input parameters. This way we can easily update our scraper if more books are added or if the base url changes. \n",
    "\n",
    "Second, the `extract_book_urls()` function takes a list of page urls as input and returns a dictionary of book titles and URLs. Note the two-step structure of the for-loops: on every page we create a `books` object which we subsequently loop over by extracting the `book_title` and `book_url` from each book. These records are added to the dictionary `book_dict` which is eventually returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_page_urls(base_url, num_pages):\n",
    "    '''generate a list of full page urls from a base url and counter that has takes on the values between 1 and num_pages'''\n",
    "    page_urls = []\n",
    "    \n",
    "    for counter in range(1, num_pages + 1):\n",
    "        counter_url = f\"page-{counter}.html\"\n",
    "        full_url = base_url + counter_url \n",
    "        page_urls.append(full_url)\n",
    "        \n",
    "    return page_urls\n",
    "    \n",
    "def extract_book_urls(page_urls):\n",
    "    '''collect the book title and url for every book on all page urls'''\n",
    "    book_dict = {}\n",
    "    \n",
    "    for page_url in page_urls: \n",
    "        res = requests.get(page_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        books = soup.find_all(class_=\"product_pod\")\n",
    "\n",
    "        for book in books: \n",
    "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"][6:]\n",
    "            book_dict[book_title] = book_url\n",
    "            \n",
    "        sleep(1)  # pause 1 second after each request\n",
    "            \n",
    "    return book_dict\n",
    "    \n",
    "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
    "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
    "book_dict = extract_book_urls(page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this code works without problems, there is one little improvement that we can make. If the number of pages changes, we need to manually update the `num_pages` parameter. For example, we may miss out once new books are added which appear on page 51 and further. \n",
    "\n",
    "\n",
    "### 2.2 Next Page Button\n",
    "\n",
    "A general solution is therefore to look up whether there is a `next` button on the page (HTML code below). If so, it means a next page exists and we keep on incrementing the page counter by 1. If not, it means we have reached the last page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/next_page.png\" align=\"left\" width=70% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, we write the function `check_next_page()` which takes an URL as input and returns the outgoing link of the next button (if present):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next page is: page-2.html\n"
     ]
    }
   ],
   "source": [
    "def check_next_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    next_btn = soup.find(class_= \"next\")\n",
    "    return next_btn.find(\"a\").attrs[\"href\"] if next_btn else None\n",
    "\n",
    "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "print(f\"The next page is: {check_next_page(page_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it the first page of the bookshop, and it returns the link to the second page (note that `page-2.html` is a relative path from the current URL). Now let's check what happens once we pass it the 50th page!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "1. Pass `https://books.toscrape.com/catalogue/page-50.html` to `check_next_page()` and observe the output. Is that what you expected? \n",
    "2. Write a function that that checks whether the output of `check_next_page()` is not `None` (i.e., anything but `None`). If so, it should return a new variable `page_url` that concatenates the base url and the relative path to the next page. If not, it should print the statement `This is the last page!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, we have revised the `extract_book_urls()` function. Instead of generating the list of page URLs upfront, we now use a `while` loop that stays `True` as long as there is a page url. At the end of each loop we update the `page_url` according to the link of the next button (using `check_next_page()`). On the last page there is no new page url and thus we break out of the while loop. All in all, we have modularized our code into functions, made it future-proof (e.g. if new books are added), and reduced the number of lines of code to get the job done! Try to run the function and inspect the output for yourself (you may need to wait for a bit as the scraper loops through all 50 pages!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_book_urls(page_url):\n",
    "    '''collect the book title and url for every book on all page urls'''\n",
    "    book_dict = {}\n",
    "\n",
    "    while page_url: \n",
    "        res = requests.get(page_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        books = soup.find_all(class_=\"product_pod\")\n",
    "\n",
    "        for book in books: \n",
    "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"][6:]\n",
    "            book_dict[book_title] = book_url\n",
    "\n",
    "        sleep(1)  # pause 1 second after each request\n",
    "        \n",
    "        if check_next_page(page_url) != None: \n",
    "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
    "        else: \n",
    "            break\n",
    "        \n",
    "    return book_dict\n",
    "\n",
    "book_dict = extract_book_urls(\"https://books.toscrape.com/catalogue/page-1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Page-level Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember trying to obtain the URL of the [Black Bust](https://books.toscrape.com/catalogue/black-dust_976/index.html) book in exercise 2? Let's see whether it works this time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/black-dust_976/index.html\n"
     ]
    }
   ],
   "source": [
    "print(book_dict[\"Black Dust\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, it works flawlessly! But why did we need the book URLs in the first place? It forms the seed for other web scraping efforts. For example, the product descriptions can only be obtained from the book pages itself which means we need to loop over all book urls to extract the right information. In the follow-up exercise, we'll look at how to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "1. We'd like to extract the product description of our 3 favorite books. Fill in the blanks below to finish the `get_book_description()` function. Each `#` represents a single missing character (e.g. `####` means the solution requires 4 characters). \n",
    "2. Run the function and inspect the output. If you look carefully, you may spot `\\x80\\x99t` symbols throughout the product description. Look up the original text on the book pages and compare it side-by-side with the output of `book_dict`. What do these symbols mean? In the  Web Scraping Advanced in week 5, we'll go more in-depth about what it is exactly and how you can encode such characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_description(books):\n",
    "    book_descriptions = {}\n",
    "\n",
    "    for book in books: \n",
    "        page_url = book_dict[####]\n",
    "\n",
    "        res = requests.get(########)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # tip: look at the Google Inspector screenshot below \n",
    "        description = soup.find(id=\"content_inner\").find_all(\"p\")[#].get_text()\n",
    "        book_descriptions[####] = ###########\n",
    "\n",
    "    return book_descriptions\n",
    "\n",
    "favorite_books = [\"Black Dust\", \"The Grand Design\", \"Twenties Girl\"]\n",
    "book_descriptions = get_book_description(##############)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/black_dust.png\" align=\"left\" width=90% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 CSV Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we convert the dictionary into a Comma Separated Values (CSV) file which you can open up in any spreadsheet program (e.g., Excel). More specifically, we'd like to have a file with three columns: one for the book title, one for the product description, and another one for the current date and time. The latter helps you to distinguish between data from scrapers you run repeatedly. For example, you may run the book scraper at the beginning of every month to keep track of price changes of any of the books. Although you could store the data of each extraction moments into a separate file (e.g., `2021_01_01_book_prices.csv` for January '21, `2021_02_01_book_prices.csv` for February '21), we recommend always adding a timestamp column to your scraped datasets. After all, losing or overwriting data can be disastrous as you may not be able to obtain historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datetime` library contains a function `now()` that automatically determines the current date and time which we'll incorporate into our final dataset. Run the cell again, and you'll see how the values change: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 15:55:49.569541\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, CSV files are just text files with a symbol that indicates the beginning of a new column (i.e., delimiter). Below you find a screenshot of the `book_descriptions.csv` file opened in a text editor. Every `;` and enter (whiteline) indicate the start of a new column and row, respectively.\n",
    "\n",
    "<img src=\"images/csv_files.png\" align=\"left\" width=50% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel then uses this logic to assign the data points to their respective cells: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/excel.png\" align=\"left\" width=50% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets more complicated once the delimiter has been embodied into data. For example, a comma is sometimes used as a delimiter but that would not work here because the product description also contains commas (e.g., `No matter how busy he keeps himself, successfull Broadway...`). In that case, the part after the comma (`successfull Broadway...`) would be regarded as a new column whereas it actually still belongs to the product description. For that reason, setting the delimiter to `;` is a safer choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like you learned in the Python Bootcamp, we can write to a text file with the `csv` library. The first row is the header and contains the three column names (`\"title\", \"description\", \"date_time\"`). Thereafter, we iterate over the key value pairs in the dictionary and add the current date time to it. Importantly, the `w` flag in the `with` statement determines that the file will be overwritten every time the cell is executed. If you, however, want to append data to an existing file, you can swap `w` for `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "with open(\"book_descriptions.csv\", \"w\") as csv_file: \n",
    "    writer = csv.writer(csv_file, delimiter = \";\")\n",
    "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
    "    now = datetime.now()\n",
    "    for title, description in book_descriptions.items(): \n",
    "        writer.writerow([title, description, now])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6 \n",
    "1. Run the cell above and look at the `book_descriptions.csv` file in Excel and make sure it looks like the screenshot above. Depending on the language settings on your machine, the data may not be correctly distributed over the columns. In that case, go to the \"Data\" tab in Excel, click the \"Text to Columns\" button in the ribbon, choose for \"Delimited\", put a checkmark in front of \"Semicolon\" (see below), and choose \"Finish\".\n",
    "\n",
    "<img src=\"images/text_to_column.png\" align=\"left\" width=50% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Close Excel, change the flag to `a`, and run the cell again. Open the `book_descriptions.csv` file again (and repeat the Text to Columns procedure if necessary). How does the output differ from the previous step? Why is that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Executing Python Files\n",
    "\n",
    "### 3.1 Jupyter Notebooks vs Spyder\n",
    "Jupyter Notebooks are ideal for combining programming and markdown (e.g., text, plots, equations) which makes it the default choice for sharing and presenting data analyses from a reproducibilty standpoint. Since we can execute code blocks one by one it's suitable for developing and debugging code on the fly. There are, however, some limitations to Jupyter Notebooks which makes us consider other Integrated Development Environments (IDEs) such as Spider. First, the order in which you run cells within a notebook may affect the results. While prototyping you may lose sight of the top down hierarchy which can cause problems once you restart the kernel (e.g., a library is imported after is being used). Second, there is no easy way to browse through directories and files within a Jupyter Notebook. Third, notebooks cannot handle large codebases nor big data particularly well. For those reasons, we recommend starting out in Jupyter Notebooks, moving code into functions along the way, and once all seems to be running well, copy paste all necessary code into Spider. From there you can save it as a Python file (`.py`) - rather than a notebook (`.ipynb`) - and execute the file from the command line. Next, we introduce you to the Spider IDE, and learn you how to run Python files from the command line. \n",
    "\n",
    "### 3.2 Introduction to Spyder\n",
    "The first time you need to click on the green \"Install\" button in Anaconda Navigater after which you start Spyder by clicking on the blue \"Launch\" button (alternatively, type `spyder` in the terminal). \n",
    "\n",
    "<img src=\"images/anaconda_navigator.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interface consists of three panels: \n",
    "1. **Code editor** = where you write Python code (i.e., the content of code cells in a notebook)\n",
    "2. **Variable / files** = depending on which tab you choose either an overview of all declared variables (e.g. look up their type or change their values) or a file explorer (e.g., to open other Python files)\n",
    "3. **Console** = the output of running the Python script from the code editor (what normally appears below each cell in a notebook)\n",
    "\n",
    "<img src=\"images/spyder.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `webscraping_101.py` file above, we have put together all code snippets from this notebook needed to scrape and store the URLs of all books. To run the script you either click on the green play button to run all code (from line 1 to 46). As an alternative, you can highlight the parts of the script you want to execute and then click the run selection button.\n",
    "\n",
    "<img src=\"images/toolbar.png\" width=50% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the script is running, you may need to interrupt the execution because it is simply taking too long or you spotted a bug somewhere. Click on the red rectangular in  the console to stop the execution. \n",
    "\n",
    "<img src=\"images/stop_spyder.png\" width=80% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "1. Start (and install) Spyder and open `webscraping101.py` (`File` > `Open`). Compare this notebook and the Python script in Spyder side-by-side: which do you find clearer? \n",
    "2. Run the script and then open the `book_urls.csv` file in Excel. Where is the file stored on your computer? How many records are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run Python Files \n",
    "* *Mac*\n",
    "    1. Open the terminal and navigate to the folder in which the `.py` file has been saved (use `cd` to change directories and `ls` to list all files).\n",
    "    2. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`).\n",
    "\n",
    "<img src=\"images/terminal.png\" width=50% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Windows*\n",
    "    1. XXX\n",
    "    2. XXX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
