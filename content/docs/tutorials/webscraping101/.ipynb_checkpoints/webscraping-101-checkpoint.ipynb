{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101 (oDCM)\n",
    "\n",
    "*In the Webdata for Dummies tutorial we had a first brief look at scraping elements from a single web page (the \"A Light in the Attic\" book). In this tutorial we extend this idea by collecting information from multiple pages, exporting the data to CSV, and running Python files from the command line.*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to: \n",
    "* Generate a seed by scraping and parsing URLs from a parent object\n",
    "* Select data for extraction on a website using CSS selectors\n",
    "* Loop through list of URLs to capture data in bulk using functions\n",
    "* Generate a seed by scraping and parsing URLs from a parent object\n",
    "* Write a dictionary of data to a CSV file (enriched with metadata)\n",
    "* Run data entire data collection from a Python script on the command line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This course draws on online resources built by Brian Keegan, Colt Steele, David Amos, Hannah Cushman Garland, Kimberly Fessel, and Thomas Laetsch. \n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Contact\n",
    "For technical issues try to be as specific as possible (e.g., include screenshots, your notebook, errors) so that we can help you better.\n",
    "\n",
    "**WhatsApp**  \n",
    "+31 13 466 8938\n",
    "\n",
    "**Email**  \n",
    "odcm@uvt.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generating Seeds\n",
    "\n",
    "\n",
    "### 1.1 Collecting Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In web scraping we typically refer to a \"seed\" as a starting point for data collection. For example, we can first scrape all book links from the overview page and then iterate over all links to scrape the product description (or anything else on that page). In this case we pick the [book catalogue](https://books.toscrape.com/catalogue/category/books_1/index.html) as our seed. From this page there are two ways to move towards a book page: either by clicking on the book cover or on the title of the book (figure below). \n",
    "\n",
    "<img src=\"images/books_links.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also becomes clear once we inspect the underlying HTML code with Google Chrome Inspector. The thumbnails (`<img>`) are surrounded by `<a>` tags which contain a link (`href`) to the book. Also, within the book headers (`<h3>`) we find nested links (`<a>`) to the book pages:\n",
    "\n",
    "<img src=\"images/inspector_links.png\" align=\"left\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we got away with selecting elements by tag (e.g., `<h2>`) but this time we'll run into problems if we try to filter down on `<a>` tags (i.e., links). Why? Because the overview page also contains `<a>` elements we are not interested in: \n",
    "\n",
    "* \"Books to Scrape\" link at the top\n",
    "* \"Home\" breadcrumb link \n",
    "* Left sidebar with all book genres (e.g., Travel)\n",
    "* The next button at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that reason, we need to be more specific so that we only scrape the links to the book pages and ignore all other `<a>` tags. Let's briefly revisit the notion of HTML classes. A class is often used as a reference in the code. For example, to make all text elements with a given class blue or increase the font size. In the screenshot above you find an `<article>` tag with class `product_pod` in which a `<div>` is nested which contains the image and link attribute we're after. Every link to a book is nested within this class, but aforementioned `<a>` tags on other parts of the page are not. Thus, if we can tell our scraper that we're only interested in the `<a>` tags within the `product_pod` class, we end up with our desired selection of links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use `.find_all()` to capture all matching elements on the page. The difference, however, is that we specifiy it's a class (`class_=`), rather than a HTML tag, and the class name we need (`product_pod`). This returns a list with all 20 `product_pod` classes on the page (i.e., one for each book). In this example, we pick the first book from the list (A Light in the Attic, element `[0]` from the list) and extract the `<a>` tag nested within the `product_pod` class. Finally, we pull out the `href` attribute from the `<a>` tag which gives us the book link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "soup.find_all(class_=\"product_pod\")[0].find(\"a\").attrs[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `../../` in front of the link which tells the browser: go back two directories from the current URL:\n",
    "* Current URL: https://books.toscrape.com/catalogue/category/books_1/index.html\n",
    "* 1 step back: https://books.toscrape.com/catalogue/category/books_1\n",
    "* 2 steps back: https://books.toscrape.com/catalogue/category/\n",
    "\n",
    "Thereafter, it appends `a-light-in-the-attic_1000/index.html` to the URL which forms the full link to the [A Light in the Attic](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html) book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "1. Extract the link from the second book (Tipping the Velvet) using BeautifulSoup.\n",
    "2. Create a new variable `book_url` that concatenates the base URL (` https://books.toscrape.com/catalogue/category/`) and the string you extracted in the previous exercise. Use slicing to remove the `../../` part in between. The final output should be: `https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Iterating over items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd like our code to extract the URL from every book (not just a single one). Hence, we store the temporary results in a variable `books` and loop over each book of which the link is stored in a list `book_urls`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../a-light-in-the-attic_1000/index.html', '../../tipping-the-velvet_999/index.html', '../../soumission_998/index.html', '../../sharp-objects_997/index.html', '../../sapiens-a-brief-history-of-humankind_996/index.html']\n"
     ]
    }
   ],
   "source": [
    "# list of all books on the overview page\n",
    "books = soup.find_all(class_=\"product_pod\")\n",
    "book_urls = []\n",
    "\n",
    "for book in books: \n",
    "    book_url = book.find(\"a\").attrs[\"href\"]\n",
    "    book_urls.append(book_url)\n",
    "    \n",
    "# print the first five urls\n",
    "print(book_urls[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, it may be more convenient to create a dictionary in which the `book_title` is the key and the `book_url` the value. This way it is more intuitive to look up the URL from a given book because you don't have to remember the exact position in the list but can simply pass it the title of the book. \n",
    "\n",
    "In the Google Inspector screenshot in the beginning of this section, you can see that the book title is stored in the `alt` attribute from the `<img>` tag (as well as in the `title` attribute from the second `<a>` tag). Using a similar approach as above, we collect the `book_title` and `book_url` of each book and use these records to update `book_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = {}\n",
    "\n",
    "for book in books: \n",
    "    book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "    book_url = book.find(\"a\").attrs[\"href\"]\n",
    "    book_dict[book_title] = book_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we can simply pass the book title (mind the capitals!) to the dictionary to obtain the corresponding URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../a-light-in-the-attic_1000/index.html\n"
     ]
    }
   ],
   "source": [
    "print(book_dict['A Light in the Attic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "1. Like exercise 1.2, write a program that transforms the relative URLs (`../..`) in `book_dict` into a full URL. Tip: you can use `for key, value in book_dict.items():` to iterate over the key value pairs in the dcitionary and update URLs accordingly. \n",
    "2. One of the books on `books.toscrape.com` is [Black Dust](https://books.toscrape.com/catalogue/black-dust_976/index.html). What happens once you pass this title as a key to `book_dict`? Why is that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Page Navigation\n",
    "The [`books.toscrape.com`](https://books.toscrape.com/catalogue/category/books_1/index.html) contains 1000 books which are spread across 50 pages. At the bottom of the page, you can click on \"next\" to move to the next page. \n",
    "\n",
    "<img src=\"images/books.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you repeat this a couple of times, you get the following pattern of URLs:\n",
    "\n",
    "`https://books.toscrape.com/catalogue/category/books_1/page-1.html`\n",
    "`https://books.toscrape.com/catalogue/category/books_1/page-2.html`\n",
    "`https://books.toscrape.com/catalogue/category/books_1/page-3.html`\n",
    "\n",
    "Can you guess the next one? Indeed, the URL can be divided into a fixed base url (`https://books.toscrape.com/catalogue/category/books_1/`) and a counter that is dependent on the page you're visiting (e.g., `page-1.html`). Now let's create a list of all 50 URLs! First, we create a f-string `counter_url` variable of which we can change the `counter` variable. Next, we concatenate the `base_url` and the `counter_url` to get the `full_url`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/category/books_1/page-1.html\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "counter_url = f\"page-{counter}.html\"\n",
    "full_url = base_url + counter_url \n",
    "print(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we generate a list of 50 `page_urls` with a for loop that starts at 1 and ends at 50 (not 51!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
    "page_urls = []\n",
    "\n",
    "for counter in range(1, 51):\n",
    "    counter_url = f\"page-{counter}.html\"\n",
    "    full_url = base_url + counter_url \n",
    "    page_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this gives a list of all page urls that contain books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of page urls in the list is: 50\n"
     ]
    }
   ],
   "source": [
    "# print the last five page urls\n",
    "print(f\"The number of page urls in the list is: {len(page_urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "In this exercise, we practise with generating a seed for another website, [`quotes.toscrape.com`](https://quotes.toscrape.com/), which displays 100 famous quotes from GoodReads categorized by tag. \n",
    "\n",
    "<img src=\"images/quotes.png\" align=\"left\" width=70% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make yourself comfortable with how the [site](https://quotes.toscrape.com) works and ask yourself questions such as: how does the nagiation work, how many pages are there, what is the base url, and how does it change if I move to the next page?\n",
    "2. Generate a list `quote_pager_urls` that contains the page urls we need if we'd like to scrape all 100 quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction\n",
    "\n",
    "\n",
    "### 2.1 Timers\n",
    "Next, we combine the concepts from section 1.2 and 1.3 to extract the book urls across different pages. But before we start implementing it, we need to realize that sending many requests at the same time can overload a server. Therefore, it's highly recommended to pause between requests rather than sending them simultaneously. This avoids that your IP address (i.e., numerical label assigned to each device connected to the internet) gets blocked and you can no longer visit (and scrape) the website. \n",
    "\n",
    "In Python you can import the `sleep` module which pauses the execution of future commands for a given amount of time. For example, the print statement after `sleep(5)` will only be executed after 5 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll be printed to the console after 5 seconds!\n"
     ]
    }
   ],
   "source": [
    "# run this cell again to see the timer in action yourself!\n",
    "from time import sleep\n",
    "sleep(5)\n",
    "print(\"I'll be printed to the console after 5 seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this addition to our toolkit, let's finish up our book URL scraper by putting together everything we have learned thus far. Following Python conventions, let's modularize our  code into functions to improve readability and reusabilty. \n",
    "\n",
    "First, we define a function `generate_page_urls()` that takes a base url and an upper limit of the number of pages (50) as input parameters. This way we can easily update our scraper if more books are added or if the base url changes. \n",
    "\n",
    "Second, the `extract_book_urls()` function takes a list of page urls as input and returns a dictionary of book titles and URLs. Note the two-step structure of the for-loops: on every page we create a `books` object which we subsequently loop over by extracting the `book_title` and `book_url` from each book. These records are added to the dictionary `book_dict` which is eventually returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_page_urls(base_url, num_pages):\n",
    "    '''generate a list of full page urls from a base url and counter that has takes on the values between 1 and num_pages'''\n",
    "    page_urls = []\n",
    "    \n",
    "    for counter in range(1, num_pages + 1):\n",
    "        counter_url = f\"page-{counter}.html\"\n",
    "        full_url = base_url + counter_url \n",
    "        page_urls.append(full_url)\n",
    "        \n",
    "    return page_urls\n",
    "    \n",
    "def extract_book_urls(page_urls):\n",
    "    '''collect the book title and url for every book on all page urls'''\n",
    "    book_dict = {}\n",
    "    \n",
    "    for page_url in page_urls: \n",
    "        res = requests.get(page_url)\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        books = soup.find_all(class_=\"product_pod\")\n",
    "\n",
    "        for book in books: \n",
    "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"][6:]\n",
    "            book_dict[book_title] = book_url\n",
    "            \n",
    "        sleep(1)  # pause 1 second after each request\n",
    "            \n",
    "    return book_dict\n",
    "    \n",
    "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
    "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
    "book_dict = extract_book_urls(page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this code works without problems, there is one little improvement that we can make. If the number of pages changes, we need to manually update the `num_pages` parameter. For example, we may miss out once new books are added which appear on page 51 and further. \n",
    "\n",
    "\n",
    "### 2.2 Next Page Button\n",
    "\n",
    "A general solution is therefore to look up whether there is a `next` button on the page (HTML code below). If so, it means a next page exists and we keep on incrementing the page counter by 1. If not, it means we have reached the last page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/next_page.png\" align=\"left\" width=90% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, we write the function `check_next_page()` which takes an URL as input and returns the outgoing link of the next button (if present):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next page is: page-2.html\n"
     ]
    }
   ],
   "source": [
    "def check_next_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    next_btn = soup.find(class_= \"next\")\n",
    "    return next_btn.find(\"a\").attrs[\"href\"] if next_btn else None\n",
    "\n",
    "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "print(f\"The next page is: {check_next_page(page_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it the first page of the bookshop, and it returns the link to the second page (note that `page-2.html` is a relative path from the current URL). Now let's check what happens once we pass it the 50th page!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "1. Pass `https://books.toscrape.com/catalogue/page-50.html` to `check_next_page()` and observe the output. Is that what you expected? \n",
    "2. Write a function that that checks whether the output of `check_next_page()` is not `None` (i.e., anything but `None`). If so, it should return a new variable `page_url` that concatenates the base url and the relative path to the next page. If not, it should print the statement `This is the last page!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, we have revised the `extract_book_urls()` function. Instead of generating the list of page URLs upfront, we now use a `while` loop that stays `True` as long as there is a page url. At the end of each loop we update the `page_url` according to the link of the next button (using `check_next_page()`). On the last page there is no new page url and thus we break out of the while loop. All in all, we have modularized our code into functions, made it future-proof (e.g. if new books are added), and reduced the number of lines of code to get the job done! Try to run the function and inspect the output for yourself (you may need to wait for a bit as the scraper loops through all 50 pages!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_book_urls(page_url):\n",
    "    '''collect the book title and url for every book on all page urls'''\n",
    "    book_dict = {}\n",
    "\n",
    "    while page_url: \n",
    "        res = requests.get(page_url)\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        books = soup.find_all(class_=\"product_pod\")\n",
    "\n",
    "        for book in books: \n",
    "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
    "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"][6:]\n",
    "            book_dict[book_title] = book_url\n",
    "\n",
    "        sleep(1)  # pause 1 second after each request\n",
    "        \n",
    "        if check_next_page(page_url) != None: \n",
    "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
    "        else: \n",
    "            break\n",
    "        \n",
    "    return book_dict\n",
    "\n",
    "book_dict = extract_book_urls(\"https://books.toscrape.com/catalogue/page-1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Page-level Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember trying to obtain the URL of the [Black Bust](https://books.toscrape.com/catalogue/black-dust_976/index.html) book in exercise 2? Let's see whether it works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/black-dust_976/index.html\n"
     ]
    }
   ],
   "source": [
    "print(book_dict[\"Black Dust\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, it works flawlessly! But why did we need the book URLs in the first place? It forms the seed for other web scraping efforts. For example, the product descriptions can only be obtained from the book pages itself which means we need to loop over all book urls to extract the right information. In the follow-up exercise, we'll look at how to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "1. We'd like to extract the product description of our 3 favorite books. Fill in the blanks below to finish the `get_book_description()` function. Each `#` represents a single missing character (e.g. `####` means the solution requires 4 characters). \n",
    "2. Run the function and inspect the output. If you look carefully, you may spot `\\x80\\x99t` symbols throughout the product description. Look up the original text on the book pages and compare it side-by-side with the output of `book_dict`. What do these symbols mean? In the  Web Scraping Advanced in week 5, we'll go more in-depth about what it is exactly and how you can encode such characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_description(books):\n",
    "    book_descriptions = {}\n",
    "\n",
    "    for book in books: \n",
    "        page_url = book_dict[####]\n",
    "\n",
    "        res = requests.get(########)\n",
    "        soup = BeautifulSoup(res.text)\n",
    "\n",
    "        # tip: look at the Google Inspector screenshot below \n",
    "        description = soup.find(id=\"content_inner\").find_all(\"p\")[#].get_text()\n",
    "        book_descriptions[####] = ###########\n",
    "\n",
    "    return book_descriptions\n",
    "\n",
    "favorite_books = [\"Black Dust\", \"The Grand Design\", \"Twenties Girl\"]\n",
    "book_descriptions = get_book_description(##############)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/black_dust.png\" align=\"left\" width=90% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 CSV Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Include relevant metadata (e.g., time of extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book_descriptions.csv\", \"w\") as file: \n",
    "    headers = [\"title\", \"description\"]\n",
    "    csv_writer = DictWriter(file, fieldnames=headers)\n",
    "    csv_writer.writeheader()\n",
    "    for description in book_descriptions: \n",
    "        csv_writer.writerow(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Executing Python Files\n",
    "\n",
    "* Discuss pros and cons of Jupyter Notebooks and `.py` files\n",
    "    * Top down hierarchy \n",
    "    * Experiment and develop code on the fly\n",
    "    * Easier to debug (run cell by cell) \n",
    "    * Reproducibility (Latex + Markdown + inline plots) \n",
    "    * Browse through directories (file explorer)\n",
    "    * Can handle large codebases far easier\n",
    "    * Presenting/sharing -> iPython\n",
    "    * Prototyping -> iPython -> Move code into functions -> create new cell that uses function from cell above -> when all seems to be running good -> copy paste in a `.py` file -> move to module\n",
    "* Introduction to Spyder\n",
    "* Instructions on how to run `.py` files in the terminal \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
