{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101 (oDCM)\n",
    "\n",
    "*Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce pretium risus at ultricies egestas. Vivamus sit amet arcu sem. In hac habitasse platea dictumst. Nulla pharetra vitae mauris sed mollis. Pellentesque placerat mauris dui, in venenatis nisl posuere ac. Nunc vitae tincidunt risus, ut pellentesque odio. Donec quam neque, iaculis id eros et, condimentum vulputate nulla. Nullam sed ligula leo.*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to: \n",
    "* A\n",
    "* B\n",
    "* C\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This course draws on online resources built by Brian Keegan, Colt Steele, David Amos, Hannah Cushman Garland, Kimberly Fessel, and Thomas Laetsch. \n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Contact\n",
    "For technical issues try to be as specific as possible (e.g., include screenshots, your notebook, errors) so that we can help you better.\n",
    "\n",
    "**WhatsApp**  \n",
    "+31 13 466 8938\n",
    "\n",
    "**Email**  \n",
    "odcm@uvt.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generating Seeds\n",
    "* Scrape all fiction books (65 books) - 4 pagina's\n",
    "    * books on pages - href attribute\n",
    "    * page numbers https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
    "    * mocht er geen logica in zitten -> via next button\n",
    "        * base_url (quotes.toscrape.com)\n",
    "        * url = /page/1\n",
    "    * timers\n",
    "* Scrapy\n",
    "    * with a couple of lines really impressive results (but have to follow the rules of the framework) - trade-off speed of use vs flexbility\n",
    "        * variable names (start_url)\n",
    "    * good documentation\n",
    "    * crawl = scraping a page and moving on to the next page finding another link on that site and following that \n",
    "    \n",
    "https://quotes.toscrape.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# don't overload server\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "\n",
    "# write data to csv\n",
    "with open(\"blog_data.csv\", \"w\") as csv_file:\n",
    "    csv_writer = writer(csv_file)\n",
    "    csv_writer.writerow([\"title\", \"link\", \"date\"])\n",
    "    \n",
    "    for article in articles: \n",
    "        title = ...\n",
    "        url = ... \n",
    "        date = ...\n",
    "        csv_writer.writerow([title, url, date])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/kimfetti/Conferences/tree/master/PyCon_2020\n",
    "* https://www.youtube.com/watch?v=RUQWPJ1T6Zc&t=190s\n",
    "* https://github.com/hancush/web-scraping-with-python/blob/master/session/web-scraping-with-python.ipynb#HTML-basics\n",
    "* https://www.udemy.com/course/the-modern-python3-bootcamp/learn/lecture/7991196#overview\n",
    "* https://campus.datacamp.com/courses/web-scraping-with-python/introduction-to-html?ex=1\n",
    "* https://realpython.com/python-web-scraping-practical-introduction/\n",
    "* https://github.com/CU-ITSS/Web-Data-Scraping-S2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.find(class_='price_color'))\n",
    "#print(soup.find(id='product_gallery'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(attrs={\"data-example\": \"yes\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS Selectors\n",
    "* Select by id of foo: #foo\n",
    "* Select by class of bar: .bar\n",
    "* Select children: div > p\n",
    "* Select descendents: div p\n",
    "\n",
    "* name - tag name\n",
    "* attrs - dictionary of attributes -> bijv. number of stars - niet een aparte text voor -> attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('body').find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in feite kun je dit ook met find(class_ = \"... \") doen -> makkelijker\n",
    "soup.select('.price_color')[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in feite kun je dit ook met find(class_ = \"... \") doen -> makkelijker\n",
    "soup.select('.price_color')[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of stars\n",
    "soup.find(class_=\"star-rating\").attrs[\"class\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of elements (including '\\n')\n",
    "soup.body.contents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sibling is on the same level of the hierarchy \n",
    "soup.body.contents[1].next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.contents[1].find_next_sibling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.parent`\n",
    "`contents`\n",
    "`next_sibling`\n",
    "`previous_sibling`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project\n",
    "* Scrape data into CSV\n",
    "* Goal: Grab all links from blog\n",
    "* Data: store URL, anchor tag text, and date\n",
    "\n",
    "\n",
    "* Looping through a list of books \n",
    "for book in books: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XPath & Selectors \n",
    "\n",
    "* Single forward slash `/` used to move forward one generation\n",
    "* Double forward slashes `//` used to direct to all elements within the entire HTML code\n",
    "* Tag names between slashes give direction to which element(s)\n",
    "* Brackets [] after a tag name tell us which of the selected siblings to choose.\n",
    "\n",
    "In case the cell below throws a `ModuleNotFoundError` at you, you first need to install the `scrapy` package. Go to your terminal and type `conda install scrapy` and press `y` to proceed.\n",
    "\n",
    "* select the same book title with XPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy \n",
    "\n",
    "class BookSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "    start_urls = ['http://books.toscrape.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # response is what you get back from the HTTP request\n",
    "        # yield instead of return\n",
    "        for article in response.css('article.product_pod'):\n",
    "            yield {\n",
    "                'price': article.css(\".price_color::text\") \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "html = requests.get(url).content \n",
    "sel = Selector(text=html)\n",
    "\n",
    "sel.xpath(\"//h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legality\n",
    "* Some websites don't want people scraping them\n",
    "* Best practise: consult the robots.txt file \n",
    "    * website's way of saying: we don't want any code accessing all of these pages but this page is OK\n",
    "    * it's not law; it's convention\n",
    "    * imdb.com/robots.txt\n",
    "    * Andere regels voor Yahoo's code dan voor anderen zoekmachines en crawlers (everything is disallowed)\n",
    "    * rithmschool.com/robots.txt\n",
    "        * User-agent: * = wherever you're coming form you're allowed to access everything\n",
    "        * Allow: / = you're allowed to access everything\n",
    "* If making many requests, time them out (you don't want to constantly be making these requests one after another over and over way faster than any human would)\n",
    "    * For one reason that's to be polite (don't overload their servers)\n",
    "    * But also if they notice if the developers or a server notices 100000 requests coming from one IP address -> very clear that somebody is scraping them. \n",
    "    * https://en.wikipedia.org/wiki/Craigslist_Inc._v._3Taps_Inc.\n",
    "        * Company 3Taps Inc. was scraping Craigslist and using their data to build their own website (not just analyzing) \n",
    "        * You cannot be sued, go to jail, or be fined or something simply for scraping, but once a cease-and-desist letter has been sent and enacting an IP address block is sufficient notice of online trespassing which a plaintiff can use to claim a violation of the computer fraud and abuse act. \n",
    "        * You don't want to just launch a website or a company that relies on scraping, especially if you do get a cease and desist letter (or they block your IP address)\n",
    "        * https://www.lexology.com/library/detail.aspx?g=210e78b2-41df-4f75-a7fb-4e60909e231a\n",
    "        * We're going to stay away from anything potentially in that gray area. We're going to scrape my own sites and sites that I've worke don where we have permission to scrape. \n",
    "* If you're too aggressive, your IP can be blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
