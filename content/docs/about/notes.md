
# Some more content notes (to be integrated elsewhere)

## Possible activities

- Data collection pitch (group) / presentation
- Legal battle + anonymization outcome
- Building a prototype for either a web scraper or API
- Deployment as large project with the team
- Data sharing / auditing of data
- Real-time analytics (use database (learnt here), in combination with research method (e.g., regression), to create insights in realtime

session chair, presentations, pitches, discussions


## Content

### Web scraping

- Data retrieval from websites
- Application protocol interfaces
- Data/functionality/algorithm access
- API get
- API provide
- Releasing/sharing data via APIs
- Parsing

### Data management

- Database technology
- Structured: MySQL, Google BigQuery
- Unstructured: Amazon DynamoDB, MongoDB
- File-based systems
- Local: SSD, HDD
- Server/cloud: S3, FTP, Google Drive
- Per database
- Schema/design
- Extracting and writing data
- Indexing
- Maintenance
- ShinyApps / Interactive dashboards


- Scraper 1: Static scraper, single-machine, no database (two versions: either with or without browser); backward-looking versus forward-looking
- Scraper 2: Dynamic scraper, with database connection + monitoring

### Ethics

### Topics for the guide

#### Web scraping

- Design principles
- Connecting to a website without browser window
- Connecting to a website with browser window
- Using headers
- Cookies and continuing existing sessions
- Identifying objects using CSS selectors
- Identifying objects using XPATH selectors
- Loop through objects
- Timing / delays
- Looping
- Planning a data collection
- Seeding
- Sampling
- Writing to CSV
- Writing to JSON

#### Saving and writing locally and remotely (databases, file-based systems)
- Writing to file
- Writing to S3
- SQL - write
- SQL - read
